
Machine Super <span class="t20 hoverable">Intelligence</span>

Doctoral Dissertation submitted to the
Faculty of Informatics of the University of Lugano
in partial fulfillment of the requirements for the degree of
Doctor of Philosophy

presented by

Shane Legg

under the supervision of

Prof. Dr. Marcus Hutter

June 2008

Copyright © Shane Legg 2008
This document is licensed under a Creative Commons
Attribution-Share Alike 2.5 Switzerland License.

Dissertation Committee

Prof. Dr. Marcus Hutter
Prof. Dr. Jürgen Schmidhuber

Australian National University, Australia
IDSIA, Switzerland
Technical University of Munich, Germany

Prof. Dr. Fernando Pedone

University of Lugano, Switzerland

Prof. Dr. Matthias Hauswirth

University of Lugano, Switzerland

Prof. Dr. Marco Wiering

Supervisor
Prof. Dr. Marcus Hutter

Utrecht University, The Netherlands

PhD program director
Prof. Dr. Fabio Crestani

Contents
Preface
Thesis outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Prerequisite <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> . . . . . . . . . . . . . . . . . . . . . . . . . .
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . .

i
<span class="t1 t2 t3 t5 t6 t8 t9 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">iii</span>
vi
vi

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
1.1. Theories of <span class="t20 hoverable">intelligence</span> . . . . . . . .
1.2. Definitions of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span> . .
1.3. Definitions of machine <span class="t20 hoverable">intelligence</span> .
1.4. <span class="t20 hoverable">Intelligence</span> testing . . . . . . . . . .
1.5. <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">Human</span> <span class="t20 hoverable">intelligence</span> tests . . . . . . .
1.6. Animal <span class="t20 hoverable">intelligence</span> tests . . . . . . .
1.7. Machine <span class="t20 hoverable">intelligence</span> tests . . . . . .
1.8. Conclusion . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

1
3
4
9
11
13
15
16
22

. . . . . . .
. . . . . . .
. . . . . . .
complexity
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

23
23
25
27
30
32
36
38
39
43
47

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

53
53
57
60
62
65
68

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
4.1. A formal definition of machine <span class="t20 hoverable">intelligence</span> . . . . . . . . . . . .

71
72

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
2.1. Inductive inference . . . . . . . . .
2.2. Bayes’ rule . . . . . . . . . . . . .
2.3. Binary sequence prediction . . . .
2.4. Solomonoff’s prior and Kolmogorov
2.5. Solomonoff-Levin prior . . . . . . .
2.6. Universal inference . . . . . . . . .
2.7. Solomonoff induction . . . . . . . .
2.8. Agent-environment model . . . . .
2.9. Optimal informed agents . . . . . .
2.10. Universal AIXI agent . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

3. Taxonomy of Environments
3.1. Passive environments . . . . . . . . . . .
3.2. Active environments . . . . . . . . . . .
3.3. Some common problem classes . . . . .
3.4. Ergodic MDPs . . . . . . . . . . . . . .
3.5. Environments that admit self-optimising
3.6. Conclusion . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

. . . .
. . . .
. . . .
. . . .
agents
. . . .

4.2.
4.3.
4.4.
4.5.

Universal <span class="t20 hoverable">intelligence</span> of various agents
Properties of universal <span class="t20 hoverable">intelligence</span> . .
Response to common criticisms . . . .
Conclusion . . . . . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

78
83
86
93

5. Limits of Computational Agents
5.1. Preliminaries . . . . . . . . . . . . . . . .
5.2. Prediction of computable sequences . . . .
5.3. Prediction of simple computable sequences
5.4. Complexity of prediction . . . . . . . . . .
5.5. Hard to predict sequences . . . . . . . . .
5.6. The limits of mathematical analysis . . .
5.7. Conclusion . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

95
96
98
100
102
103
104
106

Learning Rate
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .
. . . . . . . . .

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.

109
110
112
115
117
118
119
120
123

6. Temporal Difference Updating <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span>
6.1. Temporal difference learning . . . .
6.2. Derivation . . . . . . . . . . . . . .
6.3. Estimating a small Markov process
6.4. A larger Markov process . . . . . .
6.5. Random Markov process . . . . . .
6.6. Non-stationary Markov process . .
6.7. Windy Gridworld . . . . . . . . . .
6.8. Conclusion . . . . . . . . . . . . .

a
.
.
.
.
.
.
.
.

.
.
.
.

7. Discussion
125
7.1. Are super intelligent machines possible? . . . . . . . . . . . . . 126
7.2. How <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> intelligent machines be developed? . . . . . . . . . . 128
7.3. Is building intelligent machines a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> idea? . . . . . . . . . . . 135
A. Notation and Conventions
B. Ergodic MDPs admit self-optimising agents
B.1. Basic definitions . . . . . . . . . . . . .
B.2. Analysis of stationary Markov chains . .
B.3. An optimal stationary policy . . . . . .
B.4. Convergence of expected average value .

139
.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

143
143
146
152
155

C. Definitions of <span class="t20 hoverable">Intelligence</span>
159
C.1. Collective definitions . . . . . . . . . . . . . . . . . . . . . . . . 159
C.2. Psychologist definitions . . . . . . . . . . . . . . . . . . . . . . 161
C.3. AI researcher definitions . . . . . . . . . . . . . . . . . . . . . . 164
Bibliography

167

Index

179

Mystics exult in mystery and want it to stay mysterious.
Scientists exult in mystery for a <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> reason:
it gives them something to do.
Richard Dawkins in The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">God</span> Delusion

Preface
This thesis concerns the optimal behaviour of agents in unknown computable
environments, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t6 t10 t15 hoverable">known</span> as universal artificial intelligence. These theoretical
agents are able to learn to perform optimally in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> types of environments.
Although they are able to optimally use prior information about the environment if it is available, in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> cases they <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> learn to perform optimally in the
absence of such information. Moreover, these agents can be proven to upper
bound the performance of general purpose computable agents. Clearly such
agents are extremely powerful and general, hence the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">name</span> universal artificial
intelligence.
That such agents can be mathematically defined at all <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> as a surprise to some. Surely then artificial <span class="t20 hoverable">intelligence</span> has been solved? Not quite.
The problem is that the theory behind these universal agents assumes infinite
computational resources. Although this greatly simplifies the mathematical
definitions and analysis, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that these models <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be directly
implemented as artificial <span class="t20 hoverable">intelligence</span> algorithms. Efforts have been <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> to
scale these ideas down, <span class="t27 t28 hoverable">however</span> as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> none of these methods have produced
practical algorithms that have been adopted by the mainstream. The main
use of universal artificial <span class="t20 hoverable">intelligence</span> theory <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t6 t9 t18 t26 hoverable">far</span> has been as a theoretical tool with which to mathematically study the properties of machine super
intelligence.
The foundations of universal <span class="t20 hoverable">intelligence</span> date back to the origins of philosophy and inductive inference. Universal artificial <span class="t20 hoverable">intelligence</span> proper started
with the <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> of Ray J. Solomonoff in the 1960’s. Solomonoff was considering
the problem of predicting binary sequences. What he discovered was a formulation for an inductive inference system that can be proven to very rapidly
learn to optimally predict any sequence that has a computable probability
distribution. Not only is this theory astonishingly powerful, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> brings <span class="t16 t21 hoverable">together</span> and elegantly formalises key philosophical principles behind inductive
inference. Furthermore, by considering special cases of Solomonoff’s model,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> can recover <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> statistical principles such as maximum likelihood,
minimum description length and maximum entropy. This makes Solomonoff’s
model a kind of grand unified theory of inductive inference. Indeed, if it
were not for its incomputability, the problem of induction <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be considered
solved. Whatever practical concerns <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have about Solomonoff’s model,
most <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> agree that it is nonetheless a beautiful blend of mathematics and
philosophy.

i

PREFACE

The main theoretical limitation of Solomonoff induction is that it only
addresses the problem of passive inductive learning, in particular sequence
prediction. Whether the agent’s predictions are correct or not has no effect on
the future observed sequence. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the agent is passive in the sense that it
is unable to influence the future. An example of this <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be predicting the
movement of the planets across the sky, or maybe the stock market, assuming
that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> is not wealthy enough to influence the market.
In the more general active case the agent is able to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> actions which
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> affect the observed future. For example, an agent playing chess not only
observes the other player, it is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> able to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> moves itself in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to
increase its chances of winning the game. This is a very general setting in
which seemingly any kind of goal directed problem can be framed. It is not
necessary to assume, as is typically done in game theory, that the environment,
in this case other player, plays optimally. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> do not assume that the
behaviour of the environment is Markovian, as is typically done in control
theory and reinforcement learning.
In the late 1990’s Marcus Hutter extended Solomonoff’s passive induction
model to the active case by combining it with sequential decision theory. This
produced a theory of universal agents, and in particular a universal agent for
a very general class of interactive environments, <span class="t6 t10 t15 hoverable">known</span> as the AIXI agent.
Hutter was able to prove that the behaviour of universal agents converges to
optimal in any setting where this is at all possible for a general agent, and
that these agents are Pareto optimal in the sense that no agent can perform
as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in all environments and strictly better in at least one. These are the
strongest <span class="t6 t10 t15 hoverable">known</span> results for a completely general purpose agent. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that
AIXI has such generality and extreme performance characteristics, it can be
considered to be a theoretical model of a super intelligent agent.
Unfortunately, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> stronger results showing that AIXI converges to optimal
behaviour rapidly, similar to Solomonoff’s convergence result, have been shown
to be impossible in some settings, and remain open questions in others. Indeed,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> questions about universal artificial <span class="t20 hoverable">intelligence</span> remain open. In <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span>
this is because the area is quite <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> with few <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> working in it, and partly
because proving results about universal intelligent agents seems to be difficult.
The goal of this thesis is to explore some of the open issues surrounding
universal artificial intelligence. In particular: In which settings the behaviour
of universal agents converges to optimal, the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which AIXI theory relates
to the concept and definition of intelligence, the limitations that computable
agents face when trying to approximate theoretical super intelligent agents
such as AIXI, and finally some of the big picture implications of super intelligent machines and whether this is a topic that deserves greater study.

ii

PREFACE

Thesis outline
<span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">Much</span> of the <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> presented in this thesis comes from prior publications. In
some cases <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span> chapters are heavily based on prior publications, in other
cases prior <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> is only mentioned in passing. Furthermore, while I wrote
the <span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">text</span> of the thesis, naturally not all of the ideas and <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> presented are
my own. Besides the presented background material, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of the results and
ideas in this thesis have been developed through collaboration with various
colleagues, in particular my supervisor Marcus Hutter. This section outlines
the contents of the thesis and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> provides some guidance on the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of
my contribution to each chapter.
1) <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of Intelligence. <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 1 begins the thesis
with the most fundamental question of all: What is intelligence? Amazingly,
books and papers on artificial <span class="t20 hoverable">intelligence</span> rarely delve into what <span class="t20 hoverable">intelligence</span>
actually is, or what artificial <span class="t20 hoverable">intelligence</span> is trying to achieve. When they
do address the topic they usually just mention the Turing test and that the
concept of <span class="t20 hoverable">intelligence</span> is poorly defined, before moving on to algorithms that
presumably have this mysterious quality. As this thesis concerns theoretical
models of systems that we claim to be extremely intelligent, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> explore the <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> tests and definitions of <span class="t20 hoverable">intelligence</span> that have been proposed
for humans, animals and machines. We draw from these an informal definition
of <span class="t20 hoverable">intelligence</span> that we will use throughout the rest of the thesis.
This overview of the theory, definition and testing of <span class="t20 hoverable">intelligence</span> is my own
work. This <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> is based on (Legg and Hutter, 2007c), in particular the
parts which built <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">upon</span> (Legg and Hutter 2007b; 2007a).
2) Universal Artificial Intelligence. At present AIXI is not widely <span class="t6 t10 t15 hoverable">known</span> in
academic circles, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> it has captured the imagination of a community interested in <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> approaches to general purpose artificial intelligence, so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span>
artificial general <span class="t20 hoverable">intelligence</span> (AGI). <span class="t27 t28 hoverable">However</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> <span class="t5 t7 hoverable">within</span> this community, it
is clear that there is some confusion about AIXI and universal artificial intelligence. This <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be attributable in <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> to the fact that current expositions of
AIXI are difficult for non-mathematicians to digest. As such, a less technical
introduction to the subject <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be helpful. Not only should this help clear
up some misconceptions, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> serve as an appetiser for the more technical treatments that have been published by Hutter. <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 2 provides such
an introduction. It starts with the basics of inductive inference and slowly
builds up to the AIXI agent and its key theoretical properties.
This introduction to universal artificial <span class="t20 hoverable">intelligence</span> has not been published
before, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> small parts of it were derived from (Hutter et al., 2007)
and (Legg, 1997). Section 2.6 is largely based on the material in (Hutter,
2007a), and the sections that follow this on (Hutter, 2005).

<span class="t1 t2 t3 t5 t6 t8 t9 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">iii</span>

PREFACE
3) Optimality of AIXI. Hutter has proven that universal agents converge
to optimal behaviour in any environment where this is possible for a general
agent. He further showed that the result holds for <span class="t3 hoverable">certain</span> types of Markov
decision processes, and claimed that this should generalise to related classes
of environments. Formally defining these environments and identifying the
additional conditions for the convergence result to hold was left as an open
problem. Indeed, it seems that nobody has <span class="t29 hoverable">ever</span> documented the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> abstract
environment classes that are studied and formally shown how they are related
to each other. In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 3 we create such a taxonomy and identify the
environment classes in which universal agents are able to learn to behave
optimally. The diversity of these classes of environments adds weight to our
claim that AIXI is super intelligent.
Most of the classes of environments are <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> known, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> their exact formalisations as presented are my own. The proofs of the relationships between
them and the resulting taxonomy of environment classes is my work. This
<span class="t12 t19 t25 t26 t28 hoverable">chapter</span> is largely based on (Legg and Hutter, 2004).
4) Universal <span class="t20 hoverable">Intelligence</span> Measure. If AIXI really is an optimally intelligent
machine, this suggests that we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be able to turn the problem around and
use universal artificial <span class="t20 hoverable">intelligence</span> theory to formally define a universal measure of machine intelligence. In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 4 we <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the informal definition of
<span class="t20 hoverable">intelligence</span> from <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 1 and abstract and formalise it using ideas from
the theory of universal artificial <span class="t20 hoverable">intelligence</span> in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 2. The result is an
alternate characterisation of Hutter’s <span class="t20 hoverable">intelligence</span> <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> relation. This gives us
a formal definition of machine <span class="t20 hoverable">intelligence</span> that we then compare with other
formal definitions and tests of machine <span class="t20 hoverable">intelligence</span> that have been proposed.
The specific formulation of the universal <span class="t20 hoverable">intelligence</span> measure is of my own
creation. The <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> is largely based on (Legg and Hutter, 2007c), in particular the parts of this paper which build <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">upon</span> (Legg and Hutter 2005b; 2006).
5) Limits of Computational Agents. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of the key reasons for studying incomputable but elegant theoretical models, such as Solomonoff induction and
AIXI, is that it is hoped that these will someday guide us towards powerful
computable models of artificial intelligence. Although there have been a number of attempts at converting these universal theories into practical methods,
the resulting methods have all been a mere shadow of their original founding
theory. Is this because we have not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> seen how to properly convert these
theories into practical algorithms, or are there more fundamental limitations
at work?
<span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 5 explores this question mathematically. Specifically, it looks at
the existence and <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of computable agents which are powerful and extremely general. The results reveal a number of fundamental constraints on
any endeavour to construct very general artificial <span class="t20 hoverable">intelligence</span> algorithms.

iv

PREFACE
The elementary results at the start of the <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> are already <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> known,
nevertheless the proofs <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> are my own. The more significant results towards
the <span class="t7 t10 t15 t25 hoverable">end</span> are entirely original and are my own work. The <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> is based
primarily on (Legg, 2006b) which built <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">upon</span> the results in (Legg, 2006a). The
core results <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> appear with other related <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> in the <span class="t25 hoverable">book</span> <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> (Legg
et al., 2008).
6) Fundamental Temporal Difference Learning. Although deriving practical
theories based on universal artificial <span class="t20 hoverable">intelligence</span> is problematic, there still exist
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> opportunities for theory to contribute to the development of <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> learning
techniques, albeit on a somewhat less grand scale. In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 6 we derive an
equation for temporal difference learning from statistical principles. We start
with the variational principle and then bootstrap to produce an update-rule
for discounted state value estimates. The resulting equation is similar to the
standard equation for temporal difference learning with eligibility traces, so
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> TD(λ), <span class="t27 t28 hoverable">however</span> it lacks the parameter that specifies the learning rate.
In the <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> of this free parameter there is now an equation for the learning
rate that is specific to each state transition. We experimentally test this <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span>
learning rule against TD(λ). Finally, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> some preliminary investigations
into how to extend our <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> temporal difference algorithm to reinforcement
learning.
The derivation of the temporal difference learning rate comes from a collection of unpublished derivations by Hutter. I <span class="t0 t3 t4 t5 t8 t10 t11 t16 t17 t18 t20 t21 t23 t24 t26 t27 t29 hoverable">went</span> through this collect of
handwritten notes, checked the proofs and <span class="t4 hoverable">took</span> out what seemed to be the
most promising candidate for a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> learning rule. The presented proof has
some reworking for improved presentation. The implementation and testing of
this update-rule is my own work, as is the extension to reinforcement learning
by merging it with Sarsa(λ) and Q(λ). These results were published in (Hutter
and Legg, 2007).
7) Discussion The concluding discussion on the future development of machine <span class="t20 hoverable">intelligence</span> is my own. This has not been published before.
Appendix A

A description of the mathematical notation used.

Appendix B
<span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 2

A convergence proof for ergodic MDPs needed for key results in

Appendix C This collection of definitions of intelligence, seemly the largest
in existence, is my own work. This section of the appendix was based on (Legg
and Hutter, 2007a).
Some of my other publications which are only mentioned in passing in this
thesis include (Smith et al., 1994; Legg, 1996; Cleary et al., 1996; Calude

v

PREFACE
et al., 2000; Legg et al., 2004; Legg and Hutter, 2005a; Hutter and Legg, 2006).
Coverage of the research in this thesis in the popular scientific press includes
<span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> Scientist magazine (Graham-Rowe, 2005), Le Monde de l’intelligence
(Fiévet, 2005), as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as numerous blog and online newspaper articles.

Prerequisite <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
The thesis aims to be fairly <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">self</span> contained, <span class="t27 t28 hoverable">however</span> some <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> of mathematics, statistics and theoretical computer science is assumed. From mathematics the reader should be familiar with linear algebra, calculus, basic <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span>
theory and logic. From statistics, basic probability theory and elementary
distributions such as the uniform and binomial distributions. A <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> of
measure theory <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be beneficial, but is not essential. From theoretical computer science a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> of the basics such as Turing computation, universal
Turing machines, incomputability and the halting problem are needed. The
mathematical notation and conventions adopted are described in Appendix A.
The reader <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> want to consult this before beginning <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 2 as this is
where the mathematical material begins.

Acknowledgements
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">First</span> and foremost I <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> to thank my supervisor Marcus Hutter. Getting a PhD is a somewhat <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> process and I have appreciated his guidance
throughout this endeavour. I am especially grateful for the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which he
has always gone through my <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> carefully and provided detailed feedback on
where there was room for improvement. Not <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> graduate student receives
such careful appraisal and guidance during this <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> voyage.
Essentially all of the research contained in this thesis was carried out at the
Dalle Molle Institute for Artificial <span class="t20 hoverable">Intelligence</span> (IDSIA) near Lugano, Switzerland. It has been a pleasure to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with such a talented group of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span>
over the last 4 years. In particular I <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> to thank Alexey Chernov for
encouraging me to develop a few short proofs on the limits of computational
prediction systems into a full length paper. For me, that was a turning point
in my thesis.
A special thanks goes to my reading group: Jeff Rose, Cyrus Hall, Giovanni
Luca Ciampaglia, Katerina Barone-Adesi, Tom Schaul and Daan Wierstra.
They <span class="t0 t3 t4 t5 t8 t10 t11 t16 t17 t18 t20 t21 t23 t24 t26 t27 t29 hoverable">went</span> through most of my thesis finding typos and places where <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span>
were not <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> explained. The thesis is no doubt <span class="t6 t9 t18 t26 hoverable">far</span> more intelligible due
to their efforts. Special thanks <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> to my mother Gail Legg and Christoph
Kolodziejski for further proof reading efforts.
My research has benefited from interaction with <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> other colleagues,
both at IDSIA and other research centres, in particular Jürgen Schmidhuber, Jan Poland, Daniil Ryabko, Faustino Gomez, Matteo Gagliolo, Frederick

vi

PREFACE
Ducatelle, Alex Graves, Bram Bakker, Viktor Zhumatiy and Laurent Orseau.
I <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> to thank the institute secretary, Cinzia Daldini, for her amazing ability to find solutions to all manner of issues. It <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> coming to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span>
at IDSIA and living in Switzerland a breeze. Finally, thanks to etomchek for
designing the beautiful electric sheep on the front cover, and releasing it under
the creative commons licence. I always wanted a sheep on the cover of my PhD
thesis.
This research was funded by the Swiss National Science Foundation under
grants 2100-67712.0 and 200020-107616. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> funding agencies are not willing
to support such blue-sky research. Their backing has been greatly appreciated.
Lugano, Switzerland, June 2008

Shane Legg

<span class="t14 t27 hoverable">vii</span>

PREFACE

<span class="t6 t14 t27 hoverable">viii</span>

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of
<span class="t20 hoverable">Intelligence</span>
“Innumerable tests are available for measuring intelligence, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> no
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> is quite <span class="t3 hoverable">certain</span> of what <span class="t20 hoverable">intelligence</span> is, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> just what it is
that the available tests are measuring.” Gregory (1998)
What is intelligence? It is a concept that we use in our daily lives that
seems to have a fairly concrete, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> perhaps naive, meaning. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that
our friend who got an A in his calculus test is very intelligent, or perhaps our
cat who has learnt to go into hiding at the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> mention of the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> “vet”.
Although this intuitive notion of <span class="t20 hoverable">intelligence</span> presents us with no difficulties, if
we attempt to dig deeper and define it in precise terms we find the concept to
be very difficult to nail down. Perhaps the ability to learn quickly is central to
intelligence? Or perhaps the total sum of one’s <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> is more important?
Perhaps communication and the ability to use language play a central role?
What about “thinking” or the ability to perform abstract reasoning? How
about the ability to be creative and solve problems? <span class="t20 hoverable">Intelligence</span> involves a
perplexing mixture of concepts, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of which are equally difficult to define.
Psychologists have been grappling with these issues <span class="t29 hoverable">ever</span> since humans <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span>
became fascinated with the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of the mind. Debates have raged back and
<span class="t2 t3 t4 t5 t6 t9 t10 t11 t13 t15 t17 t18 t19 t20 t22 t23 t25 t26 t29 hoverable">forth</span> concerning the correct definition of <span class="t20 hoverable">intelligence</span> and how best to measure
the <span class="t20 hoverable">intelligence</span> of individuals. These debates have in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> instances been very
heated as what is at stake is not merely a scientific definition, but a fundamental issue of how we measure and value humans: Is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> employee smarter than
another? Are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">men</span> on average more intelligent than women? Are white <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span>
smarter than black people? As a result <span class="t20 hoverable">intelligence</span> tests, and their creators,
have on occasion been the subject of intense public scrutiny. Simply determining whether a test, perhaps quite unintentionally, is partly a reflection of
the race, gender, culture or social class of its creator is a subtle, complex and
often politically charged issue (Gould, 1981; Herrnstein and Murray, 1996).
Not surprisingly, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> have concluded that it is wise to stay <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> clear of this
topic.
In reality the situation is not as bad as it is sometimes <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> out to be.
Although the details of the definition are debated, in broad terms a fair degree of consensus has been achieved about the scientific definition of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span>
<span class="t20 hoverable">intelligence</span> and how to measure it (Gottfredson, 1997a; Sternberg and Berg,
1986). Indeed it is widely recognised that when standard <span class="t20 hoverable">intelligence</span> tests are
correctly applied and interpreted, they all measure approximately the same

1

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
thing (Gottfredson, 1997a). Furthermore, what they measure is both stable
over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> in individuals and has significant predictive power, in particular for
future academic performance and other mentally demanding pursuits. The issues that continue to draw debate are questions such as whether the tests test
only a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> or a particular type of intelligence, or whether they are somehow
biased towards a particular group or <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of mental skills. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Great</span> effort has gone
into dealing with these issues, but they are difficult problems with no easy
solutions.
Somewhat disconnected from this exists a parallel debate over the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span>
of <span class="t20 hoverable">intelligence</span> in the context of machines. While the debate is less politically
charged, in some ways the central issues are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more difficult. Machines can
have physical forms, sensors, actuators, <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> of communication, information
processing abilities and exist in environments that are totally unlike those that
we experience. This makes the concept of “machine intelligence” particularly
difficult to get a handle on. In some cases, a machine <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have properties that
are similar to <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence, and so it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be reasonable to describe
the machine as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> being intelligent. In other situations this view is <span class="t6 t9 t18 t26 hoverable">far</span> too
limited and anthropocentric. Ideally we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> to be able to measure the
<span class="t20 hoverable">intelligence</span> of a wide range of systems: humans, dogs, flies, robots or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span>
disembodied systems such as chat-bots, expert systems, classification systems
and prediction algorithms (Johnson, 1992; Albus, 1991).
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> response to this problem <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to develop specific kinds of tests
for specific kinds of entities, just as <span class="t20 hoverable">intelligence</span> tests for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t16 t17 t18 t19 t20 t22 t23 t24 t26 t29 hoverable">children</span> differ to
<span class="t20 hoverable">intelligence</span> tests for adults. While this <span class="t9 t16 t26 hoverable">works</span> <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> when testing humans of
<span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> ages, it comes undone when we need to measure the <span class="t20 hoverable">intelligence</span> of
entities which are profoundly <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> to each other in terms of their cognitive
capacities, speed, senses, environments in which they operate, and so on. To
measure the <span class="t20 hoverable">intelligence</span> of such diverse systems in a meaningful <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
step back from the specifics of particular systems and establish fundamentally
what it is that we are really trying to measure.
The difficulty of forming a highly general notion of <span class="t20 hoverable">intelligence</span> is readily
apparent. Consider, for example, that memory and numerical computation
tasks were once regarded as defining hallmarks of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence. We now
<span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that these tasks are absolutely trivial for a machine and do not test its
<span class="t20 hoverable">intelligence</span> in any meaningful sense. Indeed, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> the mentally demanding
task of playing chess can now be largely reduced to brute force search (Hsu
et al., 1995). What else <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> be possible with relatively simple algorithms running on powerful machines is hard to say. What we can be sure
of is that, as technology advances, our concept of <span class="t20 hoverable">intelligence</span> will continue to
evolve with it.
How then are we to develop a concept of <span class="t20 hoverable">intelligence</span> that is applicable to
all kinds of systems? Any proposed definition <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> encompass the essence
of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence, as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as other possibilities, in a consistent way. It
should not be limited to any particular <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of senses, environments or goals,
nor should it be limited to any specific kind of hardware, such as silicon or

2

1.1. Theories of <span class="t20 hoverable">intelligence</span>
biological neurons. It should be based on principles which are fundamental
and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> unlikely to alter over time. Furthermore, the definition of <span class="t20 hoverable">intelligence</span>
should ideally be formally expressed, objective, and practically realisable as
an effective test. Before attempting to construct such a formal definition in
<span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 4, in this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> survey existing definitions, tests and
theories of intelligence. We are particularly interested in common themes and
general perspectives on <span class="t20 hoverable">intelligence</span> that <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be applicable to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> kinds of
systems, including machines.

1.1. Theories of <span class="t20 hoverable">intelligence</span>
A central question in the study of <span class="t20 hoverable">intelligence</span> concerns whether <span class="t20 hoverable">intelligence</span>
should be viewed as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> ability, or many. On <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> side of the debate are
the theories that view <span class="t20 hoverable">intelligence</span> as consisting of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> components
and that identifying these components is important to understanding intelligence. <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">Different</span> theories propose <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> ways to do this. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> was
Thurstone’s “multiple-factors” theory which considers seven “primary mental
abilities”: verbal comprehension, <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> fluency, number facility, spatial visualisation, associative memory, perceptual speed and reasoning (Thurstone, 1938).
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> approach is Sternberg’s “Triarchic Mind” which breaks <span class="t20 hoverable">intelligence</span>
down into analytical intelligence, creative intelligence, and practical <span class="t20 hoverable">intelligence</span> (Sternberg, 1985), <span class="t27 t28 hoverable">however</span> this model is now considered outdated, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span>
by Sternberg himself.
Taking the number of components to an extreme is Guilford’s “Structure of
Intellect” theory. Under this theory there are <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> fundamental dimensions:
contents, operations, and products. <span class="t16 t21 hoverable">Together</span> these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t28 t29 hoverable">give</span> rise to 120 <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
categories (Guilford, 1967). In later <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> this increased to 150 categories. This
theory has been criticised due to the fact that measuring such precise combinations of cognitive capacities in individuals seems to be infeasible and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> it
is difficult to experimentally study such a fine-grained model of intelligence.
A recently popular approach is Gardner’s “multiple intelligences” where he
argues that the components of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span> are sufficiently separate
that they are actually <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> “intelligences”(Gardner, 1993). Based on the
structure of the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> brain he identifies these intelligences to be linguistic,
musical, logical-mathematical, spatial, bodily kinaesthetic, intra-personal and
inter-personal intelligence. Although Gardner’s theory of multiple intelligences
has certainly captured the imagination of the public, it remains to be seen to
what degree it will have a lasting impact in professional circles.
At the other <span class="t7 t10 t15 t25 hoverable">end</span> of the spectrum is the <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> of Spearman and those that
have followed in his approach. Here <span class="t20 hoverable">intelligence</span> is seen as a very general
mental ability that underlies and contributes to all other mental abilities. As
evidence they point to the fact that an individual’s performance levels in reasoning, association, linguistic, spatial thinking, pattern identification etc. are
positively correlated. Spearman <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> this positive statistical correlation be-

3

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
tween <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> mental abilities the “g-factor”, where g stands for “general
intelligence”(Spearman, 1927). Because standard IQ tests measure a range of
key cognitive abilities, from a collection of scores on <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> cognitive tasks
we can estimate an individual’s g-factor. Some who consider the generality
of <span class="t20 hoverable">intelligence</span> to be primary <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the g-factor to be the very definition of
<span class="t20 hoverable">intelligence</span> (Gottfredson, 2002).
A <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> refinement to the g-factor theory due to Cattell is to distinguish between “fluid intelligence”, which is a very general and flexible innate
ability to deal with problems and complexity, and “crystallized intelligence”,
which measures the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and abilities that an individual has acquired
over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> (Cattell, 1987). For example, while an adolescent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have a similar
level of fluid <span class="t20 hoverable">intelligence</span> to that of an adult, their level of crystallized <span class="t20 hoverable">intelligence</span> is typically lower due to less <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> experience (Horn, 1970). Although it is
difficult to determine to what extent these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> influence each other, the distinction is an important <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> because it captures <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> distinct notions of what
the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> “intelligence” means.
As the g-factor is simply the statistical correlation between <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> kinds
of mental abilities, it is not fundamentally inconsistent with the view that
<span class="t20 hoverable">intelligence</span> can have multiple aspects or dimensions. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> a synthesis of the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> perspectives is possible by viewing <span class="t20 hoverable">intelligence</span> as a hierarchy with the gfactor at its apex and increasing levels of specialisation for the <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> aspects
of <span class="t20 hoverable">intelligence</span> forming branches (Carroll, 1993). For example, an individual
<span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> have a high g-factor, which contributes to all of their cognitive abilities,
but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> have an especially <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> developed musical sense. This hierarchical
view of <span class="t20 hoverable">intelligence</span> is now quite popular (Neisser et al., 1996).

1.2. Definitions of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span>
“Viewed narrowly, there seem to be almost as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> definitions of
<span class="t20 hoverable">intelligence</span> as there were experts asked to define it.” R. J. Sternberg quoted in (Gregory, 1998)
In this section and the next we will overview a range of definitions of <span class="t20 hoverable">intelligence</span> that have been <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by psychologists. For an <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more extensive
collection of definitions of intelligence, indeed the largest collection that we
are aware of, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Appendix C or visit our online collection (Legg and Hutter,
2007a). Although definitions differ, there are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> reoccurring features; in
some cases these are explicitly stated, while in <span class="t1 t7 t13 hoverable">others</span> they are more implicit.
We start by considering ten definitions that <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> a similar perspective:
“It seems to us that in <span class="t20 hoverable">intelligence</span> there is a fundamental faculty, the
alteration or the lack of which, is of the utmost importance for practical life. This faculty is judgement, otherwise <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> sense, practical sense, initiative, the faculty of adapting oneself to circumstances.”
Binet and Simon (1905)

4

1.2. Definitions of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span>
“The capacity to learn or to profit by experience.” Dearborn quoted
in (Sternberg, 2000)
“Ability to adapt oneself adequately to relatively <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> situations in life.”
Pinter quoted in (Sternberg, 2000)
“A <span class="t9 hoverable">person</span> possesses <span class="t20 hoverable">intelligence</span> insofar as he has learned, or can learn, to
adjust himself to his environment.” Colvin quoted in (Sternberg, 2000)
“We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">shall</span> use the term ‘intelligence’ to mean the ability of an organism
to solve <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> problems . . . ” Bingham (1937)
“A global concept that involves an individual’s ability to act purposefully,
think rationally, and deal effectively with the environment.” Wechsler
(1958)
“Individuals differ from <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> in their ability to understand complex ideas, to adapt effectively to the environment, to learn from experience, to engage in various forms of reasoning, to overcome obstacles
by taking thought.” American Psychological Association (Neisser et al.,
1996)
“. . . I prefer to refer to it as ‘successful intelligence.’ And the reason is
that the emphasis is on the use of your <span class="t20 hoverable">intelligence</span> to achieve success in
your life. So I define it as your skill in achieving whatever it is you want
to attain in your <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> <span class="t5 t7 hoverable">within</span> your sociocultural context — <span class="t6 t13 t14 t25 t27 hoverable">meaning</span> that
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> have <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> goals for themselves, and for some it’s to get very
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> grades in school and to do <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> on tests, and for <span class="t1 t7 t13 hoverable">others</span> it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to
<span class="t8 t13 hoverable">become</span> a very <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> basketball player or actress or musician.” Sternberg
(2003)
“Intelligence is <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the internal environment that shows through at
the interface between <span class="t9 hoverable">person</span> and external environment as a function of
cognitive task demands.” R. E. Snow quoted in (Slatter, 2001)
“. . . <span class="t3 hoverable">certain</span> <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of cognitive capacities that enable an individual to adapt
and thrive in any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> environment they find themselves in, and those
cognitive capacities include <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> memory and retrieval, and problem solving and so forth. There’s a cluster of cognitive abilities that lead
to successful adaptation to a wide range of environments.” Simonton
(2003)
Perhaps the most elementary common feature of these definitions is that
<span class="t20 hoverable">intelligence</span> is seen as a property of an individual who is interacting with an
external environment, problem or situation. Indeed, at least this <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> is
common to practically all proposed definitions of intelligence.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> common feature is that an individual’s <span class="t20 hoverable">intelligence</span> is related to
their ability to succeed or profit. This implies the existence of some kind of

5

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
objective or goal. What the goal is, is not specified, indeed individuals’ goals
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be varied. The important thing is that the individual is able to carefully
choose their actions in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that leads to them accomplishing their goals.
The greater this capacity to succeed with respect to various goals, the greater
the individual’s intelligence.
The strong emphasis on learning, adaption and experience in these definitions implies that the environment is not fully <span class="t6 t10 t15 hoverable">known</span> to the individual and
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> contain <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> situations that <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> not have been anticipated in advance.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> <span class="t20 hoverable">intelligence</span> is not the ability to deal with a fully <span class="t6 t10 t15 hoverable">known</span> environment,
but rather the ability to deal with some range of possibilities which <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be
wholly anticipated. What is important then is that the individual is able to
quickly learn and adapt so as to perform as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as possible over a wide range
of environments, situations, tasks and problems. Collectively we will refer to
these as “environments”, similar to some of the definitions above.
Bringing these key features <span class="t16 t21 hoverable">together</span> gives us what we believe to be the
essence of <span class="t20 hoverable">intelligence</span> in its most general form:
<span class="t20 hoverable">Intelligence</span> measures an agent’s ability to achieve goals in a wide
range of environments.
We <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> this to be our informal working definition of <span class="t20 hoverable">intelligence</span> for this thesis.
The remainder of this section considers a range of other definitions that are
not as strongly connected to our adopted definition. Usually it is not that they
are entirely incompatible with our definition, but rather they stress <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
aspects of intelligence. The following definition is an especially interesting
definition as it was <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> as <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of a group statement signed by 52 experts in
the field. As such it obviously represents a fairly mainstream perspective:
“Intelligence is a very general mental capability that, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">among</span> other things,
involves the ability to reason, plan, solve problems, think abstractly,
comprehend complex ideas, learn quickly and learn from experience.”
Gottfredson (1997a)
Reasoning, planning, solving problems, abstract thinking, learning from experience and so on, these are all mental abilities that allow us to successfully
achieve goals. If we were missing any <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of these capacities, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> clearly
be less able to successfully deal with such a wide range of environments. Thus,
these capacities are implicit in our definition also. The difference is that our
definition does not attempt to specify what capabilities <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be needed, something which is clearly very difficult and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> depend on the particular tasks
that the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> deal with. Our approach is to consider <span class="t20 hoverable">intelligence</span> to be
the effect of capacities such as those listed above. It is not the result of having
any specific <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of capacities. Indeed, <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be the effect of
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> other capacities, some of which humans <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> not have. In summary,
our definition is not in conflict with the above definition, rather it is that our
definition is more abstract and general.

6

1.2. Definitions of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span>
“. . . in its lowest terms <span class="t20 hoverable">intelligence</span> is present where the individual animal, or <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> being, is aware, <span class="t27 t28 hoverable">however</span> dimly, of the relevance of his
behaviour to an objective. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> definitions of what is indefinable have
been attempted by psychologists, of which the least unsatisfactory are 1.
the capacity to meet novel situations, or to learn to do so, by <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> adaptive responses and 2. the ability to perform tests or tasks, involving the
grasping of relationships, the degree of <span class="t20 hoverable">intelligence</span> being proportional to
the complexity, or the abstractness, or both, of the relationship.” Drever
(1952)
This definition has <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> similarities to ours. Firstly, it emphasises the
agent’s ability to choose its actions so as to achieve an objective, or in our
terminology, a goal. It then goes on to stress the agent’s ability to deal with
situations which have not been encountered before. In our terminology, this is
the ability to deal with a wide range of environments. Finally, this definition
highlights the agent’s ability to perform tests or tasks, something which is
entirely consistent with our performance orientated perspective of intelligence.
“Intelligence is not a single, unitary ability, but rather a composite of several functions. The term denotes that combination of abilities required
for survival and advancement <span class="t5 t7 hoverable">within</span> a particular culture.” Anastasi
(1992)
This definition does not specify exactly which capacities are important, only
that they should enable the individual to survive and advance with the culture.
As such this is a more abstract “success” orientated definition of intelligence,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> ours. Naturally, culture is a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the agent’s environment, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> only
complex environments with other agents <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> have <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> culture.
“The ability to carry on abstract thinking.”
in (Sternberg, 2000)

L. M. Terman quoted

This is not really <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> of a definition as it simply shifts the problem of
defining <span class="t20 hoverable">intelligence</span> to the problem of defining abstract thinking. The same
is <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> other definitions that refer to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> such as imagination,
creativity or consciousness. The following definition has a similar problem:
“The capacity for knowledge, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> possessed.” Henmon (1921)

What exactly constitutes “knowledge”, as opposed to perhaps data or information? For example, does a library contain a lot of knowledge, and if so, is it
intelligent? Or perhaps the internet? Modern concepts of the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
stress the fact that the information has to be in some sense properly contextualised so that it has meaning. Defining this more precisely appears to be

7

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
difficult however. Because this definition of <span class="t20 hoverable">intelligence</span> dates from 1921, perhaps it reflects pre-information age thinking when computers with vast storage
capacities did not exist.
Nonetheless, our definition of <span class="t20 hoverable">intelligence</span> is not entirely inconsistent with
the above definition in that an individual <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be required to <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span>
things, or have a significant capacity for knowledge, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span>
in some environments. However, our definition is narrower in that knowledge,
or the capacity for knowledge, is not by itself sufficient. We require that the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> can be <span class="t27 hoverable">used</span> effectively. Indeed, unless information can be effectively
utilised for various purposes, it seems reasonable to consider it to be merely
“data”, rather than “knowledge”.
“The capacity to acquire capacity.” H. Woodrow quoted in (Sternberg,
2000)
The definition of Woodrow is typical of those that emphasise not the current
ability of the individual, but rather the individual’s ability to expand and
develop <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> abilities. This is a fundamental point of divergence for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span>
views on intelligence. Consider the following question: Is a young child as
intelligent as an adult? From <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> perspective, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t16 t17 t18 t19 t20 t22 t23 t24 t26 t29 hoverable">children</span> are very intelligent
because they can learn and adapt to <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> situations quickly. On the other
hand, a child is unable to do <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> due to a lack of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and
experience and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> mistakes an adult <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> to avoid. These
need not just be physical acts, they <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be more subtle <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span>
errors in reasoning as their mind, while very malleable, has not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> matured.
In which case, perhaps their <span class="t20 hoverable">intelligence</span> is currently low, but will increase with
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> and experience?
Fundamentally, this difference in perspective is a question of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> scale:
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">Must</span> an agent be able to tackle some task immediately, or perhaps after a
short period of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> during which learning can <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> place, or perhaps it only
matters that they can eventually learn to deal with the problem? Being able
to deal with a difficult problem immediately is a matter of experience, rather
than intelligence. While being able to deal with it in the very <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> run <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span>
not require <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> <span class="t20 hoverable">intelligence</span> at all, for example, simply trying a vast number
of possible solutions <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> eventually produce the desired results. <span class="t20 hoverable">Intelligence</span>
then seems to be the ability to adapt and learn as quickly as possible <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span>
the constraints imposed by the problem at hand.
“Intelligence is a general factor that runs through all types of performance.” A. Jensen
At <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> this <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> not look <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> a definition of intelligence, but it makes an
important point: <span class="t20 hoverable">Intelligence</span> is not really the ability to do anything in particular, rather it is a very general ability that affects <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> kinds of performance.
Conversely, by measuring <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> kinds of performance we can estimate
an individual’s intelligence. This is consistent with our definition’s emphasis
on the agent’s ability to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> environments.

8

1.3. Definitions of machine <span class="t20 hoverable">intelligence</span>
“Intelligence is what is measured by <span class="t20 hoverable">intelligence</span> tests.” Boring (1923)
Boring’s famous definition of <span class="t20 hoverable">intelligence</span> takes this idea a step further. If <span class="t20 hoverable">intelligence</span> is not the ability to do anything in particular, but rather an abstract
ability that indirectly affects performance in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> tasks, then perhaps it is
most concretely described as the ability to do the kind of abstract problems
that appear in <span class="t20 hoverable">intelligence</span> tests? In which case, Boring’s definition is not as
facetious as it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> appears. This definition <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> highlights the fact that the
concept of intelligence, and how it is measured, are intimately related. In the
context of this paper we refer to these as definitions of intelligence, and tests of
intelligence, respectively, although in some cases the distinction is not sharp.

1.3. Definitions of machine <span class="t20 hoverable">intelligence</span>
The following sample of informal definitions of machine <span class="t20 hoverable">intelligence</span> capture a
range of perspectives. There <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> exist several formal definitions and tests of
machine intelligence, <span class="t27 t28 hoverable">however</span> we will deal with those in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 4. We begin
with five definitions that have clear connections to our informal definition:
“. . . the mental ability to sustain successful life.” K. Warwick quoted
in (Asohan, 2003)
“. . . doing <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> at a broad range of tasks is an empirical definition of
‘intelligence’ ” Masum et al. (2002)
“Intelligence is the computational <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the ability to achieve goals in
the world. Varying kinds and degrees of <span class="t20 hoverable">intelligence</span> occur in people,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> animals and some machines.” McCarthy (2004)
“Any system . . . that generates adaptive behaviour to meet goals in a
range of environments can be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">said</span> to be intelligent.” Fogel (1995)
“. . . the ability of a system to act appropriately in an uncertain environment, where appropriate action is that which increases the probability
of success, and success is the achievement of behavioral subgoals that
support the system’s ultimate goal.” Albus (1991)
The position <span class="t19 hoverable">taken</span> by Albus is especially similar to ours. Although the
quote above does not explicitly mention the need to be able to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in
a wide range of environments, at a later point in the same paper he mentions
the need to be able to succeed in a “large variety of circumstances”.
“Intelligent systems are expected to work, and <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> well, in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> environments. Their property of <span class="t20 hoverable">intelligence</span> allows them to maximize the probability of success <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if full <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> of the situation
is not available. Functioning of intelligent systems <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be considered
separately from the environment and the concrete situation including
the goal.” Gudwin (2000)

9

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
While this definition is consistent with the position we have taken, when
trying to actually test the <span class="t20 hoverable">intelligence</span> of an agent Gudwin does not believe
that a “black box” behaviour based approach is sufficient, rather his approach
is to look at the “. . . architectural details of structures, organizations, processes
and algorithms <span class="t27 hoverable">used</span> in the construction of the intelligent systems,” (Gudwin,
2000). Our perspective is simply to not care whether an agent looks intelligent
on the inside. If it is able to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in a wide range of environments,
that is all that matters.
“We define <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> perspectives on artificial system intelligence: (1) native intelligence, expressed in the specified complexity inherent in the
information content of the system, and (2) performance intelligence, expressed in the successful (i.e., goal-achieving) performance of the system
in a complicated environment.” Horst (2002)
Here we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> distinct notions of intelligence, a performance based <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
and an information content one. This is similar to the distinction between
fluid <span class="t20 hoverable">intelligence</span> and crystallized <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> by the psychologist Cattell
(see Section 1.1). The performance based notion of <span class="t20 hoverable">intelligence</span> is similar to
our definition with the exception that performance is measured in a complex
environment rather than across a wide range of environments. This perspective
appears in some other definitions also,
“. . . the ability to solve hard problems.” Minsky (1985)
“Achieving complex goals in complex environments” Goertzel (2006)
The emphasis on complex goals and environments is not really so <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
to our “wide range of environments” in that any agent which <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> not achieve
simple goals in simple environments presumably <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> not be considered intelligent. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> argue that the ability to achieve truly complex goals in
complex environments requires the ability to achieve simple ones, in which
case the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> perspectives are equivalent.
Some definitions emphasise not just the ability to perform well, but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> the
need for efficiency:
“[An intelligent agent does what] is appropriate for its circumstances
and its goal, it is flexible to changing environments and changing goals,
it learns from experience, and it makes appropriate choices <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> perceptual limitations and finite computation.” Poole et al. (1998)
“. . . in any real situation behavior appropriate to the ends of the system
and adaptive to the demands of the environment can occur, <span class="t5 t7 hoverable">within</span> some
limits of speed and complexity.” Newell and Simon (1976)
“Intelligence is the ability to use optimally limited resources – including
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> – to achieve goals.” Kurzweil (2000)

10

1.4. <span class="t20 hoverable">Intelligence</span> testing
“Intelligence is the ability for an information processing agent to adapt
to its environment with insufficient <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and resources.” Wang
(1995)
We consider the addition of resource limitations to the definition of <span class="t20 hoverable">intelligence</span> to be either superfluous, or wrong. In the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> case, if limited computational resources are a fundamental and unavoidable <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of reality, which
certainly seems to be the case, then their addition to the definition of <span class="t20 hoverable">intelligence</span> is unnecessary. Perhaps the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> definitions above fall into this
category.
On the other hand, if limited resources are not a fundamental restriction,
for example a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> model of computation was discovered that was vastly more
powerful than the current model, then it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be odd to claim that the
unbelievably powerful machines that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> then result were not intelligent.
Normally we do not judge the <span class="t20 hoverable">intelligence</span> of something relative to the resources
it uses. For example, if a rat had <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> level learning and problem solving
abilities, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> not think of the rat as being more intelligent than a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span>
due to the fact that its brain was <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> smaller.
While we do not consider efficiency to be a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the definition of intelligence, this is not to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that considering the efficiency of agents is unimportant.
Indeed, a key goal of artificial <span class="t20 hoverable">intelligence</span> is to find algorithms which have the
greatest efficiency of intelligence, that is, which achieve the most <span class="t20 hoverable">intelligence</span>
per unit of computational resources consumed.

1.4. <span class="t20 hoverable">Intelligence</span> testing
Having explored what <span class="t20 hoverable">intelligence</span> is, we now turn to how it is measured.
Contrary to popular public opinion, most psychologists believe that standard
psychometric tests of intelligence, such as IQ tests, reliably measure something important in humans (Neisser et al., 1996; Gottfredson, 1997b). In fact,
standard <span class="t20 hoverable">intelligence</span> tests are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">among</span> the most statistically stable and reliable psychological tests. Furthermore, it is <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> that these scores are a
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> predictor of various things, such as academic performance. The question
then is not whether these tests are useful or measure something meaningful,
but rather whether what they measure is indeed “intelligence”. Some experts
believe that they do, while <span class="t1 t7 t13 hoverable">others</span> think that they only succeed in measuring
<span class="t3 hoverable">certain</span> aspects of, or types of, intelligence.
There are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> properties that a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> test of <span class="t20 hoverable">intelligence</span> should have. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span>
important property is that the test should be repeatable, in the sense that it
consistently returns about the same score for a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> individual. For example,
the test subject should not be able to significantly improve their performance
if tested again a short <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> later. Statistical variability can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be a problem
in short tests. Longer tests help in this regard, <span class="t27 t28 hoverable">however</span> they are naturally
more costly to administer.

11

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> important reliability factor is the bias that <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be introduced by
the individual administering the test. Purely <span class="t23 hoverable">written</span> tests avoid this problem
as there is minimal interaction between the tested individual and the tester.
However, this lack of interaction <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> has disadvantages as it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> mean that
other sources of bias, such as cultural differences, language problems or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span>
something as simple as poor eyesight, <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> not be properly identified. Thus,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> with a <span class="t23 hoverable">written</span> test the individual being tested should <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> be examined
by an expert in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to ensure that the test is appropriate.
Cultural bias in particular is a difficult problem, and tests should be designed to minimise this problem where possible, or at least detect potential
bias problems when they occur. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to do this is to test each ability in
multiple ways, for example both verbally and visually. While language is an
obvious potential source of cultural bias, more subtle forms of bias are difficult to detect and remedy. For example, <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> cultures emphasise <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
cognitive abilities and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> it is difficult, perhaps impossible, to compare <span class="t20 hoverable">intelligence</span> scores in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that is truly objective. Indeed, this choice of emphasis
is a key issue for any <span class="t20 hoverable">intelligence</span> test, it depends on the perspective <span class="t19 hoverable">taken</span> on
what <span class="t20 hoverable">intelligence</span> is.
An <span class="t20 hoverable">intelligence</span> test should be valid in the sense that it appears to be testing
what it claims it is testing for. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to check this is to show that the test
produces results consistent with other manifestations of intelligence. A test
should <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> have predictive power, for example the ability to predict future
academic performance, or performance in other cognitively demanding tasks.
This ensures that what is being measured is somehow meaningful, beyond just
the ability to answer the questions in the test. Standard <span class="t20 hoverable">intelligence</span> tests are
thoroughly tested for <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> on the above criteria, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> others, before they
are ready for wide spread use.
Finally, when testing large numbers of individuals, for example when testing
army recruits, the cost of administering the test becomes important. In these
cases less accurate but more economical test procedures <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be used, such
as purely <span class="t23 hoverable">written</span> tests <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> any direct interaction between the individuals
being tested and a psychologist.
Standard <span class="t20 hoverable">intelligence</span> tests, such as those described in the next section, are
all examples of “static tests”. By this we mean that they test an individual’s
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and ability to solve one-off problems. They do not directly measure
the ability to learn and adapt over time. If an individual was <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> at learning
and adapting then we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> expect this to be reflected in their total <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> be picked up in a static test. However, it <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be that an individual
has a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">great</span> capacity to learn, but that this is not reflected in their <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
due to limited education. In which case, if we consider the capacity to learn
and adapt to be a defining characteristic of intelligence, rather than the sum
of knowledge, then to class an individual as unintelligent due to limited access
to education <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be a mistake.
What is needed is a more direct test of an individual’s ability to learn and
adapt: a so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> “dynamic test”(Sternberg and Grigorenko, 2002) (for re-

12

1.5. <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">Human</span> <span class="t20 hoverable">intelligence</span> tests
lated <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> Johnson-Laird and Wason, 1977). In a dynamic test the
individual interacts over a period of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> with the tester, who now becomes a
kind of teacher. The tester’s task is to present the test subject with a <span class="t14 hoverable">series</span>
of problems. After each attempt at solving a problem, the tester provides
feedback to the individual who then has to adapt their behaviour accordingly
in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to solve the next problem.
Although dynamic tests <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> in theory be very powerful, they are not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span>
<span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> established due to a number of difficulties. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of the drawbacks is that
they require a <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> greater degree of interaction between the test subject and
the tester. This makes dynamic testing more costly to perform and increases
the danger of tester bias.

1.5. <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">Human</span> <span class="t20 hoverable">intelligence</span> tests
The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> modern style <span class="t20 hoverable">intelligence</span> test was developed by the French psychologist Alfred Binet in 1905. Binet believed that <span class="t20 hoverable">intelligence</span> was best studied
by looking at relatively complex mental tasks, unlike earlier tests developed
by Francis Galton which focused on reaction times, auditory discrimination
ability, physical coordination and so on. Binet’s test consisted of 30 short
tasks related to everyday problems such as: naming parts of the body, comparing lengths and weights, counting coins, remembering digits and definitions
of words. For each task category there were a number of problems of increasing difficulty. The child’s results were obtained by normalising their raw score
against peers of the same age. Initially his test was designed to measure
the mental performance of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t16 t17 t18 t19 t20 t22 t23 t24 t26 t29 hoverable">children</span> with learning problems (Binet and Simon,
1905). Later versions were <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> developed for normal <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t16 t17 t18 t19 t20 t22 t23 t24 t26 t29 hoverable">children</span> (Binet, 1911). It
was <span class="t0 t16 t25 t28 hoverable">found</span> that Binet’s test results were a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> predictor of children’s academic
performance.
Lewis Terman of Stanford University developed a version of Binet’s test in
English. As the age norms for French <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t16 t17 t18 t19 t20 t22 t23 t24 t26 t29 hoverable">children</span> did not correspond <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> with
American children, he revised Binet’s test in various ways, in particular he
increased the upper age limit. This resulted in the now famous Stanford-Binet
test (Terman and Merrill, 1950). This test formed the basis of a number of
other <span class="t20 hoverable">intelligence</span> tests, such as the Army Alpha and Army Beta tests which
were <span class="t27 hoverable">used</span> to classify recruits. Since its development, the Stanford-Binet has
been periodically revised, with updated versions being widely <span class="t27 hoverable">used</span> today.
<span class="t4 t22 hoverable">David</span> Wechsler believed that the original Binet tests were too focused on
verbal skills and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> disadvantaged <span class="t3 hoverable">certain</span> otherwise intelligent individuals,
for example the deaf or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> who did not speak the test language as a
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> language. To address this problem, he proposed that tests should contain a combination of both verbal and nonverbal problems. He <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> believed
that in addition to an overall IQ score, a profile should be produced showing the performance of the individual in the various areas tested. Borrowing
significantly from the Stanford-Binet, the US army Alpha test, and others,

13

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
he developed a range of tests targeting specific age groups from preschoolers up to adults (Wechsler, 1958). Due in <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> to problems with revisions of
the Stanford-Binet test in the 1960’s and 1970’s, Wechsler’s tests became the
standard. They continue to be <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> respected and widely used.
Modern versions of the Wechsler and the Stanford-Binet have a similar basic
structure (Kaufman, 2000). Both test the individual in a number of verbal
and non-verbal ways. In the case of a Stanford-Binet the test is broken up
into five key areas: fluid reasoning, knowledge, quantitative reasoning, visualspatial processing, and working memory. In the case of the Wechsler Adult
<span class="t20 hoverable">Intelligence</span> Scale (WAIS-III), the verbal tests include areas such as knowledge,
basic arithmetic, comprehension, vocabulary, and short term memory. Nonverbal tests include picture completion, spatial perception, problem solving,
symbol search and object assembly.
As <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of an effort to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> <span class="t20 hoverable">intelligence</span> tests more culture neutral John
Raven developed the progressive matrices test (Raven, 2000). In this test each
problem consists of a short sequence of basic shapes. For example, a circle
in a box, then a circle with a cross in the middle followed by a circle with
a triangle inside. The test subject then has to select from a <span class="t6 hoverable">second</span> list the
image that best continues the pattern. Simple problems have simple patterns,
while difficult problems have more subtle and complex patterns. In each case,
the simplest pattern that can explain the observed sequence is the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> that
correctly predicts its continuation. Thus, not only is the ability to recognise
patterns tested, but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> the ability to evaluate the complexity of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
explanations and then correctly apply the philosophical principle of Occam’s
razor (see Section 2.1). This will play a key role for us in later chapters.
Today several <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> versions of the Raven test exist designed for <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> age groups and ability levels. As the tests depend strongly on the ability
to identify abstract patterns, rather than knowledge, they are considered to
be some of the most “g-loaded” <span class="t20 hoverable">intelligence</span> tests available (see Section 1.1).
The Raven tests remain in common use today, particularly when it is <span class="t0 t5 t6 t14 t18 hoverable">thought</span>
that culture or language bias <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be an issue. The universality of abstract
sequence prediction tests makes them potentially useful in the context of machine intelligence, indeed we will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that some tests of machine <span class="t20 hoverable">intelligence</span>
<span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> this approach.
The <span class="t20 hoverable">intelligence</span> quotient, or IQ, was originally introduced by Stern (1912).
It was computed by taking the age of a child as estimated by their performance
in an <span class="t20 hoverable">intelligence</span> test, and then dividing this by their <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> biological age and
multiplying by 100. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> a 10 year <span class="t25 hoverable">old</span> child <span class="t3 t4 t5 t6 t8 t10 t11 t12 t13 t14 t18 t19 t21 t23 t24 t25 t26 t27 hoverable">whose</span> mental performance was
equal to that of a normal 12 year old, had an IQ of 120. As the concept of
mental age has now been discredited, and was <span class="t0 t10 t11 t14 t18 hoverable">never</span> applicable to adults anyway, modern IQ scores are simply normalised to a Gaussian distribution with
a mean of 100. The standard deviation <span class="t27 hoverable">used</span> varies: in the United States 15 is
commonly used, while in Europe 25 is common. For <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t16 t17 t18 t19 t20 t22 t23 t24 t26 t29 hoverable">children</span> the normalising
Gaussian is based on peers of the same age.

14

1.6. Animal <span class="t20 hoverable">intelligence</span> tests
Whatever normalising distribution is used, by definition an individual’s IQ
is always an indication of their cognitive performance relative to some larger
group. Clearly this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be problematic in the context of machines where
the performance of some machines <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> orders of magnitude greater
than others. Furthermore, the distribution of machine performance <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be
continually changing due to advancing technology. Thus, for machine intelligence, an absolute measure is more meaningful than a traditional IQ type of
measure.
For an overview of the history of <span class="t20 hoverable">intelligence</span> testing and the structure of
modern tests, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> (Kaufman, 2000).

1.6. Animal <span class="t20 hoverable">intelligence</span> tests
Testing the <span class="t20 hoverable">intelligence</span> of animals is of particular interest to us as it moves
beyond strictly <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> focused concepts of <span class="t20 hoverable">intelligence</span> and testing methods.
Difficult problems in <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span> testing, such as bias due to language
differences or physical handicap, <span class="t8 t13 hoverable">become</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more difficult if we try to compare animals with <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> perceptual and cognitive capacities. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> <span class="t5 t7 hoverable">within</span>
a single species measurement is difficult as it is not always obvious how to
conduct the tests, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> what should be tested for. Furthermore, as humans
devise the tests, there is a persistent danger that the tests <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be biased in
terms of our sensory, motor, and motivational systems (Macphail, 1985). For
example, it is <span class="t6 t10 t15 hoverable">known</span> that rats can learn some types of relationships <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more
easily through smell rather than other senses (Slotnick and Katz, 1974). Furthermore, while an IQ test for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t16 t17 t18 t19 t20 t22 t23 t24 t26 t29 hoverable">children</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> in some sense be validated by its
ability to predict future academic or other success, it is not always clear how
to validate an <span class="t20 hoverable">intelligence</span> test for animals: if survival or the total number of
offspring was a measure of success, then bacteria <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be the most intelligent
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> on earth!
As is often the case when we try to generalise concepts, abstraction is necessary. When attempting to measure the <span class="t20 hoverable">intelligence</span> of lower animals it is necessary to focus on simple <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> short and <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term memory, the forming
of associations, the ability to generalise simple patterns and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> predictions,
simple counting and basic communication. It is only with relatively intelligent
social animals, such as birds and apes, that more sophisticated properties such
as deception, imitation and the ability to recognise oneself are relevant. For
simpler animals, the focus is more on the animal’s essential information processing capacity. For example, the <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> on measuring the capacity of ants to
remember patterns (Reznikova and Ryabko, 1986).
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> interesting difficulty when testing animal <span class="t20 hoverable">intelligence</span> is that we are
unable to directly explain to the animal what its goal is. Instead, we have to
guide the animal towards a problem by carefully rewarding selected behaviours
with something <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> food. In general, when testing machine <span class="t20 hoverable">intelligence</span> we
face a similar problem in that we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> assume that a machine will have a

15

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
sufficient level of language comprehension to be able to understand commands.
A simple solution is to use basic “rewards” to guide behaviour, as we do with
animals. Although this approach is extremely general, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> difficulty is that
solving the task, and simply learning what the task is, <span class="t8 t13 hoverable">become</span> confounded
and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the results need to be interpreted carefully (Zentall, 1997).
For <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> overviews of animal <span class="t20 hoverable">intelligence</span> research <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> (Zentall, 2000), (Herman and Pack, 1994) or (Reznikova, 2007).

1.7. Machine <span class="t20 hoverable">intelligence</span> tests
This section surveys proposed tests of machine intelligence. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that the
measurement of machine <span class="t20 hoverable">intelligence</span> is fundamental to the field of artificial
intelligence, it is remarkable that few researchers are aware of research in this
area beyond the Turing test and some of its variants. Indeed, to the best of
our <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> the survey presented in this section (derived from Legg and
Hutter, 2007b) is the only general survey of tests of machine <span class="t20 hoverable">intelligence</span> that
has been published!
Turing test and derivatives. The classic approach to determining whether
a machine is intelligent is the so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> Turing test (Turing, 1950) which has
been extensively debated over the last 50 <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> (Saygin et al., 2000). Turing
realised how difficult it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to directly define <span class="t20 hoverable">intelligence</span> and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> attempted to side step the issue by setting up his now famous imitation game:
if <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> judges <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> effectively discriminate between a computer and a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> through teletyped conversation then we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> conclude that the computer
is intelligent.
<span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Though</span> simple and clever, the test has attracted <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> criticism. Block
and Searle argue that passing the test is not sufficient to establish <span class="t20 hoverable">intelligence</span>
(Block, 1981; Searle, 1980; Eisner, 1991). Essentially they both argue that a
machine <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> appear to be intelligent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> having any “real intelligence”,
perhaps by using a very large table of answers to questions. While such a
machine <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be impossible in practice, due to the vast size of the table
required, it is not logically impossible. Thus, an unintelligent machine could,
at least in theory, consistently <span class="t5 hoverable">pass</span> the Turing test. Some consider this to
<span class="t22 hoverable">bring</span> the validity of the test into question.
In response to these challenges, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more demanding versions of the Turing
test have been proposed such as the total Turing test in which the machine
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> respond to all forms of input that a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> could, rather than just
teletyped <span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">text</span> (Harnad, 1989). For example, the machine should have sensorimotor capabilities. Going further, the truly total Turing test demands the
performance of not just <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> machine, but of the <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span> “race” of machines
over an extended period of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> (Schweizer, 1998). <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> extension is the
inverted Turing test in which the machine takes the <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> of a judge and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
be able to distinguish between humans and machines (Watt, 1996). Dowe

16

1.7. Machine <span class="t20 hoverable">intelligence</span> tests
argues that the Turing test should be extended by ensuring that the agent
has a compressed representation of the domain area, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> ruling out look-up
table counter arguments (Dowe and Hajek, 1998). Of course these attacks on
the Turing test can be applied to any test of <span class="t20 hoverable">intelligence</span> that considers only a
system’s external behaviour, that is, most <span class="t20 hoverable">intelligence</span> tests.
A more common criticism is that passing the Turing test is not necessary
to establish intelligence. Usually this argument is based on the fact that the
test requires the machine to have a highly detailed model of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
and patterns of thought, making it a test of humanness rather than <span class="t20 hoverable">intelligence</span> (French, 1990; Ford and Hayes, 1998). Indeed, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> small <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span>
pretending to be unable to perform complex arithmetic quickly and faking
<span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> typing errors <span class="t8 t13 hoverable">become</span> important, something which clearly goes against
the purpose of the test.
The Turing test has other problems as well. Current AI systems are a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> from being able to <span class="t5 hoverable">pass</span> an unrestricted Turing test. From a practical
point of view this <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that the full Turing test is unable to offer <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> guidance to our work. Indeed, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> the Turing test is the most famous test
of machine intelligence, almost no current research in artificial <span class="t20 hoverable">intelligence</span> is
specifically directed towards passing it. Simply restricting the domain of conversation in the Turing test to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> the test easier, as is done in the Loebner
competition (Loebner, 1990), is not sufficient. With restricted conversation
possibilities the most successful Loebner entrants are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more focused on faking <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> fallibility, rather than anything resembling <span class="t20 hoverable">intelligence</span> (Hutchens,
1996). Finally, the Turing test returns <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> results depending on who the
<span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> judges are. Its unreliability has in some cases lead to clearly unintelligent machines being classified as human, and at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> instance of a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span>
actually failing a Turing test. When queried about the latter, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the judges
explained that “no <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> being <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> have that amount of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> about
Shakespeare”(Shieber, 1994).

Compression tests. Mahoney has proposed a particularly simple solution to
the binary <span class="t5 hoverable">pass</span> or fail problem with the Turing test: replace the Turing test
with a <span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">text</span> compression test (Mahoney, 1999). In essence this is somewhat
similar to a “Cloze test” where an individual’s comprehension and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
in a domain is estimated by having them guess missing <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t24 t25 t26 t27 t28 hoverable">words</span> from a passage
of text.
While simple <span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">text</span> compression can be performed with symbol frequencies,
the resulting compression is relatively poor. By using more complex models
that capture higher level features such as aspects of grammar, the best compressors are able to compress <span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">text</span> to about 1.5 bits per character for English.
<span class="t27 t28 hoverable">However</span> humans, which can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> use of general <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> knowledge, the
logical structure of the argument etc., are able to reduce this down to about
1 bit per character. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the compression statistic provides an easily com-

17

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
puted measure of how complete a machine’s models of language, reasoning and
domain <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> are, relative to a human.
To <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> the connection to the Turing test, consider a compression test based
on a very large corpus of dialogue. If a compressor <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> perform extremely
<span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> on such a test, this is mathematically equivalent to being able to determine
which sentences are probable at a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t28 t29 hoverable">give</span> point in a dialogue, and which are
not (for the equivalence of compression and prediction <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Bell et al., 1990).
Thus, as failing a Turing test occurs when a machine (or person!) generates a
sentence which <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be improbable for a human, extremely <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> performance
on dialogue compression implies the ability to <span class="t5 hoverable">pass</span> a Turing test.
A recent development in this area is the Hutter Prize (Hutter, 2006). In
this test the corpus is a 100 MB extract from Wikipedia. The idea is that
this should represent a reasonable sample of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> any
compressor that can perform very <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> on this test <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> have a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> model of
not just English, but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> in general.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> criticism of compression tests is that it is not clear whether a powerful
compressor <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> easily translate into a general purpose artificial intelligence.
Also, while a young child has a significant amount of elementary <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
about how to interact with the world, this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be of little use
when trying to compress an encyclopedia full of abstract “adult knowledge”
about the world.
Linguistic complexity. A more linguistic approach is <span class="t19 hoverable">taken</span> by the HAL
project at the company Artificial <span class="t20 hoverable">Intelligence</span> NV (Treister-Goren and
Hutchens, 2001). They propose to measure a system’s level of conversational
ability by using techniques developed to measure the linguistic ability of children. These methods examine <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> such as vocabulary size, length of utterances, response types, syntactic complexity and so on. This <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> allow
systems to be “. . . assigned an age or a maturity level beside their binary Turing test assessment of ‘intelligent’ or ‘not intelligent’ ”(Treister-Goren et al.,
2000). As they consider communication to be the basis of intelligence, and the
Turing test to be a valid test of machine intelligence, in their view the best
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to develop <span class="t20 hoverable">intelligence</span> is to retrace the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> linguistic
development occurs. Although they do not explicitly refer to their linguistic
measure as a test of intelligence, because it measures progress towards what
they consider to be a valid <span class="t20 hoverable">intelligence</span> test, it acts as one.
Multiple cognitive abilities. A broader developmental approach is being
<span class="t19 hoverable">taken</span> by IBM’s Joshua Blue project (Alvarado et al., 2002). In this project
they measure the performance of their system by considering a broad range
of linguistic, social, association and learning tests. Their goal is to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t5 hoverable">pass</span>
what they call a toddler Turing test, that is, to develop an AI system that can
<span class="t5 hoverable">pass</span> as a young child in a similar <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> up to the Turing test.

18

1.7. Machine <span class="t20 hoverable">intelligence</span> tests
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> company pursuing a similar developmental approach based on measuring system performance through a broad range of cognitive tests is the a2i2
project at Adaptive AI (Voss, 2005). Rather than toddler level intelligence,
their current goal is to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> toward a level of cognitive performance similar to that of a small mammal. The idea being that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> a small mammal
has <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of the key cognitive abilities required for <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> level <span class="t20 hoverable">intelligence</span>
working <span class="t16 t21 hoverable">together</span> in an integrated way.
Competitive games. The Turing Ratio method of Masum et al. has more
emphasis on tasks and games rather than cognitive tests. Similar to our own
definition, they propose that “. . . doing <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> at a broad range of tasks is an empirical definition of ‘intelligence’.”(Masum et al., 2002) To quantify this they
seek to identify tasks that measure important abilities, admit a <span class="t14 hoverable">series</span> of strategies that are qualitatively different, and are reproducible and relevant over an
extended period of time. They suggest a system of measuring performance
through pairwise comparisons between AI systems that is similar to that <span class="t27 hoverable">used</span>
to rate players in the international chess rating system. The key difficulty
however, which the authors acknowledge is an open challenge, is to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> out
what these tasks should be, and to quantify just how broad, important and
relevant each is. In our view these are some of the most central problems that
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be solved when attempting to construct an <span class="t20 hoverable">intelligence</span> test. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> we
consider this approach to be incomplete in its current state.
Collection of psychometric tests. An approach <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> Psychometric AI tries
to address the problem of what to test for in a pragmatic way. In the view
of Bringsjord and Schimanski, “Some agent is intelligent if and only if it excels at all established, validated tests of [human] intelligence.”(Bringsjord and
Schimanski, 2003) They later broaden this to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> include “tests of artistic and
literary creativity, mechanical ability, and so on.” With this as their goal, their
research is focused on building robots that can perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> on standard psychometric tests designed for humans, such as the Wechsler Adult <span class="t20 hoverable">Intelligence</span>
Scale and Raven Progressive Matrices (see Section 1.5).
As effective as these tests are for humans, we believe that they are unlikely
to be adequate for measuring machine intelligence. For a start they are highly
anthropocentric. <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> problem is that they embody basic assumptions
about the test subject that are likely to be violated by computers. For example,
consider the fundamental assumption that the test subject is not simply a
collection of specialised algorithms designed only for answering common IQ
test questions. While this is obviously <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> of a human, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> an ape,
it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> not be <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> of a computer. The computer <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be <span class="t15 t25 t26 t29 hoverable">nothing</span> more
than a collection of specific algorithms designed to identify patterns in shapes,
predict number sequences, write poems on a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> subject or solve verbal
analogy problems — all <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> that AI researchers have worked on. Such a
machine <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be able to obtain a respectable IQ score (Sanghi and Dowe,

19

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
2003), <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> outside of these specific test problems it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be next
to useless. If we try to correct for these limitations by expanding beyond
standard tests, as Bringsjord and Schimanski seem to suggest, this once again
opens up the difficulty of exactly what, and what not, to test for. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span>
we consider Psychometric AI, at least as it is currently formulated, to only
partially address this central question.

C-Test. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> perspective <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">among</span> psychologists is that <span class="t20 hoverable">intelligence</span> is “the ability to deal with complexity”(Gottfredson, 1997b). Thus, in a test of intelligence, the most difficult questions are the ones that are the most complex
because these will, by definition, require the most <span class="t20 hoverable">intelligence</span> to solve. It follows then that if we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> formally define and measure the complexity of test
problems using complexity theory we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> construct a formal test of intelligence. The possibility of doing this was perhaps <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> suggested by Chaitin
(1982). While this path requires numerous difficulties to be dealt with, we
believe that it is the most natural and offers <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> advantages: it is formally
motivated and precisely defined, and potentially <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be <span class="t27 hoverable">used</span> to measure
the performance of both computers and biological systems on the same scale
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> the problem of bias towards any particular species or culture.
The C-Test consists of a number of sequence prediction and abduction problems similar to those that appear in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> standard IQ tests (HernándezOrallo, 2000b). This test has been successfully applied to humans with interesting results showing a positive correlation between individual’s IQ test scores
and C-Test scores (Hernández-Orallo and Minaya-Collado, 1998; HernándezOrallo, 2000a). Similar to standard IQ tests, the C-Test always ensures that
each question has an unambiguous answer in the sense that there is always <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
hypothesis that is consistent with the observed pattern that has significantly
lower complexity than the alternatives. Other than making the test easier to
score, it has the added advantage of reducing the test’s sensitivity to changes
in the reference machine <span class="t27 hoverable">used</span> to define the complexity measure.
The key difference to sequence problems that appear in standard <span class="t20 hoverable">intelligence</span> tests is that the questions are based on a formally expressed measure of
complexity. As Kolmogorov complexity is not computable (see Section 2.5),
the C-Test instead uses Levin’s related Kt complexity (Levin, 1973). In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span>
to retain the invariance property of Kolmogorov complexity, Levin complexity
requires the additional assumption that the universal Turing machines are able
to simulate each other in linear time, for example, pointer machines. As <span class="t6 t9 t18 t26 hoverable">far</span>
as we know, this is the only formal definition of <span class="t20 hoverable">intelligence</span> that has so <span class="t6 t9 t18 t26 hoverable">far</span>
produced a usable test of intelligence.
To illustrate the C-Test, below are some example problems <span class="t19 hoverable">taken</span>
from (Hernández-Orallo and Minaya-Collado, 1998). Beside each question is
its complexity, naturally more complex patterns are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> more difficult:

20

1.7. Machine <span class="t20 hoverable">intelligence</span> tests

Complexity
9
12
14

Sequence Prediction Test
Sequence
a, d, g, j, , . . .
a, a, z, c, y, e, x, , . . .
c, a, b, d, b, c, c, e, c, d, , . . .

Answer
m
g
d

Complexity
8
10
13

Sequence Abduction Test
Sequence
a, , a, z, a, y, a, . . .
a, x, , v, w, t, u, . . .
a, y, w, , w, u, w, u, s, . . .

Answer
a
y
y

Our main criticism of the C-Test is that it does not require the agent to be
able to deal with problems that require interacting with an environment. For
example, an agent <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> have a very high C-Test score due to being a very
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> sequence predictor, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> be unable to deal with more general kinds
of problems. This falls short of what is required by our informal definition of
intelligence, that is, the ability to achieve goals in a wide range of environments.
Smith’s Test. <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> complexity based formal definition of <span class="t20 hoverable">intelligence</span> that
appeared recently in an unpublished report is due to Smith (2006). His approach has a number of connections to our work, indeed Smith states that his
<span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> is largely a “. . . rediscovery of recent <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> by Marcus Hutter”. Perhaps
this is over stating the similarities because while there are some connections,
there are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> important differences.
The basic structure of Smith’s definition is that an agent faces a <span class="t14 hoverable">series</span> of
problems that are generated by an algorithm. In each iteration the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
try to produce the correct response to the problem that it has been given. The
problem generator then responds with a score of how <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> the agent’s answer
was. If the agent so desires it can submit <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> answer to the same problem.
At some point the agent requests the problem generator to move onto the next
problem and the score that the agent received for its last answer to the current
problem is then added to its cumulative score. Each interaction cycle counts
as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> step and the agent’s <span class="t20 hoverable">intelligence</span> is then its total cumulative score
considered as a function of time. In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to keep <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> feasible, the problems
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> all be in the complexity class P, that is, decision problems which can
be solved by a deterministic Turing machine using a polynomial amount of
computation time.
We have <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> main criticisms of Smith’s definition. Firstly, while for practical reasons it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> sense to restrict problems to be in P, we do not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span>
why this practical restriction should be a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the very definition of intelligence. If some breakthrough meant that agents <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> solve difficult problems
in not just P but sometimes <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> in the larger complexity class NP, then surely
these <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> agents <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be more intelligent? We had similar objections to in-

21

1. <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Nature</span> and Measurement of <span class="t20 hoverable">Intelligence</span>
formal definitions of machine <span class="t20 hoverable">intelligence</span> that included efficiency requirements
in Section 1.3.
Our <span class="t6 hoverable">second</span> criticism is similar to that of the C-Test. Although there is some
interaction between the agent and the environment, this interaction is rather
limited. The problem-answer format of the test is too limited to fully test an
agent’s capabilities.
The final criticism is that while the definition is somewhat formally defined,
it still leaves open the important question of what exactly the individual tests
should be. Smith suggests that researchers should dream up tests and then
contribute them to some common pool of tests. As such, this <span class="t20 hoverable">intelligence</span> test
is not fully specified.

1.8. Conclusion
Although this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> provides only a short treatment of the complex topic
of intelligence, for a <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> on artificial <span class="t20 hoverable">intelligence</span> to devote more than a few
paragraphs to the topic is rare. We believe that this is a mistake: if artificial
<span class="t20 hoverable">intelligence</span> research is <span class="t29 hoverable">ever</span> to produce systems with real intelligence, questions
of what <span class="t20 hoverable">intelligence</span> actually <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> and how to measure it in machines need
to be <span class="t19 hoverable">taken</span> seriously. At present practically nobody is doing this. The reason,
it appears, is that the definition and measurement of <span class="t20 hoverable">intelligence</span> are viewed
as being too difficult. We accept that the topic is difficult, <span class="t27 t28 hoverable">however</span> we do not
accept that the topic is so difficult as to be hopeless and best avoided. As we
have seen in our survey of definitions, there are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> commonalities across
the various proposals. This leads to our informal definition of <span class="t20 hoverable">intelligence</span> that
we argue captures the essence of these. Furthermore, although <span class="t20 hoverable">intelligence</span>
tests for humans are widely treated with suspicion by the public, by various
metrics these tests have proven to be very effective and reliable when correctly
applied. This gives us hope that useful tests of machine <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be possible. At present only a handful of researchers are working on
these problems, mostly in obscurity. No doubt these fundamental issues will
someday return to the fore when the field is more advanced.

22

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
Having reviewed what <span class="t20 hoverable">intelligence</span> is and how it is measured, we now turn
our attention to artificial systems that appear to be intelligent, at least in
theory. The problem is that although machines and algorithms are becoming
progressively more powerful, as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> no existing system can be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">said</span> to have
<span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> <span class="t20 hoverable">intelligence</span> — they simply lack the power, and in particular the breadth,
to really be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> intelligent. However, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">among</span> theoretical models which are
free of practical concerns such as computational resource limitations, intelligent machines can be defined and analysed. In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we introduce a
very powerful theoretical model: Hutter’s universal artificial <span class="t20 hoverable">intelligence</span> agent,
<span class="t6 t10 t15 hoverable">known</span> as AIXI.
A full treatment of this topic requires a significant amount of technical mathematics. The goal here is to explain the foundations of the topic and some of
the key results in the area in a relatively easy to understand fashion. For the
full details, including precise mathematical definitions, proofs and connections
to other fields, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> (Hutter, 2005), or for a more condensed presentation (Hutter, 2007b). At this point the reader <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> wish to browse Appendix A that
describes the mathematical notation and conventions <span class="t27 hoverable">used</span> in this thesis.

2.1. Inductive inference
Inductive inference is the process by which <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> observes the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> and then
infers the causes behind what has been observed. This is a key process by which
we try to understand the universe and so naturally examples of it abound.
Indeed <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> of science can be viewed as a process of inductively inferring
natural causes. For example, at a microscopic level, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> <span class="t1 t2 t3 t5 t6 t7 t10 t11 t13 t15 t16 t17 t18 t19 t20 t21 t22 t24 t28 t29 hoverable">fire</span> sub-atomic
particles into a gas chamber, observe the patterns they trace out, and then
try to infer what the underlying principles are that govern these events. At
a larger scale <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> observe that global temperatures are changing along
with other atmospheric conditions, and from this information attempt to infer
what processes <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be driving climate change.
Science is not the only domain where inductive inference is important. A
businessman <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> observe stock prices over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> and then attempt to infer a
model of this process in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to predict the market. A parent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> return
home from <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> to discover a chair propped against the refrigerator with the
cookie jar on top a little emptier. Whether we are a detective trying to catch
a thief, a scientist trying to discover a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> physical law, or a businessman

23

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
attempting to understand a recent change in demand, we are all in the process
of collecting information and trying to infer the underlying causes.
Formally we can abstract the inductive inference problem as follows: An
agent has observed some data D := x1 , x2 , . . . xt and has a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of hypotheses
H := h1 , h2 , . . ., some of which <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> models of the unknown process
µ that is generating D. The task is to decide which hypothesis, or hypotheses
in H are the most likely to accurately reflect µ. For example, x1 , x2 , . . . <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span>
be the market value of a stock over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> and H <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> consist of a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of
mathematical models of the stock price. Once we have identified which model
or models are likely to accurately describe the price behaviour, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> want
to use this information to predict future stock prices. Typically this is the
case: Often our goal is not just to understand our observations, but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> to
be able to predict future observations. It is in prediction that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> models
<span class="t8 t13 hoverable">become</span> truly useful.
Inductive inference has a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> history in philosophy. An early thinker on
the subject was the Greek philosopher Epicurus (342? B.C. – 270 B.C.) who
noted that there typically exist <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> hypotheses which are consistent with all
of the available data. Logically then, we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> use the data to rule out any
of these hypotheses; they <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> all be kept as potential explanations. This is
<span class="t6 t10 t15 hoverable">known</span> as Epicurus’ principle of multiple explanations and is often stated as,
Keep all hypotheses that are consistent with the data.
To illustrate this, consider the cookie jar example again and <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> yourself
in the position of the parent returning home from work. Having observed the
chair by the refrigerator and missing cookies, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> seemingly likely hypothesis
is that your daughter has pushed the chair over to the refrigerator, climbed on
top of it, and then removed some cookies. <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> hypothesis is that a hungry
but unusually short thief picked the lock on the back door, saw the cookie jar
and decided to move the chair over to the refrigerator in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to get some
cookies. Although this seems <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> less likely, you <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> completely rule out
this possibility, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more elaborate explanations, based solely on the scene
in the kitchen. Philosophically this leaves you in the uncomfortable situation
of having to consider all sorts of strange explanations as being theoretically
possible <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the information available. The need to keep these hypotheses
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t8 t13 hoverable">become</span> clear if you were then to walk into the living room and notice that your <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> television and other expensive items were <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> missing —
suddenly the unlikely seems more plausible.
Although we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> accept that all hypotheses which are consistent with the
observed facts should be considered at least possible, it is intuitively clear
that some hypotheses are <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more likely than others. For example, if you
had previously observed similar techniques being employed by your daughter
to access the cookie jar, but had <span class="t0 t10 t11 t14 t18 hoverable">never</span> been burgled, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be natural to
consider that the small thief in question was of your own flesh and blood,
rather than a career criminal. However, you are basing this judgement on
your experience prior to returning home. What if you really had very little

24

2.2. Bayes’ rule
prior knowledge? How then should you judge the relative likelihood of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
hypotheses?
2.1.1 Example. Consider the following sequence:
1, 3, 5, 7
What process do you think is generating these numbers? What do you predict
will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> next?
An obvious hypothesis is that these are the positive odd numbers. If this
is <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> then the next number is going to be 9. A more complex hypotheses
is that the sequence is being generated by the equation 2n − 1 + (n − 1)(n −
2)(n − 3)(n − 4) for n ∈ N. In this case the next number <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be 33. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span>
when <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> are aware that this equation generates a sequence consistent with
the digits above, most <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> not consider it to be very likely at all.
3
The philosophical principle behind this intuition was <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> clearly stated by
the English logician and Franciscan friar, William of Ockham (1285 – 1349,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> spelt Occam). He argued that when inferring a <span class="t1 t3 t9 t15 t20 t21 t26 t29 hoverable">cause</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> should not
include in the explanation anything that is not strictly required to explain the
observations. Or as originally stated, “entia non sunt multiplicanda praeter
necessitatem”, which translates as “entities should not be multiplied beyond
necessity”. A more modern and perhaps clearer expression is,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Among</span> all hypotheses consistent with the observations, the simplest
is the most likely.
This philosophical principle is <span class="t6 t10 t15 hoverable">known</span> as Occam’s razor as it allows <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> to
cut <span class="t0 t1 t2 t3 t4 t6 t8 t10 t11 t12 t13 t14 t15 t16 t17 t20 t21 t22 t24 t25 t26 t27 t28 hoverable">away</span> unnecessary baggage from explanations. If we consider the number
prediction problem again, it is clear that the principle of Occam’s razor agrees
with our intuition: the simple hypothesis seemed to be more likely, a priori,
than the complex hypothesis. As we saw in the previous chapter, the ability
to apply Occam’s razor is a standard feature of <span class="t20 hoverable">intelligence</span> tests.

2.2. Bayes’ rule
Although fundamental, the principles of Epicurus and Occam by themselves
are insufficient to provide us with a mechanism for performing inductive inference. A major step forward in this direction <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">came</span> from the English mathematician and Presbyterian minister, Thomas Bayes (1702 – 1761).
In inductive inference we seek to find the most likely hypothesis, or hypotheses, <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the data. Expressed in terms of probability, we seek to find h ∈ H
such that the probability of h <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> D, <span class="t23 hoverable">written</span> P (h|D), is high. From the
definition of conditional probability, P (h|D) := P (h ∩ D)/P (D). Rearranging
this we get P (h|D)P (D) = P (h ∩ D) = P (D|h)P (h), from which it follows
that,
P (D|h)P (h)
P (D|h)P (h)
=P
.
P (h|D) =
′
′
P (D)
′
h ∈H P (D|h )P (h )
25

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
This equation is <span class="t6 t10 t15 hoverable">known</span> as Bayes’ rule. It allows <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> to compute the probability of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> hypotheses h ∈ H <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the observed data D, and a distribution
P (h) over H. The probability of the observed data, P (D), is <span class="t6 t10 t15 hoverable">known</span> as the
evidence. P (h) is <span class="t6 t10 t15 hoverable">known</span> as the prior distribution as it is the distribution over
the space of hypotheses before taking into account the observed data. The
distribution P (h|D) is <span class="t6 t10 t15 hoverable">known</span> as the posterior distribution as it is the distribution after taking the data into account. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> in essence, Bayes’ rule takes
some beliefs that we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have about the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> and updates these <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span>
to some observed data. In the above formulation we have assumed that the
<span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of hypotheses H is countable. For uncountable sets the sum is replaced by
an integral.
Despite its elegance and simplicity, Bayesian inference is controversial. To
this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">day</span> professional statisticians can be roughly divided into Bayesians who
accept the rule, and classical statisticians who do not. The debate is a subtle
and complex <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> and there are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> positions <span class="t5 t7 hoverable">within</span> each of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span>
camps. At the core of the debate is the very notion of what probability means,
and in particular what the prior probability P (h) means.
How can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> talk about the probability of a hypothesis before seeing any
data? <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> if this prior probability is meaningful, how can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> what
its value is? We need to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> this question seriously because in Bayes’ rule
the choice of prior affects the relative values of P (h|D) for each h, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
influences the inference results. Indeed, if Bayesians are free to choose the
prior over H, how can they claim to have objective results?
Bayesians respond to this in a number of ways. Firstly, they point out that
the problem is generally small, in the sense that with a reasonable prior and
quantity of data, the posterior distribution P (h|D) depends almost entirely
on D rather than the chosen prior P (h). In fact on any sizable data set, not
only does the choice of prior not especially matter, but Bayesian and classical
statistical methods typically produce similar results, as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect. It
is only with relatively small data sets or complex models that the choice of
prior becomes an issue.
If classical statistical methods <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> avoid the problem of prior bias when
dealing with small data sets then this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be a significant argument in their
favour. <span class="t27 t28 hoverable">However</span> Bayesians argue that all systems of inductive inference that
obey some basic consistency principles define, either explicitly or implicitly,
a prior distribution over hypotheses. Thus, methods from classical statistics
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> assumptions that are in effect equivalent to defining a prior. The difference is that in Bayesian statistics these assumptions <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> of an
explicit prior distribution. In other words, it is not that prior bias in Bayesian
statistics is necessarily any better or worse than in classical statistics, it is
simply more transparent.
In practical terms, if priors <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be avoided, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> strategy to reduce the
potential for prior selection abuse is to use <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> priors whenever possible. To this <span class="t7 t10 t15 t25 hoverable">end</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> standard prior distributions have been developed. The
key desirable property is that a prior should not strongly influence the poste-

26

2.3. Binary sequence prediction
rior distribution and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> unduly affect the inference results. This <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that
the prior should express a high degree of impartiality by treating the various
hypotheses somewhat equally. For example, when the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> H is finite, an obvious choice is to assign equal prior probability to each hypothesis, formally,
1
for all h ∈ H. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Things</span> <span class="t8 t13 hoverable">become</span> more problematic in infinite hyP (h) := |H|
pothesis spaces as it is then mathematically impossible to assign an equal finite
probability to each of the hypotheses in H, and still have P (H) = 1. Some
Bayesians abandon this condition and use so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> improper priors which are
not <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> probability distributions. For the classical statistician, such a radical
departure from the definition of probability does not really solve the problem of the unknown prior, rather it suggests that something is fundamentally
amiss.
Instead of mathematical tricks or other workarounds, what Bayesians <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
ideally <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> is to solve the unknown prior problem once and for all by having
a universal prior distribution. Only then <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> the Bayesian approach be
truly complete. The principles of Epicurus and Occam provide some hints on
how this <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be done. From Epicurus, whenever a hypothesis is consistent
with the data, that is P (D|h) > 0, we should keep this hypothesis by having
P (h|D) > 0. From Bayes’ rule this requires that ∀h ∈ H : P (h) > 0. From
Occam we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that P (h) should decrease with the complexity of h, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
we need a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to measure the complexity of hypotheses. <span class="t27 t28 hoverable">However</span> before
continuing with this, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> consider the inference problem from <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span>
perspective.

2.3. Binary sequence prediction
An alternate characterisation of inductive inference can be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> in terms of
binary sequence prediction. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> reason this is useful is that binary sequences
and strings provide a more natural setting in which to deal with issues of computability. The problem can be formulated as follows: There is an unknown
probability distribution µ over the space of binary sequences B∞ . From this
distribution a sequence ω is drawn <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> bit at a time. At <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t ∈ N we have
observed the initial string ω1:t := ω1 ω2 . . . ωt , and our task is to predict what
the next bit in the sequence will be, that is, ωt+1 . To do this we select a
model, or models, from a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of potential models that explain the observed
sequence so <span class="t6 t9 t18 t26 hoverable">far</span> and that, we hope, will be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> at predicting future bits in
the sequence.
In terms of inductive inference the observed initial binary string ω1:t is the
observed data D, and our <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of potential models of the data is the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of
hypotheses H. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> to find a model ν ∈ H, or models, that are as
close as possible to the unknown <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> model of the data µ, in the sense that ν
will allow us to predict future bits in the sequence as accurately as possible.
We begin by clarifying what we mean by a probability distribution. In mathematical statistics a probability distribution is <span class="t6 t10 t15 hoverable">known</span> as a probability measure

27

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
as it belongs to the class of functions <span class="t6 t10 t15 hoverable">known</span> as measures. Over the space of
binary strings these can be defined as follows:
2.3.1 Definition.
that,

A probability measure is a function ν : B∗ → [0, 1] such
ν(ǫ) = 1,
∀x ∈ B

∗

ν(x) = ν(x0) + ν(x1).

In this thesis we will interpret ν(x) to mean the probability that a binary
sequence sampled <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the distribution ν begins with the string x ∈ B∗ .
As all strings and sequences begin with the null string ǫ, by definition, the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span>
condition above simply <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> that the ν probability that a sequence belongs to
the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of all sequences is 1. The <span class="t6 hoverable">second</span> condition <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> that the ν probability
that a sequence begins with string x0, plus the ν probability that it begins with
x1, is equal to the ν probability that it begins with x. This makes sense <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span>
that all sequences that begin with x <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> have either a 0 or a 1 as their next
bit and so we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect the probabilities of these sets of sequences to add
up. This style of notation for measures will be convenient for our purposes,
<span class="t27 t28 hoverable">however</span> it is somewhat unusual. To <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> how it relates to conventional measure
theory <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Appendix A.
Sequence prediction forms a large <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of this thesis and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> we will often
be interested in what comes next in a sequence <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> an initial string. More
precisely, if a sequence ω has been sampled from the distribution µ, and ω
begins with the string y ∈ B∗ , what is the probability that the next bits from
ω will be the string x ∈ B∗ ? For this will we adopt the following notation for
the conditional probability, µ(yx) := µ(yx)/µ(y). The benefit of this notation
will <span class="t8 t13 hoverable">become</span> apparent later when we need to deal with complex interaction
sequences. Not only does it preserve the <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> in which the sequence occurs,
it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> allows for more compact expressions when we need to condition on only
<span class="t3 hoverable">certain</span> parts of a sequence.
As noted earlier in this section, sequence prediction can be viewed as an
inductive inference problem. Thus, we can use Bayes’ rule to estimate how
likely some model ν ∈ H is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the observed sequence ω1:t :

 P (ω 1:t |ν)P (ν)
ν(ω 1:t )P (ν)
=P
.
P ν ω1:t =
P (ω 1:t )
̺∈H ̺(ω 1:t )P (̺)

2.3.2 Example. Consider the problem of inferring whether a coin is a normal fair coin based on a sample of coin flips. To simplify things, assume
that the coin is either heads on both sides, tails on both sides, or a normal fair coin. Further assume that t = 4 and we have the observed data
D = head, head, head, head. In terms of binary sequence prediction, the outcome of t coin tosses can be expressed as a string ω1:t ∈ Bt , with each tail being
represented as a 0 bit, and each head as a 1 bit. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> we have ω1:4 = 1111.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> H be the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of models consisting of the distributions νp (ω 1:t ) := pr (1 −
	

Pt
p)t−r , where p ∈ 0, 12 , 1 and r := i=1 ωi is the number of observed heads.
28

2.3. Binary sequence prediction
As
	 are just <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> models in H, assume a uniform prior, that is, ∀p ∈
 there
0, 21 , 1 <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> P (νp ) := 13 . Now from Bayes’ rule,
P (ν 21 |ω1:4 = 1111) =

1
3
1
3

h

04 (1 − 0)0 +


1 4
2

1 4
2

1−
1−


1 0
2

1 0
2

+ 14 (1 − 1)0

i =

1
.
17

Similarly, P (ν0 |ω1:4 = 1111) = 0 and P (ν1 |ω1:4 = 1111) = 16
17 . <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the results
clearly point towards the coin being double headed, as we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect having
just observed four heads in a row.
3

More complex examples <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> involve data collected from medical measurements, music or weather satellite images. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Good</span> models <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> then have to
describe biological processes, music styles, or the dynamics of weather systems
respectively. In each case the binary string representing D <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be simply the
string of bits as they <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> appear in a computer file. However, finding a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span>
prior over such spaces is not trivial. Furthermore, actually computing Bayes’
rule and finding the most likely models, as we did in the example above, can
<span class="t8 t13 hoverable">become</span> very computationally difficult and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> need to be approximated. In
any case, Bayes’ rule at least tells us how to solve the induction problem in
theory, so <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> as we have a prior distribution.
Rather than just estimating which model or models are the most likely, we
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be interested in actually predicting the sequence. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> possibility is to
calculate the probability that the next bit is a 1 based on the most likely model.
The full Bayesian approach, however, is to consider each possible model ν ∈ H
and weight the prediction <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> by each <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to how confident we are
about each model, i.e. P (ν|ω1:t ). This gives us the mixture model predictor,
P (ω1:t 1)

=

X

P (ν|ω1:t ) ν(ω1:t 1)

ν∈H

=

X ν(ω )P (ν) ν(ω 1)
P (ω 1:t 1)
1:t
1:t
=
.
P (ω 1:t )
ν(ω 1:t )
P (ω 1:t )

ν∈H

As we can see, the Bayes mixture predictor reduces to the definition of conditional probability. This has removed the prior over H, and in its <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> we now
have the related prior over D, in this setting the space of binary sequences.
The fact that we can use <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> prior to define the other <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span>
unknown priors are in fact <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> perspectives on the same fundamental problem
of specifying our prior knowledge.

29

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
2.3.3 Example. Continuing Example 2.3.2, we can compute the prior distribution over sequences from the prior distribution over H,
X
ν(ω 1:t )P (ν)
P (ω 1:t ) =
ν∈H

=
=

"
#
 t 
t−r
1
1 t
1
1−
0 (1 − 0)t−r +
+ 1t (1 − 1)t−r
3
2
2
" 
#
2t−r
1
1
+ δtr .
3
2

Pt
where r := i=1 ωi is the number of observed heads, and the Kronecker delta
symbol δab is defined to be 1 if a = b, and 0 otherwise. Thus, <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that
ω1:4 = 1111, <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the mixture model the probability that the next bit
is a 1 is,
i
h 

1
1 2(5)−5
1 5
+
δ
5,5
+1
3
2
33
2
i = 4
.
=
P (11111) = h 2(4)−4
1
1
1
34
+1
+ δ4,4
2
3
2
3

2.4. Solomonoff’s prior and Kolmogorov
complexity
In the 1960’s Ray J. Solomonoff (1926–) investigated the problem of inductive
inference from the perspective of binary sequence prediction (Solomonoff 1964;
1978). He was interested in a very general <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> of the problem, specifically,
learning to predict a binary sequence that has been sampled from an arbitrary
unknown computable distribution. Solomonoff defined his prior distribution
over sequences as follows: The prior probability that a sequence begins with
a string x ∈ B∗ is the probability that a universal Turing machine running
a randomly generated program computes a sequence that begins with x. By
randomly generated, we mean that the bits of the program have a uniform
distribution, for example, they <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> from flipping a fair coin. Formally,
2.4.1 Definition. The Solomonoff prior probability that a sequence begins
with the string x ∈ B∗ is,
X
M (x) :=
2−ℓ(p) ,
p:U (p)=x∗

where U (p) = x∗ <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that the universal Turing machine U computes an
output sequence that begins with x ∈ B∗ when it runs the program p, and ℓ(p)
is the length of p in bits.

30

2.4. Solomonoff’s prior and Kolmogorov complexity
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that the 2−ℓ(p) term in this definition comes from the fact that the
probability of p under a uniform distribution halves for each additional bit.
We will assume that U is a prefix universal Turing machine. This <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span>
that no valid program for U is a prefix of any other. More precisely, if p, q ∈ B∗
are valid programs on U , then there does not exist a string x ∈ B∗ such that
p = qx. Prefix universal Turing machines have technical properties that we
will need and so throughout this thesis we will assume that U is of this type.
Technically, U is actually a type of prefix universal Turing machine <span class="t6 t10 t15 hoverable">known</span> as
a monotone universal Turing machine (see Section 5.1). For the moment we
can safely gloss over these details.
2.4.2 Example. Rather than a classic universal Turing machine running a
program specified by a binary string on an input tape, it is often more intuitive
to think in terms of a program <span class="t23 hoverable">written</span> in a high level programing language
that is being executed on a real computer. Indeed, if a computer had infinite
memory and <span class="t0 t10 t11 t14 t18 hoverable">never</span> broke down it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be technically equivalent to a universal
Turing machine. Consider a short program in C that prints a binary sequence
of all 1’s:
main(){while(1)printf("1");}
As <span class="t6 t9 t18 t26 hoverable">far</span> as C programs go, this is nearly as simple as they get. This is not
surprising <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that the output is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> very simple. If we want a program that
generates a more complex sequence, such as an infinite sequence of successive
digits of the mathematical constant π = 3.141592 . . ., such a program <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
be at least ten times as long. It follows then that the probability of randomly
generating a program that outputs all 1’s is <span class="t6 t9 t18 t26 hoverable">far</span> higher than the probability of
randomly generating a program that computes π. Thus, Solomonoff’s prior
assigns <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> higher probability to the sequences of all 1’s than to the sequence
for π. More complex sequences <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> require still larger programs and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> lower prior probability.
3
Although Solomonoff’s definition requires <span class="t15 t25 t26 t29 hoverable">nothing</span> more than random bits
being fed into a universal Turing machine, we can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the resulting distribution over sequences neatly formalises Occam’s razor. Specifically, sets
of sequences that have short programs, and are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> in some sense simple,
are <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> higher prior probability than sets of sequences that have only <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span>
programs.
The idea that the complexity of a sequence is related to the length of the
shortest program that generates the sequence motivates the following definition:
2.4.3 Definition. The Kolmogorov complexity of a sequence ω ∈ B∞ is,
K(ω) := min∗ { ℓ(p) : U (p) = ω},
p∈B

31

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
where U is a prefix universal Turing machine. If no such p exists, we define
K(ω) = ∞. For a string x ∈ B∗ , we define K(x) to be the length of the
shortest program that outputs x and then halts.
Kolmogorov complexity has <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> powerful theoretical properties and is a
central ingredient in the theory of universal artificial intelligence. Its most
important property is that the complexity it assigns to strings and sequences
does not depend too <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> on the choice of the universal Turing machine U .
This comes from the fact that universal Turing machines are universal in the
sense that they are able to simulate each other with a constant number of
additional input bits. Thus, if we change U above to some other universal
Turing machine U ′ , the minimal value of ℓ(p) and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> K(x), can only change
by a bounded number of bits. This bound depends on U and U ′ , but not on
x.
The biggest problem with Kolmogorov complexity is that the value of K
is not in general computable. It can only be approximated from above. The
reason for this is that in general we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> find the shortest program to compute a string x on U due to the halting problem. Intuitively, there <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> exist
a very short program p∗ such that U (p∗ ) = x, <span class="t27 t28 hoverable">however</span> we do not <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> this
because p∗ takes such a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> to run. Nevertheless, in theoretical applications the simplicity and theoretical <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> of Kolmogorov complexity often
outweighs this computability problem. In practical applications Kolmogorov
complexity is approximated, for example by using a compression algorithm to
estimate the length of the shortest program (Cilibrasi and Vitányi, 2005).

2.5. Solomonoff-Levin prior
Besides the prior described in the previous section, Solomonoff <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> suggested
to define a universal prior by taking a mixture of distributions (Solomonoff,
1964). In the 1970’s this alternate approach was generalised and further developed (Zvonkin and Levin, 1970; Levin, 1974). As Leonid Levin (1948–)
played an important role in this, here we will refer to this as the SolomonoffLevin prior. It is closely related to the universal prior in the previous section:
they lie <span class="t5 t7 hoverable">within</span> a multiplicative constant of each other and share key technical
properties.
Although taking mixtures is perhaps less intuitive, it has the advantage
of making important theoretical properties of the prior more transparent. It
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> gives an explicit prior over both the hypothesis space and the space of
sequences. The topic is quite technical, <span class="t27 t28 hoverable">however</span> it is worth spending some
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> on as it lies at the <span class="t0 t1 t4 t8 t9 t10 t11 t12 t16 t19 t22 t23 t25 t26 hoverable">heart</span> of universal artificial intelligence.
The hypotheses we have been working with up to now have all been probability measures. These can be generalised as follows:
2.5.1 Definition. A semi-measure is a function ν : B∗ → [0, 1] such that,
ν(ǫ) ≤ 1,
32

2.5. Solomonoff-Levin prior
∀x ∈ B∗

ν(x) ≥ ν(x0) + ν(x1).

Intuitively <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> think of a semi-measure that is not a probability measure
as being a kind of defective probability measure <span class="t3 t4 t5 t6 t8 t10 t11 t12 t13 t14 t18 t19 t21 t23 t24 t25 t26 t27 hoverable">whose</span> probabilities do not
quite add up as they should. This defect can be fixed in the sense that a
semi-measure can be built up to be a probability measure by appropriately
normalising things.
Intuitively, a function is enumerable if it can be progressively approximated
from below. More formally, f : X → R is enumerable if there exists a computable function g : X × N → Q such that ∀x ∈ X, ∀i ∈ N : gi+1 (x) ≥ gi (x)
and ∀x ∈ X : limi→∞ gi (x) = f (x). Enumerability is weaker than computability because for any x ∈ X we only <span class="t29 hoverable">ever</span> have the lower bound gi (x) on the
value of f (x). <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> we can <span class="t0 t10 t11 t14 t18 hoverable">never</span> <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> for sure how <span class="t6 t9 t18 t26 hoverable">far</span> our bound is from the
<span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> value of f (x), that is, we do not <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> how large f (x) − gi (x) <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be.
If a similar condition holds, but with the approximation function converging
to f from above rather than below, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that f is coenumerable. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span>
example of such a function is the Kolmogorov complexity function K in the
previous section. If a function is both enumerable and coenumerable, then
we have both upper and lower bounds and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> can compute the value of f
to any required accuracy. In this case we simply <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that f is a real valued
computable function. Clearly then, the enumerable functions are a superset of
the computable functions.
Our task is to construct a prior distribution over the enumerable semimeasures. To do this we need to formalise Occam’s razor, and for that we
need to define a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to measure the complexity of enumerable semi-measures.
Solomonoff measured the complexity of sequences <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the length of
their programs, here we can do something similar.
By definition, all enumerable functions can be approximated from below
by a computable function. Thus, it is not too hard to prove that the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of
enumerable functions can be indexed by a Turing machine, and that this can be
further restricted to just the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of enumerable semi-measures. More precisely,
there exists a Turing machine T that for any enumerable semi-measure µ there
exists an index i ∈ N such that ∀x ∈ B∗ : µ(x) = νi (x) := limk→∞ T (i, k, x)
with T increasing in k. In effect, the index i is a description of µ in that once
we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> i we can approximate the value of µ from below for any x by using
the Turing machine T . As k increases, these approximations increase towards
the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> value of µ(x). For details on how all this is done <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 4.5
of (Li and Vitányi, 1997) or (Legg, 1997). The main thing we will need is the
computable enumeration of enumerable semi-measures itself, which we will
denote by Me := ν1 , ν2 , ν3 , . . .. As all probability measures are semi-measures
by definition, and all computable functions are enumerable, it follows that
the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of enumerable semi-measures is a superset of the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of computable
probability measures. We write Mc to denote an enumeration of just the
computable measures.

33

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> uses of the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> “enumerable” above. When we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span>
can be enumerated, what we mean is that there exists a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to step <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> all
the elements in this set. When we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that an individual function is enumerable, such as a semi-measure, what we mean is that it can be approximated
from below by a <span class="t14 hoverable">series</span> of computable functions. Some authors avoid this
dual usage by referring to enumerable functions as lower semi-computable. In
this terminology, what Me provides is a computable enumeration of the lower
semi-computable semi-measures. In any case, it is still a mouthful!
We can now return to the question of how to measure the complexity of
an enumerable semi-measure. As noted above, for νi ∈ Me the index i is in
effect a description of νi . At this point, it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> seem that the natural thing
to do is to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the value of an enumerable semi-measure’s index to be its
complexity. The problem, however, is that some extremely large index values,
such as 21000 , contain a lot less information than <span class="t6 t9 t18 t26 hoverable">far</span> smaller index values which
are not as easily described: for example, an index <span class="t3 t4 t5 t6 t8 t10 t11 t12 t13 t14 t18 t19 t21 t23 t24 t25 t26 t27 hoverable">whose</span> binary representation
is a string of 100 random bits. The solution is that we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> measure not
the value, but the information content of the index in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to measure the
complexity of the enumerable semi-measure it describes. We do this by taking
the Kolmogorov complexity of the index. That is, we define the complexity
of an enumerable semi-measure to be the length of the shortest program that
computes its index. Formally,
2.5.2 Definition. The Kolmogorov complexity of µ ∈ Me is,
K(µ) := min∗ { ℓ(p) : U (p) = i },
p∈B

where µ is the ith element in the recursive enumeration of all enumerable
semi-measures Me , and U is a prefix universal Turing machine.
In essence this is just an extension of the Kolmogorov complexity function for
strings and sequences (Definition 2.4.3), to enumerable semi-measures. Indeed,
all the key theoretical properties of the complexity function remain the same.
We can again <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> echos of Solomonoff’s prior for sequences in that enumerable
semi-measures that can be described by short programs are considered to be
simple, while ones that require <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> programs are complex.
Having defined a suitable complexity measure for enumerable semimeasures, we can now construct a prior distribution over Me in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that
is similar to what Solomonoff did. As each enumerable semi-measure µ has
some shortest program p ∈ B∗ that specifies its index, we can <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> the prior
probability of µ to be the probability of randomly generating p by flipping a
coin to get each bit. Formally:
2.5.3 Definition. The algorithmic prior probability of µ ∈ Me is,
PMe (µ) := 2−K(µ) .

34

2.5. Solomonoff-Levin prior
As K is coenumerable, from the definition above we can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that PMe is
enumerable. Thus, this distribution can only be approximated from below.
With K as the definition of hypothesis complexity, PMe clearly respects
Occam’s razor as each hypothesis µ ∈ Me is assigned a prior probability that
is a decreasing function of its complexity. Furthermore, <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> enumerable
semi-measure has some shortest program that specifies its index and so ∀µ ∈
Me : K(µ) > 0 and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> ∀µ ∈ Me : PMe (µ) > 0. It follows then that for an
induction system based on Bayes’ rule and the prior PMe , the value of P (ν|D)
will be non-zero whenever D is consistent with ν, that is, P (D|ν) > 0. Such
systems will not discard hypotheses that are consistent with the data and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
respect Epicurus’ principle of multiple explanations.
With a prior over our hypothesis space Me , we can now <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> a mixture to
define a prior over the space of sequences, just as we did in Example 2.3.3:
2.5.4 Definition. The Solomonoff-Levin prior probability of a binary sequence beginning with the string x ∈ B∗ is,
ξ(x) :=

X

ν∈Me

PMe (ν) ν(x).

Clearly this distribution respects Occam’s razor as sets of sequences which
have high probability under some simple distribution ν, will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> have high
probability under ξ, and vice versa.
It is easy to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the presence of just <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> semi-measure in the above
mixture is sufficient to <span class="t1 t3 t9 t15 t20 t21 t26 t29 hoverable">cause</span> ξ to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be a semi-measure, rather than a probability measure. Furthermore, it can be proven that ξ is enumerable but not
computable. Thus, we have that ξ ∈ Me . The fact that ξ is not a probability measure is not too <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> of a problem because, as mentioned earlier, it is
possible to normalise a semi-measure to convert it into a probability measure.
In situations where we need a universal probability measure the normalised
version of ξ is useful. Its main drawback is that it is no longer enumerable,
and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> no longer a member of Me . In most theoretical applications it is
usual to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with the plain ξ as defined above.
A fundamental result is that the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> priors are strongly related:
2.5.5 Theorem. The Solomonoff prior M and Solomonoff-Levin prior ξ lie
×
<span class="t5 t7 hoverable">within</span> a multiplicative constant of each other. That is, M = ξ.
Due to this relation, in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> theoretical applications the differences between
the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> priors are unimportant. Indeed, it is their shared property of dominance that is the key to their theoretical power:
2.5.6 Definition. For some <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of semi-measures M, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that ν ∈ M is
dominant if ∀̺ ∈ M there exists a constant c̺ > 0 such that ∀x : c̺ ν(x) ≥
×
̺(x). Or more compactly, ∀̺ ∈ M : ν ≥ ̺.
35

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
It is easy to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that ξ is dominant over the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of enumerable semi-measures
from its construction: for x ∈ B∗ we have ξ(x) ≥ PMe (µ) µ(x) = 2−K(µ) µ(x).
×
It follows then that ∀µ ∈ Me , ∀x ∈ B∗ : 2K(µ) ξ(x) ≥ µ(x). As M = ξ, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span>
that M is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> dominant. We call distributions that are dominant over large
spaces, such as Me , universal priors. This is due to their extreme generality
and performance as prior distributions, something that we will explore in the
next <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> sections: firstly in the context of Bayesian theory in general, and
then in the context of sequence prediction.

2.6. Universal inference
As we saw in Section 2.2, Bayes’ rule partially solves the induction problem
by providing an equation for updating beliefs. This is only a partial solution
because it leaves open <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> important issues: how do we choose the class of
hypotheses H, and what prior distribution PH should we use over this class?
Various principles and methods have been proposed to solve these problems,
<span class="t27 t28 hoverable">however</span> they tend to run into trouble, especially for large H. In this section
we will look at how taking the universal prior PMe over the hypothesis space
Me theoretically solves these problems.
When approaching an inductive inference problem from a Bayesian perspective, the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> step is to define H. The most obvious consideration is that H
should be large enough to contain the correct hypothesis, or at least a sufficiently close one. Being forced to expand our initial H due to <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> evidence
is problematic as both the redistribution of the prior probabilities, and the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which H is extended, can bias the induction process. In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to avoid
these problems, we should <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> sure that H is large enough to contain a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span>
hypothesis to start with. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> solution is to simply choose H to be very large,
as we did in the previous section where we <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> H = Me . As this contains all
computable stochastic hypotheses, a larger hypothesis space should <span class="t0 t10 t11 t14 t18 hoverable">never</span> be
required.
Having selected H, the next problem is to define a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> prior distribution
over this space. Essentially, a prior distribution PH over a space of hypotheses
H is an expression of how likely we think <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> hypotheses are before taking
the data into account. If this prior <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> is easily quantifiable we can use
it to construct a prior. In the case of PMe this simply <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> taking the
conditional <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> of the Kolmogorov complexity function and conditioning on
this prior information.
More often, however, we either have insufficient prior information to construct a prior, or we simply wish the data to ‘speak for itself’. The latter
case is important when we want to present our findings to <span class="t1 t7 t13 hoverable">others</span> who <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span>
not share our prior beliefs. The standard solution is to select a prior that
is in some sense neutral about the relative likelihood of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> hypotheses.
This is <span class="t6 t10 t15 hoverable">known</span> as the indifference principle. It is what we applied in the coin

36

2.6. Universal inference
estimation problem in Example 2.3.2 when we <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> P (νp ) := |H|−1 = 31 . That
is, we simply assumed that a priori all hypotheses were equally likely.
The indifference principle <span class="t9 t16 t26 hoverable">works</span> <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> for small discrete H, <span class="t27 t28 hoverable">however</span> if we
extend the concept to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> small continuous parametrised classes by defining
a probability density function in terms of the volume of H, problems start to
arise. Consider again the coin problem, <span class="t27 t28 hoverable">however</span> this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> allow the bias of
the coin to be θ ∈ [0, 1]. By the indifference principle the prior is the uniform
probability density P (νθ ) = 1. Now consider what happens if we look at this
coin estimation problem
in a <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> way, where the parameter of interest
√
is actually θ′ := θ. Obviously, if we <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> a uniform prior over θ′ this is
not equivalent to taking a uniform prior over θ. In other words, the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in
which we view a problem, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which we parametrise it, affects
the prior probabilities assigned by a ‘uniform prior’. Obviously this is not as
neutral and objective as we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> like.
Consider how the algorithmic prior probability PMe behaves under a simple
reparametrisation. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> H := {νθ ∈ Mc : θ ∈ Θ} be a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of probability
measures indexed by a parameter θ ∈ Θ, and define θ′ := f (θ) where f is
a computable bijection. It is an elementary fact of Kolmogorov complexity
+
+
theory that K(f (θ)) < K(θ) + K(f ), and similarly K(f −1 (θ′ )) < K(θ′ ) +
+
K(f ), from which it follows that K(θ) = K(θ′ ). With a straight forward
extension, the same argument can be applied to the Kolmogorov complexity
+
of the indexed measures, resulting in K(νθ ) = K(νθ′ ). From Definition 2.5.3
we then <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that,
PMe (νθ ) := 2−K(νθ ) = 2−K(νθ′ ) =: PMe (νθ′ ).
×

That is, for any bijective reparametrisation f the algorithmic prior probability
assigned to parametrised hypotheses is invariant up to a multiplicative constant. If f is simple this constant is small and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> quickly washes out in the
posterior distribution, leaving the inference results essentially unaffected by
the change.
A more difficult version of the above problem occurs when the transformation is non-bijective. For example, define the <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> parameter θ′ := (θ − 21 )2 .
Now θ = 14 and θ = 34 both correspond to the same value of θ′ . Unlike
in the bijective case, non-bijective transformations <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t1 t3 t9 t15 t20 t21 t26 t29 hoverable">cause</span> problems for
finite discrete hypothesis spaces. For example, we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> have <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> hypotheses, H3 := {heads biased, tails biased, fair}. Alternatively, we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span>
regroup to have just <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> hypotheses, H2 := {biased, fair}. Both H3 and
H2 cover the full range of possibilities for the coin. However, a uniform prior
over H3 assigns a prior probability of 31 to the coin being fair, while a uniform
prior over H2 assigns a prior probability of just 12 to the same thing.
The standard Bayesian approach is to try to find a symmetry group for the
problem and a prior that is invariant under group transformations. However,
in some cases there <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be no obvious symmetry, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if there is the
resulting prior <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be improper, <span class="t6 t13 t14 t25 t27 hoverable">meaning</span> that the area under the distribution

37

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
is no longer 1. Invariance under group transformations is a highly desirable but
difficult property to attain. Remarkably, under simple group transformations
PMe can be proven to be invariant, again up to a small multiplicative constant.
For a proof of this, as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as further powerful properties of the universial prior
distribution, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> the paper that this section is based on (Hutter, 2007a).

2.7. Solomonoff induction
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> a prior distribution ξ over B∞ , it is straightforward to predict the continuation of a binary sequence using the same approach as we <span class="t27 hoverable">used</span> in Section 2.3.
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> prior distribution ξ and the observed string ω1:t ∈ B∞ from a sequence
ω ∈ B∞ that has been sampled from an unknown computable distribution
µ ∈ Mc , our estimate of the probability that the next bit will be 0 is,
ξ(ω1:t 0) =

ξ(ω 1:t 0)
.
ξ(ω 1:t )

Is this predictor based on ξ any good? By definition, the best possible
predictor <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be based on the unknown <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> distribution µ that ω has been
sampled from. That is, the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> probability that the next bit is a 0 <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> an
observed initial string ω1:t is,
µ(ω1:t 0) =

µ(ω 1:t 0)
.
µ(ω 1:t )

As this predictor is optimal by construction, it can be <span class="t27 hoverable">used</span> to quantify the
relative performance of the predictor based on ξ. For example, consider the
expected squared error in the estimated probability that the tth bit will be a
0:
X
2
µ(x) ξ(x0) − µ(x0) .
St =
x∈Bt−1

If ξ is a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> predictor, then its predictions should be close to those <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> by
the optimal predictor µ, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> St will be small.
Solomonoff (1978) was able to prove the following remarkable convergence
theorem:
2.7.1 Theorem. For any computable probability measure µ ∈ Mc ,
∞
X
t=1

St ≤

ln 2
K(µ).
2

That is, the total of all the prediction errors over the length of the infinite sequence ω is bounded by a constant. This implies rapid convergence for
any unknown hypothesis that can be described by a computable distribution
(for a precise analysis <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Hutter, 2007a). This <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> includes all computable

38

2.8. Agent-environment model
hypotheses over binary strings, which is essentially the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of all <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> defined
hypotheses. If it were not for the fact that the universal prior ξ is not computable, Solomonoff induction <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be the ultimate all purpose universal
predictor.
Although we will not present Solomonoff’s proof, the following highlights
the key step required to obtaining the convergence result. For any probability
measure µ the following relation can be proven,
n
X
t=1

St ≤

µ(x)
1 X
µ(x) ln
.
2
ξ(x)
n
x∈B

This in fact holds for any semi-measure ξ, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> no special properties of the
universal distribution have been <span class="t27 hoverable">used</span> up to this point in the proof. Now, by the
universal dominance property of ξ, we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that ∀x ∈ B∗ : ξ(x) ≥ 2−K(µ) µ(x).
Substituting this into the above equation,
n
X
t=1

St ≤

X
µ(x)
ln 2
ln 2
1 X
=
µ(x) =
µ(x) ln −K(µ)
K(µ)
K(µ).
2
2
2
2
µ(x)
n
n
x∈B

x∈B

As this holds for all n ∈ N, the result follows. It is this application of
dominance to obtain powerful convergence results that lies at the <span class="t0 t1 t4 t8 t9 t10 t11 t12 t16 t19 t22 t23 t25 t26 hoverable">heart</span> of
Solomonoff induction, and indeed universal artificial <span class="t20 hoverable">intelligence</span> in general.
Although Solomonoff induction is not computable and is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> impractical,
it nevertheless has <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> connections to practical principles and methods that
are <span class="t27 hoverable">used</span> for inductive inference. Clearly, if we define a computable prior rather
than ξ, we recover normal Bayesian inference. If we define our prior to be uniform, for example by assuming that all models have the same complexity, then
the result is maximum a posteriori (MAP) estimation, which in turn is related
to maximum likelihood (ML) estimation. Relations can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be established to
Minimum Message Length (MML), Minimum Description Length (MDL), and
Maximum entropy (ME) based prediction (see <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 5 of Li and Vitányi,
1997). Thus, although Solomonoff induction does not yield a prediction algorithm itself, it does provide a theoretical framework that can be <span class="t27 hoverable">used</span> to
understand various practical inductive inference methods. It is a kind of ideal,
but unattainable, model of optimal inductive inference.

2.8. Agent-environment model
Up to this point we have only considered the inductive inference problem,
either in terms of inferring hypotheses, or predicting the continuation of a
sequence. In both cases the agents were passive in the sense that they were
unable to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> actions that affect the future. Obviously this greatly limits
them. More powerful is the class of active agents which not only observe their
environment, they are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> able to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> actions that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> affect the environment. Such agents are able to explore and achieve goals in their environment.

39

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
observation

reward

agent

environment

action

Figure 2.1.: The agent and the environment interact by sending action, observation and reward signals to each other.
We will need to consider active agents in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to satisfy our definition of intelligence, that is, the ability to achieve goals in a wide range of environments.
Solomonoff induction, although extremely powerful for sequence prediction,
operates in too limited a setting.
2.8.1 Example. Consider an agent that plays chess. It is not sufficient for
the agent to merely observe the other player. The agent actually has to decide
which moves to make, so as to win the game. Of course an important <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of
this will be to carefully observe the other player, infer the strategy they are
using, and then predict which moves they are likely to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> in the future.
Clearly then, inductive inference still plays an important role in the active
case. Now, however, the agent has to somehow <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> this inferred <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
and use it to develop a strategy of moves that will likely lead to winning the
game. This <span class="t6 hoverable">second</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> not be easy. Indeed, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if the agent knew the
other player’s strategy in detail, it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> considerable effort to find a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span>
to overcome the other player’s strategy and win the game.
3
The framework in which we describe active agents is what we call the agentenvironment model. The model consists of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> entities <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> the agent and
the environment. The agent receives input information from the environment,
which we will refer to as perceptions, and sends output information back to
the environment, which we call actions. The environment on the other <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span>
receives actions from the agent as input and generates perceptions as output.
Each perception consists of an observation component and a reward component. Observations are just regular information, <span class="t27 t28 hoverable">however</span> rewards have a
special significance because the goal of the agent is to try to gain as <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span>
reward as possible from the environment. The basic structure of this agentenvironment interaction model is illustrated in Figure 2.1.
The only <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that the agent can influence the environment, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the
rewards it receives, is through its action signals. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> agent is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
that carefully selects its actions so as to <span class="t1 t3 t9 t15 t20 t21 t26 t29 hoverable">cause</span> the environment to generate as
<span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> reward as possible. Presumably such an agent will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> use of any

40

2.8. Agent-environment model
useful information contained in past rewards, actions and observations. For
example, the agent <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> find that <span class="t3 hoverable">certain</span> actions tend to produce rewards
while <span class="t1 t7 t13 hoverable">others</span> do not. In more complex environments the relationship between
the agent’s actions, what it observes and the rewards it receives <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be very
difficult to discover.
The agent-environment model is the framework <span class="t27 hoverable">used</span> in the area of artificial
<span class="t20 hoverable">intelligence</span> <span class="t6 t10 t15 hoverable">known</span> as reinforcement learning. It is equivalent to the controllerplant framework <span class="t27 hoverable">used</span> in control theory, where the controller takes the <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> of
the agent, and the plant is the environment that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be controlled. With a
little imagination, a huge variety of problems can be expressed in this framework: everything from playing a game of chess, to landing an aeroplane, to
writing an award winning novel. Furthermore, the model <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> <span class="t15 t25 t26 t29 hoverable">nothing</span> about
how the agent or the environment work, it only describes their role <span class="t5 t7 hoverable">within</span> the
framework and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> environments and agents are possible.
2.8.2 Example. (Two coins game) To illustrate the agent-model consider
the following game. In each cycle <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> 50¢ coins are tossed. Before the coins
settle the player <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> guess at the number of heads that will result: either 0,
1, or 2. If the guess is correct the player gets to keep both coins and then <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span>
<span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> coins are produced and the game repeats. If the guess is incorrect the
player does not receive any coins, and the game is repeated.
In terms of the agent-environment model, the player is the agent and the
system that produces all the coins, tosses them and distributes the reward
when appropriate, is the environment. The agent’s actions are its guesses at
the number of heads in each iteration of the game: 0, 1 or 2. The observation
is the state of the coins when they settle, and the reward is either $0 or $1.
It is easy to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that for unbiased coins the most likely outcome is 1 head
and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the optimal strategy for the agent is to always guess 1. However,
if the coins are significantly biased it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be optimal to guess either 0 or 2
heads depending on the bias.
3
Having introduced the framework, we now formalise it. The agent sends
information to the environment by sending symbols from some finite alphabet
of symbols, for example, {left, right, up, down}. We call this <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> the action
space and denote it by A. Similarly, the environment sends signals to the agent
with symbols from an alphabet <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> the perception space, which we denote
X . The reward space, denoted by R, is always a subset of the rational unit
interval [0, 1] ∩ Q. Restricting to ration numbers is a technical detail to ensure
that the information contained in each perception is finite. <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Every</span> perception
consists of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> separate parts: an observation and a reward. For example,
we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> have X := {(cold, 0.0), (warm, 1.0), (hot, 0.3)} where the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span>
describes what the agent observes (cold, warm or hot) and the <span class="t6 hoverable">second</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span>
describes the reward (0.0, 1.0 or 0.3).
To denote symbols being sent we use the lower case variable names a, o and
r for actions, observations and rewards respectively. We index these in the

41

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
<span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> in which they occur, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> a1 is the agent’s <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> action, a2 is the <span class="t6 hoverable">second</span> action and so on. The agent and the environment <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> turns at sending
symbols, starting with the agent. This produces a history of actions, observations and rewards which can be written, a1 o1 r1 a2 o2 r2 a3 o3 r3 . . .. As we refer
to interaction histories a lot, we need to be able to represent these compactly.
Firstly, we introduce the symbol x ∈ X to stand for a perception that consists of an observation and a reward. That is, ∀k : xk := ok rk . Our <span class="t6 hoverable">second</span>
trick is to squeeze symbols <span class="t16 t21 hoverable">together</span> and then index them as blocks of symbols. For the complete interaction history up to and including cycle t, we
can write ax1:t := a1 x1 a2 x2 a3 . . . at xt . For the history before cycle t we use
ax<t := ax1:t−1 .
Before this section all our strings and sequences have been binary, now we
have strings and sequences from potentially larger alphabets, such as A and
X . Either we can encode symbols from these alphabets as uniquely identifiable
binary strings, or we can extend our previous definitions of strings, measures
etc. to larger alphabets in the obvious way. In some results technical problems
can arise, for example, it takes some <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> to extend Theorem 2.7.1 to arbitrary
alphabets (Hutter, 2001). Here we can safely ignore these technical issues and
simply extend our previous definitions to general alphabets.
Formally, the agent is a function, denoted by π, which takes the current
history as input and chooses the next action as output. We do not want to
restrict the agent in any way, in particular we do not require that it is deterministic. A convenient <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> of representing the agent then is as a probability
measure over actions conditioned on the complete interaction history. Thus,
π(ax1 a2 ) is the probability of action a2 in the <span class="t6 hoverable">second</span> cycle, <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that the
current history is ax1 . A deterministic agent is simply <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> that always assigns
a probability of 1 to a single action for any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> history. As the history that
the agent can use to select its action expands indefinitely, the agent need not
be Markovian. Indeed, how the agent produces its distribution over actions
for any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> history is left open. In practical artificial <span class="t20 hoverable">intelligence</span> the agent
will of course be a machine and so π will be a computable function. In general
however, the system generating the probabilities for <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> actions <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be
just about anything:√ An algorithm that generates probabilities <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to
successive digits of e, an incomputable function, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> pushing
buttons on a keyboard.
We define the environment, denoted by µ, in a similar way. Specifically, for
all k ∈ N the probability of xk , <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the current interaction history ax<k ak ,
is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by the conditional probability measure µ(ax<k axk ).
Technically, <span class="t4 t18 t22 hoverable">neither</span> µ nor π completely define a measure over the space of
interaction sequences. They only define the conditional probability of <span class="t3 hoverable">certain</span>
symbols <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> an interaction history: π defines the conditional probability over
the actions, and µ of the perceptions. However, <span class="t19 hoverable">taken</span> <span class="t16 t21 hoverable">together</span> they do define
a measure over the interaction sequences that we will denote πµ . Specifically,
we can chain <span class="t16 t21 hoverable">together</span> the conditional probabilities defined by π and µ to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span>

42

2.9. Optimal informed agents
out the probability of any interaction. For example,
π
µ (ax1:2 )

:= π(a1 ) µ(ax1 ) π(ax1 a2 ) µ(ax1 ax2 ).

When we need to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> an expectation over interaction sequences this is the
measure we will use. <span class="t27 t28 hoverable">However</span> in most other cases we will only need the
conditional probabilities defined by π or µ.
2.8.3 Example. To illustrate this formalism, consider again the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Two</span> Coins
Game introduced in Example 2.8.2. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> X := {0, 1, 2} × {0, 1} be the perception space representing the number of heads after tossing the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> coins and
the value of the received reward. Likewise <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> A := {0, 1, 2} be the action
space representing the agent’s guess at the number of heads that will occur.
Assuming <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> fair coins, and recalling that xk := ok rk , we can represent this
environment by defining ∀k ∈ N:
 1
if ak = 0 ∧ ok = 0 ∧ rk = 1,


 4


3

if ak = 0 ∧ ok 6= 0 ∧ rk = 0,
 4



1


if ak = 1 ∧ ok = 1 ∧ rk = 1,

 2
1
if ak = 1 ∧ ok 6= 1 ∧ rk = 0,
µ(ax<k axk ) :=
2



1
 4
if ak = 2 ∧ ok = 2 ∧ rk = 1,




3


if ak = 2 ∧ ok 6= 2 ∧ rk = 0,

4



0
otherwise.

An agent that performs <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in this environment <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be,

1 for ak = 1,
π(ax<k ak ) :=
0 otherwise.

That is, always guess that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> head will be the result of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> coins being
tossed. A more complex agent <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> keep count of how <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> heads occur in
each cycle and then adapt its strategy if it seems that the coins are sufficiently
biased. For example, a Bayesian agent <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> use techniques similar to those
<span class="t27 hoverable">used</span> to predict coin flips in Examples 2.3.2 and 2.3.3.
3

2.9. Optimal informed agents
In the agent-environment model, the agent’s goal is to receive as <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> reward
as possible. Unfortunately, this is not sufficiently precise as there <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span>
possible reward sequences in a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> environment and it is not clear which is
preferred.
2.9.1 Example. Consider the following <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> agents: Agent π 1 immediately
finds a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to get a reward of 0.5 and does so in <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> cycle. Thus, after

43

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
100 cycles it has received a total reward of 50. Agent π 2 , however, spends the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> 90 cycles trying to find the best possible <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to receive reward in each
cycle. During this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> it gets an average reward of 0.1 in each cycle. At
cycle 90 it <span class="t9 t16 t26 hoverable">works</span> out the optimal behaviour and then receives a reward of 1
in <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> cycle thereafter. Thus, after 100 cycles it has received a total reward
of 90 × 0.1 + 10 = 19. In terms of the total reward received after 100 cycles,
π 1 is superior to π 2 . However, after 1,000 cycles this has reversed as π 1 has a
total reward of 500, while π 2 has a total reward of 919.
3
Which of these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> agents is the better one? The answer depends on how
we value reward at <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> points in the future. In some situations we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span>
want our agent to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> quickly, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> more value on short
term rewards. In others, we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> only care that it eventually reaches a level
of performance that is as high as possible, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> relatively high value
on rewards <span class="t6 t9 t18 t26 hoverable">far</span> into the future. Before we can define an optimal agent, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span>
need to formally express our temporal preferences.
A general approach is to weight, or discount, each reward in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that
depends on which cycle it occurs in. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> γ1 , γ2 , . . . be the discountsPwe apply
∞
to the reward in each successive cycle, where ∀i : γi ≥ 0, and
i=1 γi <
∞ in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to avoid infinite weighted sums. Now define the expected future
discounted reward for agent π interacting with environment µ <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> interaction
history ax<t to be,
Vγπµ (ax<t )

:=

E

∞
X
i=t

=

lim

m→∞



γi ri  ax<t

X

axt:m

!

(γt rt + · · · + γm rm ) πµ (ax<t axt:m ).

As the sum is monotonically increasing in m, and finitely upper bounded, the
limit always exists. For t = 1 we drop the interaction history from the notation
and simply write Vγπµ .
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of the most common ways to <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> the discount parameters is to decrease
them geometrically into the future. That is, <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> ∀i : γi := αi for some discount
rate α ∈ (0, 1). By increasing α towards 1 we weight <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term rewards more
heavily, conversely by reducing it we weight them less so. Thus, the parameter
α controls how short term greedy, or <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term farsighted, the agent should
be.
2.9.2 Example. Consider again the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> agents from Example 2.9.1. As the
rewards are deterministic for π 1 we can drop the expectation,
Vγπ

1

µ

=

∞
X
i=1

44

αi 0.5 = 0.5A,

2.9. Optimal informed agents
α
where A := 1−α
is from the standard formulae for geometric series. On the
other <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span> for agent π 2,

Vγπ

2

µ

=

90
X

αi 0.1 +

∞
X

i=91

i=1

αi = 0.1A(1 − α90 ) + Aα90 .

2
Equating the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> and then solving, we find
p that π has higher expected future
90
1
4/9 ≈ 0.991.
3
discounted reward than π when α >

A major advantage of geometric discounting is that it is mathematically
convenient to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with. Indeed, it is what we will use in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 6 for our
reinforcement learning algorithm. In the present context, however, we want
to keep the development fairly general and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> we will leave the structure
of γ unspecified. An <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more general approach is to consider the space of
all bounded enumerable discount sequences. We will <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> this approach when
formally defining <span class="t20 hoverable">intelligence</span> in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 4 as it will allow us to completely
remove γ from the model. Here we will follow the more conventional approach
to AIXI and simply <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> γ to be a free parameter.
Having formalised the agent’s temporal preference in terms of γ, we can now
define the optimal agent:
2.9.3 Definition. The optimal agent for an environment µ and discounting γ is the agent π µ that has maximal expected future discounted reward.
Formally,
π µ := arg max Vγπµ .
π

The superscript µ emphasises the fact that the agent is optimal with respect
to the specific environment µ. This optimality is possible because the agent
was constructed using µ. In a sense the agent knows what its environment
is before it has <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> interacted with it. This is similar to Section 2.7 where
the optimal sequence predictor was defined using the distribution that was
generating the sequence to be predicted.
To understand how the optimal agent π µ behaves in each cycle, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span>
express the value function in a recursive <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> for an arbitrary agent π. From
the definition of V ,
X
Vγπµ (ax<t ) = lim
(γt rt + · · · + γm rm ) πµ (ax<t axt:m )
m→∞

=

X
axt

=

"

Xh
axt

axt:m

lim

X

(γt rt + · · · + γm rm )

m→∞
axt+1:m

γt rt + Vγπµ (ax1:t )

i

π
µ (ax<t

π
µ (ax1:t

axt ).

axt+1:m )

#

π
µ (ax<t

axt )

(2.1)

In the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> step we broke cycle t off both the sum and πµ . As these do not
involve m, we pushed them outside the square brackets and moved the limit

45

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
inside. In the <span class="t6 hoverable">second</span> step we broke off the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> discounted reward and dropped
the sum for this term as it was redundant. The remaining discounted rewards
are just V with t advanced by one, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> producing the desired recursion in V .
This is essentially a discrete <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> of the Bellman equation commonly
<span class="t27 hoverable">used</span> in control theory, finance, reinforcement learning and other fields concerned with optimising dynamic systems (Bellman, 1957; Sutton and Barto,
1998). Usually it is assumed that the environment is Markovian and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> only
a limited history needs to be <span class="t19 hoverable">taken</span> into account. Here, however, we include
the entire interaction history and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> are able to avoid these restrictions on
the environment. Again, this is to keep the model as general as possible.
Consider now how at is chosen by the optimal agent π µ . By definition, the
optimal action is the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> that maximises V . <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Therefore</span> π µ (ax<t at ) = 1 for
the expected future discounted reward maximising action, and zero otherwise
(ties can be broken arbitrarily). P
Thus, after expanding Equation (2.1) with π
replaced by π µ , we can replace at and π µ (ax<t at ) with simply a maximum
over the possible actions,
µ

Vγπ µ (ax<t ) =

i
X Xh
µ
γt rt + Vγπ µ (ax1:t ) π µ (ax<t at ) µ(ax<t axt )
at

= max
at

= max
at

xt

Xh

γt rt + Vγπ

µ

xt

X
xt

· · · max
am

µ

i
(ax1:t ) µ(ax<t axt )

i
Xh
µ
γt rt +· · ·+γm rm +Vγπ µ (ax1:m ) µ(ax<t axt:m )
xm

In the last step we have simply unfolded the recursion in V for the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> m
µ
cycles. As m → ∞ the term Vγπ µ (ax1:m ) → 0. Thus, if we <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the limit
m → ∞ above we can drop V <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> affecting the result. It follows then that
the action <span class="t19 hoverable">taken</span> by the optimal policy π µ in the tth cycle is,
µ

aπt := arg max lim
at

m→∞

X
xt

max
at+1

X

xt+1

· · · max
am

X
xm


γt rt +· · ·+γm rm µ(ax<t axt:m ).

If there is more than <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> maximising action in cycle t, we simply select <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of
these in an arbitrary way.
Intuitively, in the above equation we can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the optimal agent takes
the distribution µ, and in effect does a brute force search through all possible
futures looking for the action in the current cycle that maximises the expected
future discounted reward. The agent knows that the environment will always
respond <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the distribution µ, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> in each cycle it takes the expectation by summing over all the possible observations x and weighting these by
their probability <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to µ. Furthermore, as the agent always follows an
optimal strategy, its own future actions are just a <span class="t14 hoverable">series</span> of value maximising
actions.

46

2.10. Universal AIXI agent

2.10. Universal AIXI agent
Although the agent π µ performs optimally in environment µ, this does not
meet the requirements for <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to our definition adopted in
<span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 1. What we require is a general agent that <span class="t9 t16 t26 hoverable">works</span> <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
environments. Such an agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> learn about its environment by interacting
with it, and then modify its behaviour accordingly in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to optimise performance. Only then will the agent have the kind of adaptability to <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
environments that we require of an intelligent agent.
This problem with the optimal agent π µ is similar to the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> we encountered
in Section 2.7. There the optimal sequence predictor was based on the distribution µ that was actually generating the sequence. Of course, for inductive
inference µ is unknown and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be inferred by observing the sequence. Thus,
basing a predictor on µ <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be “optimal” in terms of prediction performance,
but it is in some sense cheating. Moreover, it is certainly not general as the
predictor is designed for just <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> environment.
Solomonoff’s solution was to replace the unknown µ in the optimal predictor with a universal prior distribution, such as ξ. This produced an extremely powerful universal predictor that rapidly converged to optimal predictions whenever the distribution µ was computable (Theorem 2.7.1). In this
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> Solomonoff solved, at least in theory, the problem of predicting sequences
from unknown distributions. Hutter’s innovation was to do essentially the
same trick for active agents: he <span class="t4 hoverable">took</span> the optimal active agent π µ , described in
the previous section, and replaced the unknown µ with a generalised universal
prior distribution ξ. This produced π ξ , <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t6 t10 t15 hoverable">known</span> as AIXI, which will be
described in this section.
In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to construct π ξ , the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> thing to do is to generalise ξ in Definition 2.5.4 from sequences to active environments. As we saw earlier, an active
environment µ is an enumerable semi-measure conditioned, in chronological
order, on a sequence of actions from an agent. The presence of these actions
causes no problems, indeed the development of a universal distribution over active environments is virtually identical to what we did for sequence prediction
in Section 2.5.
It can be shown that the space of all enumerable chronological semi-measures
can be effectively enumerated. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> E := {µ1 , µ2 , . . .} be such an enumeration.
Define the Kolmogorov complexity of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of these environments to be the
length of the shortest program that computes the environment’s index: just
as we did for distributions over sequences in Definition 2.5.2. This gives us the
universal prior probability of a chronological environment ν ∈ E,
PE (ν) := 2−K(ν) .
From this prior over environments we can construct a prior over the agent’s
observations in a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> interaction history by taking a mixture over the envi-

47

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
ronments,
ξ(ax1:n ) :=

X

2−K(ν) ν(ax1:n ).

ν∈E

It is easy to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that ξ is enumerable because 2−K(ν) is enumerable, as is
each ν in the sum. Furthermore, this sum of chronological semi-measures is
itself a chronological semi-measure. Thus, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that ξ ∈ E. As was the
case for sequences, the dominance property for ξ can easily be seen by taking
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> element from the sum corresponding to the semi-measure to be dominated.
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that we are reusing the symbol ξ. Whether we are talking about ξ defined
over sequences, or over chronological environments, will always be clear from
the context.
To construct the AIXI agent π ξ , simply <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> π µ and replace µ with ξ,
X
X
X

ξ
γt rt +· · ·+γm rm ξ(ax<t axt:m ).
· · · max
aπt := arg max lim
max
at

m→∞

xt

at+1

xt+1

am

xm

This gives us an agent that does not depend on the unknown µ. Replacing
the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> distribution µ in the optimal agent π µ with the universal prior distribution ξ is essentially the same as what we did in Section 2.7 when defining
Solomonoff’s universal predictor. Of course now we are working in the more
general setting of chronological environments rather than just sequences.
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that Solomonoff prediction <span class="t9 t16 t26 hoverable">works</span> so <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> for sequence prediction, we
<span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> expect the agent π ξ defined above to be similarly powerful in chronological environments. To some extent this is the case, <span class="t27 t28 hoverable">however</span> analysing the
performance of universal agents in chronological environments turns out to be
significantly more complex.
Perhaps the most elementary question concerns whether our generalised ξ
converges to the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> environment µ. It turns out that convergence results
can be proven, including a result similar to Solomonoff’s convergence result
generalised to interaction histories. More precisely, it can be proven that the
total µ-expected squared difference between µ and ξ is finite for interaction
histories sampled from π µ interacting with a computable environment µ. Unfortunately, when the interaction history comes from µ interacting with π ξ ,
rather than π µ , we run into trouble. This problem is <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> illustrated by the
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t12 t13 t15 t18 t20 t21 t22 t24 t25 t27 t28 t29 hoverable">Heaven</span> and Hell environment from Section 5.3.2 of (Hutter, 2005):
2.10.1 Example. (Heaven and Hell) Imagine an environment where in
the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle the agent is faced with <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> unmarked doors, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of which <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
be opened. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of these doors leads to “heaven” where the agent receives
plentiful rewards, and the other leads to “hell” where the agent <span class="t0 t10 t11 t14 t18 hoverable">never</span> gets any
reward. Once a door is chosen there is no <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to go back, the agent is stuck
in either <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t12 t13 t15 t18 t20 t21 t22 t24 t25 t27 t28 t29 hoverable">heaven</span> or hell forever.
This is no problem for π µ as it knows µ and so it knows which door to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span>
to get to heaven. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> it always achieves maximal future discounted reward.
The agent π ξ , on the other hand, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> learn through experience. Of course

48

2.10. Universal AIXI agent
once it has the necessary experience to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> the <span class="t27 hoverable">right</span> choice, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> already
be too late. All π ξ can do is to guess which door to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> and hope for the best.
Obviously the expected performance of π ξ will be <span class="t6 t9 t18 t26 hoverable">far</span> below that of π µ .
3
This example does not expose a design flaw in π ξ , in the sense that no
general agent is able to consistently behave optimally in such environments.
For example, consider an environment µ′ with the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> doors switched. Agent
π µ <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> always go to hell in this environment. We <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> define an agent
′
π µ which <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be optimal in µ′ , <span class="t27 t28 hoverable">however</span> it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> always go to hell in µ.
Clearly, no agent <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> behave optimally in both environments <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> being
told what the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> environment was in advance. Matching the performance of
optimal agents in each of their respective environments is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> an impossible
task for any <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> agent. As such, we need to think carefully about what it is
that we want to prove if we are to show that π ξ is indeed a very powerful and
general agent.
We have already seen in Section 2.7 that the above problem does not occur
in the sequence prediction setting. This is because in sequence prediction an
agent’s predictions do not affect the future observed sequence and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> mistakes have no consequences beyond the current cycle. This allows Solomonoff’s
prediction system to be able to learn to perform optimally across the entire
space of computable sequence prediction problems. Such optimising behaviour
is possible in <span class="t3 hoverable">certain</span> other classes of environments. What this suggests then is
that we should focus on classes of environments, such as sequence prediction,
in which it is at least possible for a general agent to learn to behave optimally.
We begin by generalising the AIXI model to <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> classes of environments. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> E be a non-strict subset of the enumeration E of all enumerable
chronological semi-measures. Define the mixture distribution over E,
ζ(ax1:n ) :=

X

2−K(ν) ν(ax1:n ).

ν∈E

Now define the agent π ζ based on ζ, just as we defined π ξ based on ξ. <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that
while π ξ is a single agent, the agent π ζ depends on which class of environments
E we are considering. If E = E then ζ = ξ and so π ζ = π ξ . In this sense π ζ
generalises π ξ .
Perhaps the most elementary property that an optimal general agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
have is that there should not exist any other agent that is strictly superior.
More precisely:
2.10.2 Definition. An agent π is Pareto optimal if there is no other agent
ρ such that ∀µ ∈ E,
Vγρµ (ax<t ) ≥ Vγπµ (ax<t )

∀ax<t ,

with strict inequality for at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> µ.

49

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that Pareto optimality does not rule out the possibility that some other
agent exists which performs better in some environment in E. It simply <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span>
that no other agent exists which is at least as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> in all environments in E,
and strictly better in at least one. For the agent π ζ the following optimality
result can be proven (Section 5.5 of Hutter, 2005):
2.10.3 Theorem. For any E ⊆ E, the agent π ζ is Pareto optimal.
As this holds for any E, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> holds for E = E. Thus, the AIXI agent π ξ is
a Pareto optimal agent over the space of environments E. <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that for any
space of environments E <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> Pareto optimal agents <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> exist.
A stronger result can be proven showing that π ζ is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> balanced Pareto optimal (Hutter, 2005). Essentially, this <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that any increase in performance
in some environment due to switching to <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> agent, is compensated for by
an equal or greater decrease in performance in some other environment.
While these Pareto optimality results are very general, they only succeed
in showing that π ζ is superior, or at least equal, to other general agents over
the same class of environments. The result does not rule out the possibility
that all general agents, including π ζ , typically perform poorly. What we need
is to show that π ζ does indeed learn to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> environments.
The complication, as we saw in Example 2.10.1 above, is that in some types
of environments it is impossible for general agents to perform well. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> we
somehow need to characterise those types of environments in which it is at
least possible for a general agent to perform well. Furthermore, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> when
optimal performance is possible for a general agent, we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> expect such an
agent to perform optimally immediately. We need a performance measure that
gives the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> to learn about the structure of µ through interaction. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to formalise the concept of optimal performance after a period of learning
is the following:
2.10.4 Definition.
ment µ if,

An agent π is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">said</span> to be self-optimising in an environ-

1 πµ µ
1 πµ
Vγ (ax<t ) →
V
(ȧẋ<t )
Γt
Γt γ
with µ probability 1 as t → ∞. Here ax<t is an interaction history sampled
from π interacting with µ, and ȧẋ<t is an interaction history P
sampled from
∞
π µ interacting with µ. The normalisation factor, defined Γt := i=t γi , is the
total discount remaining at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t.

Essentially this <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> that with high probability the performance of the agent
π converges to the performance of the optimal agent. The normalisation is
necessary because the un-normalised expected future discounted reward always
converges to zero. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> it convergence <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> trivially hold for any
agent.
We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that π is self-optimising for the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of environments E if it is selfoptimising for <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> environment in E. Furthermore, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of
50

2.10. Universal AIXI agent
environments E admits self-optimising agents if there exists an agent π that
is self-optimising for E. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> extend the result to non-stationary agents by
<span class="t0 t2 t3 t4 t5 t9 t11 t12 t15 t16 t17 t22 t24 t27 hoverable">saying</span> that a <span class="t14 hoverable">series</span> of agents π1 , π2 , . . . is self-optimising if the above result
holds with π replaced by πt . That is, in the tth cycle agent πt is applied.
The following powerful self-optimising result can be proven (Hutter, 2005):
2.10.5 Theorem. If there exists a sequence of self-optimising agents πm for
a class of environments E, then the agent π ζ is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> self-optimising for E.
Intuitively, this result <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> that the performance of π ζ will converge to optimal performance in any class of environments where this is possible for a
single agent, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if the agent is non-stationary. This is the minimal requirement possible, in the sense that if no self-optimising agent existed for some
class of environments, then trivially π ζ <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be self-optimising in the class
either.
Although this is a strong optimality result, it does have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> limitations.
Firstly, while the result shows that π ξ converges to optimal performance whenever this is possible in a class of environments, it does not tell us how fast the
convergence is. In Solomonoff’s convergence theorem we saw that the convergence of the universal predictor was extremely rapid. Ideally we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> a similar result for active environments. Unfortunately, such a result is
impossible in general:
2.10.6 Example. (Needle in a haystack) Imagine an environment with
N buttons, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of which generates a reward of 1 in <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> cycle when pressed,
and all the rest produce no reward. The location of the correct button <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
<span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> roughly log2 N bits to encode, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> for most values of N we have K(µ) =
O(log2 N ). As π ξ is not informed prior to the start of the game as to which
button generates reward, the best it can do is to press the buttons <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> at
a time. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the expected number of incorrect choices that π ζ will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> is
O(N ) = O 2K(µ) .
3

Compare this to Theorem 2.7.1 for sequence prediction which bounds the
total squared difference in prediction error by O(K(µ)). Here in the active
case the best possible bound for any general agent is exponentially worse at
O 2K(µ) . This is not a design flaw in π ξ as the above limit applies to any general agent. Clearly then, bounds showing rapid convergence are not possible
in general. We can only hope to prove convergence speed bounds for specific
classes of environments. Unfortunately, results in this direction currently exist
only in very simple settings.
This is a significant weakness in the theory of universal agents. Until further
results are proven it is hard to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> just how fast, or slow, convergence to optimal
behaviour is in <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> classes of environments. As it appears that results in
this direction will be difficult, a more elementary result is to establish which
classes of environments at least admit self-optimising agents, and under what
conditions. Once we have established this, by Theorem 2.10.5 it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> then

51

2. Universal Artificial <span class="t20 hoverable">Intelligence</span>
follow that π ζ is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> self-optimising in these environments. If we can show
this for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> classes of environments, it then follows that π ζ is able to perform
<span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in a wide range of environments, at least in the limit.
Although the required analysis is not particularly difficult for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of the
basic classes of environments, sorting out all the definitions, the relationships between them and the conditions under which they admit self-optimising
agents is lengthy and requires some care. This is the subject of the next chapter.

52

3. Taxonomy of Environments
In the previous <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we introduced the AIXI agent π ξ , and its generalisation
to arbitrary spaces of environments, π ζ . Of particular importance was Theorem 2.10.5 which roughly said: For any class of environments for which there
exists a self-optimising agent, the agent π ζ defined over this class is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> selfoptimising. Thus, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to understand the performance of π ζ across a wide
range of environments, we need to understand which classes of environments
admit self-optimising agents, and which do not. In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we present a
partial answer to this question by showing that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> classes of
environments admit self-optimising agents under reasonable conditions.
We begin by formalising some common classes of environments. To do this
we examine the environments’ measures and in particular the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which
they condition on the interaction history. In this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> we characterise and
relate <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> classes of environments, such as Bernoulli schemes,
Markov chains, and Markov decision processes (MDPs). Some interesting <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span>
classes of environments naturally arise from the analysis. We then <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> some
important classes of problems studied in artificial intelligence, such as sequence
prediction and classification, and express these too in terms of the structure of
their measures. This formalisation in terms of chronological measures reveals
that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> classes of environments are special cases of other classes, that is, a
hierarchy of classes of environments exists. Studying this more closely we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span>
that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of these classes are in fact reducible to the class of ergodic MDPs.
Putting all these relationships <span class="t16 t21 hoverable">together</span> produces a taxonomy of classes of
environments.
It is <span class="t6 t10 t15 hoverable">known</span> that <span class="t3 hoverable">certain</span> machine learning algorithms, such as Q-learning,
are self-optimising in the class of ergodic MDPs. As <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of the classes in
our taxonomy are reducible to ergodic MDPs, it follows that these classes
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> admit self-optimising agents. Thus, from Theorem 2.10.5, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that π ζ
converges to optimal behaviour in these classes of environments. In this way,
we clarify our earlier claim that universal agents can learn to behave optimally
in a wide range of environments, as required by our definition of intelligence.

3.1. Passive environments
The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> class of environments we will consider is the class of passive environments. Loosely speaking, such environments are not affected by the agent’s
actions. We will be more interested in the active environments to be described
in later sections.

53

3. Taxonomy of Environments
3.1.1 Definition.
that ∀ax1:k ,

A Bernoulli scheme is an environment (A, X , µ) such
µ(ax<k axk ) = µ(xk ).

As the random variables x1 , x2 , . . . are independent and identically distributed
(i.i.d.), <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> think of a Bernoulli scheme as being an i.i.d. process.
The above definition involves some slight abuse of notation. Essentially,
what we are showing is that an equivalent measure with the same <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">name</span> (on the
<span class="t27 hoverable">right</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span> side) can be defined over a reduced parameter space by dropping the
parameters that have no effect on the value of the original measure (on the left
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span> side). In other words, the equation above indicates that the distribution
µ over xk can be defined in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that it is completely independent of the
history ax<k ak .
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> that we have <span class="t23 hoverable">written</span> (A, X , µ) in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to specify the action and
perception spaces associated with the measure. This will be necessary in this
<span class="t12 t19 t25 t26 t28 hoverable">chapter</span> as we will often need to consider relationships between environments
that differ in their action and perception spaces.
3.1.2 Example.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> simple stochastic processes can be described as
Bernoulli schemes. Imagine a game where a 6 sided die is thrown repetitively.
The agent receives a reward of 1 whenever a 6 is thrown, and 0 otherwise.
There are no actions that the agent can take. Formally, A := {ǫ}, O :=
{1, 2, 3, 4, 5, 6} and R := B. The measure is defined ∀ax1:k ,
 1
for xk = ok rk ∈ {(1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 1)},
6
µ(xk ) :=
0 otherwise.
3
Other than perhaps a constant environment, Bernoulli schemes are about
the simplest environments possible. Despite their simplicity, they are important in statistics where sets of i.i.d. random variables play an important role.
For example, sampling from a population should ideally produce individuals
that are both independent of each other, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> from the same underlying
distribution.
A natural generalisation of Bernoulli schemes is to allow the next perception to depend on the previous observation. This gives us a richer class of
environments where the distribution over perceptions can change with time:
3.1.3 Definition. A Markov chain is an environment (A, X , µ) that is a
Bernoulli scheme ∀ax1 , and ∀ax1:k with k > 1,
µ(ax<k axk ) = µ(ok−1 xk ).
For a Markov chain the last observation completely defines the system’s
state and so these outputs are usually referred to as states. In more general

54

3.1. Passive environments
classes of environments this is not the case, so for consistency we will use our
usual terminology of observations and perceptions. <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that we treat the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle as a special case as there is no previous perception to condition on.
By requiring the system to be a Bernoulli scheme in the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle we ensure
that the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> action has no effect.
3.1.4 Example. Imagine a game where we have a ring shaped playing board
that has been divided into 20 <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> cells. There is a pebble that starts in
cell 1 and moves around the board as follows: On each turn a standard six
sided die is thrown to decide how <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> positions the pebble will be moved
clockwise around the board. In cells 5 and 15 the agent receives a reward of
1. Otherwise the reward is 0.
We can model this system as a Markov chain. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> A := {ǫ}, O :=
{0, 1, 2, . . . , 19}, R := B. For k = 1 the pebble is in cell 1 and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> ∀ax1 ,

1 for o1 = 1 ∧ r1 = 0,
µ(x1 ) :=
0 otherwise.
Now define ∀ax1:k with k > 1,
 1

	
 6 for ok ∈ (ok−1 + 1) mod 20, . . . , (ok−1 + 6) mod 20
∧ rk = δ5,ok + δ15,ok ,
µ(ok−1 xk ) :=

0 otherwise.

In this game there is a 61 chance of obtaining reward if the pebble is currently
in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the six cells before either cell 5 or cell 15.
3

From the definitions of Bernoulli schemes and Markov chains it is clear that
in both of these classes an agent is “passive”, in the sense that its actions have
no effect on the environment’s behaviour. The difference between the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span>
classes is the size of the history that is relevant to determining the next perception. Increasing the length of this history to the full history of observations
gives us the most general class of completely passive environments:
3.1.5 Definition. A Totally Passive Environment is an environment
(A, X , µ) such that ∀ax1:k ,
µ(ax<k axk ) = µ(o<k xk ).
By construction, this class of environments is a superset of the classes of
environments defined <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> far. We <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> define a more limited version of this
class where the next perception only depends on the last n observations, rather
than the full history. However, such an environment is mathematically equivalent to a standard Markov chain where for each history of length n we create
a unique observation in an enlarged observation space. In this <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> space the
environment can then be represented by a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> Markov chain. This reduction technique will be <span class="t27 hoverable">used</span> in a more general setting to prove Lemma 3.2.6.

55

3. Taxonomy of Environments
While totally passive environments are useful in modelling some systems,
in terms of artificial <span class="t20 hoverable">intelligence</span> they are relatively uninteresting because the
agent <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> do anything. We can relax this constraint just a little by only
requiring that the agent <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> affect future observations:
3.1.6 Definition. A Passive Environment is an environment (A, X , µ)
such that ∀ax<k aok ,
µ(ax<k aok ) = µ(o<k ok ).

<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that there are no restrictions on the rewards; the environment is free
to reward or punish the agent in any way. Totally passive environments are
clearly a special case of passive environments. For more on AIXI in passive
environments <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 5.3.2 of (Hutter, 2005).
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> important special case is the class of problems where the agent is
rewarded for correctly predicting a sequence that it <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> influence:
3.1.7 Definition. A Sequence Prediction Problem is a passive environment (A, X , µ) such that ∀ax1:k ,
µ(ax<k aork ) = µ(aork ).
That is, the reward in each cycle depends entirely on the action and the
observation that immediately follows it. As sequence prediction environments
are passive the observations do not depend on the agent’s actions, <span class="t27 t28 hoverable">however</span>
there is no limit on how <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> the relevant observation history can be. The
above definition makes precise what we meant in previous chapters where
sequence prediction problems were referred to as being “passive”. For more on
how AIXI deals with sequence prediction problems <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 6.2 of (Hutter,
2005).
3.1.8 Example. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> A = O := {0, 1, . . . , 9}. Define ∀ax1:k ,
µ(o<k ok ) :=



1 if ok the k th digit in 3.141592 . . . ,
0 otherwise,

and
µ(aork ) :=



1
0

if rk = 1 − 19 |ok − ak |,
otherwise.

Thus, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to maximise reward, the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> generate successive digits
of the mathematical constant π. A correct digit gets a reward of 1, while an
incorrect digit gets a lesser reward proportional to the difference between the
correct digit and the guess.
3

56

3.2. Active environments

3.2. Active environments
The simplest active environment is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> where the next perception xk depends
on only the last action ak :
3.2.1 Definition. A Bandit is an environment (A, X , µ) such that ∀ax1:k ,
µ(ax<k axk ) = µ(axk ).
This class of environments is named after the bandit machines <span class="t0 t16 t25 t28 hoverable">found</span> in casinos around the world, although the relation to real bandit machines is tenuous.
Bandit environments are weaker than Markov chains as future perceptions do
not depend on past observations. However, they do have the ability to react to the last action, and so in this respect they are more powerful. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span>
<span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> bandit problems are conceptually simple, solving them optimally is
surprisingly involved (Berry and Fristedt, 1985; Gittins, 1989).
3.2.2 Example. Imagine a machine that has n <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> levers, or arms, that
the agent can pull. Each arm has a <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> but fixed probability of generating
a reward. For this example, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> the reward be either 0 or 1. The agent’s task
is to figure out which arm to pull so that it maximises its expected reward.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> approach <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to spend <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> pulling <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> arms and collecting
statistics in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to estimate which produces the most reward.
We can formally define this bandit as follows: <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> O := {ǫ}, R := B and
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> A := {1, 2, 3, . . . , n} represent the n <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> arms that the agent can pull.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> β1 , β2 , . . . , βn be the respective probabilities of obtaining a reward of 1
after pulling the corresponding arm. Now define ∀ax1:k ,

β ak
for rk = 1,
µ(axk ) :=
1 − βak for rk = 0.
3
A natural extension to the class of bandits is to allow the next perception
to depend on both the last observation and the last action. This produces a
<span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more powerful class of environments that has been intensively studied
and has <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> theoretical and practical applications:
3.2.3 Definition.
A (stationary) Markov Decision Process (MDP)
is an environment (A, X , µ) that is a Bernoulli scheme ∀ax1 , and ∀ax1:k with
k > 1,
µ(ax<k axk ) = µ(ok−1 axk ).
Usually MDPs are defined in such a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that the agent does not act before
the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> perception. However, in our definition the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle is a Bernoulli
scheme and so the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> action has no effect anyway. Aside from this detail,
our definition is equivalent to the standard definition (Bellman, 1957). It is

57

3. Taxonomy of Environments
immediately clear that this class generalises Bernoulli schemes, bandits and
Markov chains.
What we have defined above is a stationary MDP. This is because the probability of a perception <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the current action and the last perception does
not change, that is, it is independent of k. In some definitions the measure µ
is allowed to vary over time. These non-stationary MDPs can be modelled as
POMDPs, which will be defined shortly.
3.2.4 Example. Consider again the simple Markov chain in Example 3.1.4.
This can be extended to an MDP by allowing the agent to decide whether to
move clockwise or anticlockwise around the board. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> O := {0, 1, . . . , 19}
and R := B as before, but now A := {, 	} as the agent can select which
direction to move in. For k = 1 the pebble is in cell 1 and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> ∀ax1 ,

1 for o1 = 1 ∧ r1 = 0,
µ(ax1 ) :=
0 otherwise.
Now define ∀ax1:k with k > 1,
 1
for ok ∈ {(ok−1 + 1) mod 20, . . . , (ok−1 + 6) mod 20}

6



∧ rk = δ5,ok + δ15,ok ∧ ak =,

1
for ok ∈ {(ok−1 − 1) mod 20, . . . , (ok−1 − 6) mod 20}
µ(ok−1 axk ) :=
6


∧ rk = δ5,ok + δ15,ok ∧ ak =	,



0 otherwise.

3

A natural <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to generalise the class of MDPs is to allow the next perception
to depend on the last n observations and actions:
3.2.5 Definition. A (stationary) n th <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> Markov Decision Process
is an environment (A, X , µ) that is a Bernoulli scheme ∀ax1 , and ∀ax1:k with
k > 1,
µ(ax<k axk ) = µ(ok−m aok−m+1:k−1 axk ),
where m := min{n, k}.
Immediately from the definition we can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that a standard MDP is an nth
<span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDP where n = 1. The added complication of the variable m is to allow
for the situation where the current history length k is less than n.
It <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> appear that nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs are more general than standard MDPs,
<span class="t27 t28 hoverable">however</span> it turns out that any nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDP can be converted into an equivalent MDP. This is done by extending the observation space and appropriately
modifying the measure. The proof follows the same pattern as the reduction
of nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> Markov chains to standard Markov chains, except that now we
have to deal with the complication of having actions in the history.
3.2.6 Lemma. nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs can be reduced to MDPs.

58

3.2. Active environments
Proof. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> (A, X = O × R, µ) be an nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDP. To prove the result
we will define an equivalent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDP (A, Z = Q × R, µ̃). We begin by
defining the <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> observation space,
Q :=

n
[

i=1

O × (A × O)i−1 .

<span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Every</span> interaction history that the nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDP conditions on is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> uniquely
represented in Q. Although it complicates things, we need to include histories
of length less than n to accommodate the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> n − 1 cycles of the system.
Next we define a measure µ̃ over the perception space Z, that is equivalent
to the measure µ over the perception space X . We begin by dealing with the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> interaction cycle: ∀az1 ∈ A × Z define,

µ(az 1 ) for z1 ∈ X ,
µ̃(az 1 ) :=
0
otherwise.

This makes the processes equivalent in the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle.
Next, for interaction cycles k > 1, define ∀qk−1 azk ∈ Q × A × Z,

µ(ok−m aok−m+1:k−1 axk ) if qk−1 = ok−m aok−m+1:k−1



∧ zk = qk rk
µ̃(qk−1 az k ) :=
∧ qk = ok−m+1 aok−m+2:k−1 aok ,



0
otherwise.

That is, if the transition qk−1 azk is possible when represented in the original
environment, then this transition is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the same probability by µ̃ in the
<span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> environment. Any transition qk−1 azk which is impossible in the original
environment, for example because the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> histories represented by qk−1 and
qk are inconsistent, is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> a transition probability of zero by µ̃. Thus, the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> environments have equivalent structure and dynamics.
2

As the proof illustrates, writing down the equation for a higher <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDP
and working with it is cumbersome. Thus, the above result is useful as it <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span>
that we only have to deal with <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs in our analysis. Nevertheless,
conceptually it is often more natural to think of <span class="t3 hoverable">certain</span> problems as being nth
<span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs.
Rather than increasing the history that the measure conditions on, <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span>
extension is to assume that the agent <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> properly observe the MDP’s
outputs:
3.2.7 Definition.
A Partially Observable Markov Decision Process (POMDP) is an environment (A, X = O × R, µ) defined as follows: <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span>
(A, X̃ = Õ × R, µ̃) be an MDP <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> the core MDP. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> φ : X̃ ×X → [0, 1] be a
conditional probability measure of the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> φ(x̃x) which expresses the probability of perceiving x when the core MDP outputs x̃. Define ∀ax1:k ∈ (A×X )k ,
X
µ̃(ax̃1 ) φ(x̃1 x1 ) µ̃(õ1 ax̃2 ) φ(x̃2 x2 ) · · · µ̃(õk−1 ax̃k ) φ(x̃k xk ).
µ(ax1:k ) :=
x̃1:k ∈X̃ k

59

3. Taxonomy of Environments
The <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of POMDPs is perhaps best illustrated by example.
3.2.8 Example. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> the core MDP (A, X̃ , µ̃) be the MDP defined in Example 3.2.4. Now imagine that the agent <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> reliably observe which cell
the pebble is in. To do this, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> X := X̃ and define the observation function
∀x̃, x ∈ X ,
(
61
if x̃ = x,
100
φ(x̃x) :=
1
otherwise.
100
Thus, with probability 0.61 the agent observes the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> output of the core
MDP. The rest of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> it observes some other randomly chosen output.
The probabilities add up as |X | = |O| × |R| = 20 × 2 = 40. Thus, x can <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span>
39 values other than x̃.
3
The fact that POMDPs generalise MDPs can be seen by letting X = X̃ and
φ(x̃x) := δx̃,x , in which case it follows that µ = µ̃. That is, the POMDP reduces
to being its core MDP. Furthermore, as nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs can be reduced to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span>
<span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs, it follows that POMDPs <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> generalise higher <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs.
To <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that POMDPs can define non-stationary MDPs, consider a POMDP
where X is a strict subset of X̃ . Now use this extra internal information in
the core MDP to keep a parameter that varies over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> and that affects the
core MDP’s behaviour. To the external agent who <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> observe this extra
information, it appears that the environment is non-stationary.
Both theoretically and practically this class of environments is difficult to
<span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with. However, it does encompass a huge variety of possibilities, including all of the environments considered in this chapter, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> real <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span>
problems.

3.3. Some common problem classes
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> of the problems considered in artificial <span class="t20 hoverable">intelligence</span> can be expressed as
classes of environments using our measure notation. In this section we will
formalise some of them.
3.3.1 Definition. A Function Maximisation Problem is an environment
(A, X , µ) such that O = R and for some objective function f : A → R we have
∀ax1:k ,

1 if ok = f (ak ) ∧ rk = max{o1 , . . . , ok },
µ(ax<k axk ) :=
0 otherwise.
Essentially, the agent’s actions are interpreted as input to some function,
and in each cycle the result of the function is returned as an observation. We
do not simply return the current value of the function as the reward as this discourages the agent from exploring once a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> value has been found. Rather

60

3.3. Some common problem classes
we return the reward associated with the best value <span class="t0 t16 t25 t28 hoverable">found</span> so far. Obviously,
maximisation problems with <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> ranges, or minimisation problems, can
be expressed by applying a simple transformation to the original objective
function. For more on how AIXI deals with various types of function optimisation problems <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 6.4 of (Hutter, 2005).
3.3.2 Example. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> f (a) := 1 − (a − 41 )2 . To maximise reward the agent
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> generate the action that maximises f , that is, a = 41 .
3
Artificial <span class="t20 hoverable">intelligence</span> often considers environments that consist of some kind
of a game that is repetitively played by the agent. Games such as chess, various
card games, tic-tac-toe and <span class="t1 t7 t13 hoverable">others</span> belong to this class, so <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> as at the <span class="t7 t10 t15 t25 hoverable">end</span>
of each match a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> match is started. This can be formalised as follows:
3.3.3 Definition.
A Repeated Strategic Game is an environment
(A, X , µ) where ∃l ∈ N such that ∀ax1:k ,
µ(ax<k axk ) = µ(aolm:k−1 axk )
where m := ⌊k/l⌋ is the number of the episode when in cycle k, and l is the
episode length.
Clearly, Bernoulli schemes and bandits are repeated strategic games. If we
want to allow games to finish before the episode finishes we can pad the remaining cycles, and perhaps <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> reward the system for padded cycles following
a victory in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to encourage rapid wins. For more on how AIXI deals with
strategic games <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 6.3 of (Hutter, 2005).
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> common type of problem considered in artificial <span class="t20 hoverable">intelligence</span> is classification. A classification problem consists of a domain space W and a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of
classes Z and some function f : W → Z. The agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> try to learn this
mapping based on examples. When <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> cases where the class is missing it
has to correctly guess the class in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to obtain reward. More formally:
3.3.4 Definition. A Classification Problem is an environment (A, X , µ)
<span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> up as follows. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> W and Z be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> sets <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> the attribute space
and the class space respectively. Z includes a special symbol “?” <span class="t27 hoverable">used</span> to
indicate whether the agent needs to guess the class. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> O ⊂ W × Z and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span>
f : W → Z \ {“?”} where ∀ax1:k such that xk−1 = wzrk−1 and xk = wzrk ,
µ(ax<k awk ) = µ(wk ),
and for α ∈ (0, 1),

 α
1−α
µ(wz k ) =

0

if zk = f (wk ),
if zk = “?”,
otherwise,

61

3. Taxonomy of Environments
and


 1
1
µ(zk−1 ak rk ) =

0

zk−1 = “?” ∧ ak = f (wk−1 ) ∧ rk = 1,
zk−1 6= “?” ∧ rk = 0,
otherwise.

The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> condition <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> that the distribution of the points in the attribute
space is independent of the system’s history. In other words, this <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of xk is
a Bernoulli scheme. The <span class="t6 hoverable">second</span> condition <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> that in each cycle the system
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> either provide a training instance or ask for the agent to classify based on
the attribute vector. The parameter α controls how often the agent is asked
to guess the class. The third condition <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> that reward is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> when the
system asks for a classification and the agent guesses it correctly. It is easy to
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that classification problems are passive MDPs.
3.3.5 Example. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> each element of W be a vector of medical measurements
for a patient, and Z some list of diseases. When provided with a list of
patients’ statistics and their diseases, the agent’s job is to learn a function
that determines which disease is present <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> a patient’s medical data.
3
For more on how AIXI deals with supervised learning problems <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 6.5 of (Hutter, 2005).

3.4. Ergodic MDPs
Intuitively, a Markov chain is ergodic if the current observation <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> impose
any <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term constraints on future observations. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that the reward in a
Markov chain only depends on the last observation, being ergodic <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> implies
that the current observation does not <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> any <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term constraints on future rewards either. Although the ergodic property is typically studied in the
context of Markov chains, in this section we will extend the notion to MDPs.
To illustrate the idea, we begin by considering some Markov chains that are
not ergodic.
3.4.1 Example.
Imagine a Markov chain with O := {A, B, C}. The
chain starts with observation A and then transitions to either B or C. In all
subsequent cycles the observation remains the same. Thus, once observation
B has been generated, observation C will <span class="t0 t10 t11 t14 t18 hoverable">never</span> occur. This is a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term
constraint on future observations, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the Markov chain is not ergodic. 3
It is as if the agent has gone through a one-way door into a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the
environment that it can <span class="t0 t10 t11 t14 t18 hoverable">never</span> return from. This is similar to the <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t12 t13 t15 t18 t20 t21 t22 t24 t25 t27 t28 t29 hoverable">Heaven</span>
and Hell environment in Example 2.10.1, except that in the above example
the agent was unable to choose where to go as the environment was passive.
Consider now a slightly more subtle example of non-ergodic behaviour.

62

3.4. Ergodic MDPs
3.4.2 Example. Imagine a Markov chain with O := {A, B}. The environment starts with observation A and then in the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle transitions to B.
In the following cycle it transitions back to state A, and then in the next it
returns to B. In this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> the system alternates between the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> observations.
This environment <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> seem ergodic as both possible observations continue
to occur forever and so the agent clearly has not <span class="t8 t13 hoverable">become</span> confined to just <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the environment. However, if the current observation is A, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
be the case that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> steps into the future the observation will again be
A. In fact, for any <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> number of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> steps into the future the observation
will always be A. As this is a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term constraint on future observations the
environment is not ergodic.
3
The above example can be modified so that it is ergodic: after outputting
observation A <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> it so that there is a 0.1 probability of generating this
again, and a 0.9 probability of outputting B. Thus, no matter what the
current observation is, after <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> steps the environment <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> output
either observation.
We now formalise the proceeding concepts. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> observations
communicate if it is possible to go from <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> observation to the other and
return after some finite number of steps. A communicating class is a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of
observations that all communicate with each other, and do not communicate
with any observations outside this set. If all observations of the Markov chain
belong to the same communicating class, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that the Markov chain is
irreducible. An observation has period k if any return to the observation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
occur in some multiple of k <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> steps and k is the largest number with this
property. For example, if it is only possible to return to an observation in an
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> number of steps, then this observation has period 2. If an observation
has period 1 then it is aperiodic, and if all observations are aperiodic we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span>
that the Markov chain is aperiodic. We can now formally define what it <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span>
for a Markov chain to be ergodic:
3.4.3 Definition. A Markov chain environment (A, X , µ) is ergodic if and
only if it is irreducible and aperiodic.
Consider again the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> examples above. The technical reason the Markov
chain in Example 3.4.1 was not ergodic was because the observations A and
B were not communicating and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the Markov chain was not irreducible. In
Example 3.4.2 the problem was that both observations had period 2 and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
the Markov chain was not aperiodic.
To extend the concept of being ergodic to cover MDPs consider again the
relationship between Markov chains and MDPs. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> µ be an MDP environment,
and π an agent that is conditioned on only the last observation. That is,
∀ax<k ak with k > 1 we have,
π(ax<k ak ) = π(ok−1 ak ).

63

3. Taxonomy of Environments
Now consider the measure πµ that describes how the above environment and
agent interact. In each cycle we have,
π
µ (ax<k axk )

:=
=

X

π(ok−1 ak ) µ(ok−1 axk )
ak ∈A
π
µ (ok−1 xk ).

Thus, if π has the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> above and is fixed, the distribution over the next
perception depends on only the last observation. That is, πµ defines a Markov
chain. In other words, an MDP can be <span class="t0 t5 t6 t14 t18 hoverable">thought</span> of as a Markov chain with the
addition of actions that allow an agent to influence future observations. We
can remove this by taking an appropriate agent and building it into the MDP.
The resulting system no <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> has free actions to be chosen, and reverts back
to being a Markov chain.
Using this relationship, we can now define ergodic MDPs in the natural way:
3.4.4 Definition. An MDP environment (A, X , µ) is ergodic if and only if
there exists an agent (A, X , π) such that πµ defines an ergodic Markov chain.
As higher <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDPs are reducible to MDPs, we will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that a higher <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span>
MDP is ergodic if it is reducible to an ergodic MDP.
Consider again the <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t12 t13 t15 t18 t20 t21 t22 t24 t25 t27 t28 t29 hoverable">Heaven</span> and Hell environment defined in Example 2.10.1.
After the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> interaction cycle the agent is always in either <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t12 t13 t15 t18 t20 t21 t22 t24 t25 t27 t28 t29 hoverable">heaven</span> or hell.
Furthermore, no matter what the agent does, it <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> switch between being
in <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t12 t13 t15 t18 t20 t21 t22 t24 t25 t27 t28 t29 hoverable">heaven</span> or being in hell, it is stuck in its current location for all eternity.
Thus, no matter what agent we select we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> create a Markov chain such
that the observations of being in <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t12 t13 t15 t18 t20 t21 t22 t24 t25 t27 t28 t29 hoverable">heaven</span> communicate with the observations
of being in hell, and so this MDP environment is not ergodic.
The importance of ergodic MDPs for us will be their relationship to learning. In an MDP environment only the current observation and action have
any impact on future perceptions. When the MDP is ergodic, the current observation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> has no <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> term impact on future perceptions that <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be
overcome by taking the <span class="t27 hoverable">right</span> actions. This <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that in an ergodic MDP
environment no matter what mistakes an agent <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> make, there is always a
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to recover from these. Obviously this is a significant property for learning
agents, and indeed the following important result can be proven:
3.4.5 Theorem. Ergodic MDPs admit self-optimising agents.
For references and other details <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Appendix B. As ergodic MDPs admit
self-optimising agents it follows by Theorem 2.10.5 that the universal agent π ζ
defined over the class of ergodic MDPs is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> self-optimising. What remains
to be shown is that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> important classes of environments are in fact special
cases of ergodic MDPs. This is the topic of the next section.

64

3.5. Environments that admit self-optimising agents

3.5. Environments that admit self-optimising
agents
In this section we will prove that some of the environments we have defined
are in fact ergodic MDPs. Thus, by the results in the last section, π ζ is
self-optimising in these environments. Before we begin we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> need to introduce <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> extra property: we need to assume that environments are accessible,
<span class="t6 t13 t14 t25 t27 hoverable">meaning</span> that there are no observations that have zero probability of <span class="t29 hoverable">ever</span> being
observed.
3.5.1 Definition. A chronological environment (A, X , µ) is accessible if
∀ok , ∃ax<k ak such that µ(ax<k ak ok ) > 0.
It is reasonable to assume this because if it is impossible to find any finite
interaction history that gives some observation a non-zero probability then we
can simply remove it from the observation space. This produces an equivalent
environment that is accessible. In particular, this does not interfere with the
property that ergodic MDPs admit self-optimising agents. These unused extra
observations play no role.
3.5.2 Lemma. Bernoulli schemes are ergodic MDPs.
Proof.
Consider a Bernoulli scheme (A, X , µ). From the definition of a
Bernoulli scheme we immediately <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that they are a special case of the MDP
definition. As the environment is accessible, ∀ok , ∃ax<k ak : µ(ax<k ak ok ) > 0.
Applying the definition of a Bernoulli scheme, this reduces to ∀ok : µ(ok ) > 0.
Thus, as the next observation does not depend on observations prior to ok−1 ,
nor does it depend on the actions or rewards, the agent and environment
<span class="t16 t21 hoverable">together</span> define a Markov chain. As all observations are possible at <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> point
in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> it follows that all observations belong to the same communicating class
and are aperiodic. That is, the Markov chain is ergodic.
2
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that the above result holds independent of the agent as the environment
is passive. The same is <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> for classification problems:
3.5.3 Lemma. Classification problems are ergodic MDPs.
Proof. From Definition 3.3.4 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the distribution over the attribute
space W is not dependent on the interaction history, and the distribution over
the class space Z depends only on the attribute in the current cycle and so
it too is independent of anything in prior cycles. More formally, ∀ax<k ak ok :
µ(ax<k ak ok ) = µ(wk ) µ(wk z k ) where ok := wk zk . <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the distribution over
observations is completely independent of prior cycles.
Again from the definition of classification problems we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the distribution over rewards depends on only the observation in the previous interaction
cycle and the action in the current cycle. It follows then that in each cycle the

65

3. Taxonomy of Environments
perception (consisting of an attribute, class and reward) depends on only the
action in the current cycle and the observation in the previous cycle. Thus,
classification problems are MDPs.
As the environment is accessible, ∀ok , ∃ax<k ak : µ(ax<k ak ok ) > 0. Because
the distribution over observations is independent of previous interaction cycles this immediately reduces to ∀ok : µ(ok ) > 0 and so the environment is
ergodic.
2
As classification problems are passive, in the above proof it was again possible to construct an ergodic Markov chain <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> specifying the agent. In
active environments, such as bandits, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> define an appropriate agent:
3.5.4 Lemma. Bandits are ergodic MDPs.
Proof. Consider a bandit (A, X , µ). By definition it is trivially an MDP.
As the environment is accessible ∀ok , ∃ax<k ak : µ(ax<k aok ) > 0. Applying the
definition of a Bandit this reduces to,
∀ok , ∃ak : µ(aok ) > 0.

(3.1)

Next we need to show that there exists an agent under which the agent
interacting with the environment defines an ergodic Markov chain. If we define
1
it follows that ∀ax<k aok ,
an agent ∀ak : π(ak ) := |A|
π
µ (ax<k aok )

:=

X

ak ∈A

π(ak )µ(aok ) =

1 X
µ(aok ) =:
|A|

π
µ (ok ).

ak ∈A

From Equation 3.1 it then follows that for each ok at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the terms
in the above sum is non-zero. Thus, ∀ok : πµ (ok ) > 0 and so πµ is an ergodic
Markov chain and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">therefore</span> µ is an ergodic MDP.
2
Unfortunately, repeated strategic games are not ergodic MDPs. The problem
is that there <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be observations which can only occur at <span class="t3 hoverable">certain</span> points in each
episode, for example at the start or the end. Clearly then <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> define
an agent such that these observations have period 1, making it impossible to
construct an ergodic Markov chain. Nevertheless, through a change of action
and perception spaces a repeated strategic game can be converted into an
equivalent system which is a bandit. Bandits, as we saw above, are ergodic
MDPs. For our purposes such a conversion is sufficient as it allows these
environments to admit self-optimising agents.
3.5.5 Lemma. Repeated strategic games are reducible to ergodic MDPs.
Proof.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> (A, X , µ) be a repeated strategic game with episode length
l. Now define a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> action space Ã := Al . In this <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> action space <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span>
combination of actions that an agent can <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> in a single episode of the game
is represented by a single action. Similarly, define a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> perception space that

66

3.6. Conclusion
represents each episode, X̃ := X l , and now define a chronological measure µ̃
over the <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> spaces such that ∀ãx̃1:k ,
µ̃(ãx̃<k ãx̃k ) = µ̃(ãx̃k ) := µ(ax(k−1)l+1:(k−1)l+l ).
By construction the environment (Ã, X̃, µ̃) is a bandit and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> by
Lemma 3.5.4 it is an ergodic MDP.
2
The above results show that Bernoulli schemes, classification problems, bandits and repeated strategic games are either ergodic MDPs or can be reduced to
one. As such, they all admit self-optimising policies and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> an appropriately
defined universal agent π ζ is self-optimising in these classes of environments.
As the rewards received in totally passive environments are independent of
the agent’s behaviour, these trivially admit self-optimising agents, indeed all
agents are equally “optimal”. What about function optimisation and sequence
prediction problems?
Unfortunately, the above approach does not <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> for function optimisation
problems as we have defined them. The problem is that they are not MDPs as
the reward signal depends on more than just the current action and the last
observation. They can be modelled as a POMDP by including the last reward
in the core MDP and making this unobservable. In any case, this is not a
problem because any agent that enumerates the action space will eventually
hit <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">upon</span> the optimal action and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> is self-optimising. What this highlights
is that being self-optimising only tells us something about performance in the
limit, it <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> <span class="t15 t25 t26 t29 hoverable">nothing</span> about how quickly an agent will learn to perform well.
Sequence prediction problems are more problematic. In general, no agent
can be self-optimising over the class of all sequence prediction problems. To
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> this, simply consider that for any prediction agent there exists a sequence
where the next observation is always the observation which the agent predicted
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be the least likely (for a formal statement of this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Lemma 5.2.4). This
is <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> for incomputable agents. If we restrict the sequences to have computable distributions, but still allow the agent to be incomputable, then we
have seen that Solomonoff’s predictor has a bounded total expected prediction error. As the prediction error converges to zero, the reward converges to
optimal and so Solomonoff’s predictor is self-optimising. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that the universal agent was built <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">upon</span> the same foundations as Solomonoff’s predictor,
we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> then expect the same result to hold. At present nobody has been able
to prove this, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> it is conjectured to be true. Currently the best bound
is exponentially worse and holds only for deterministic computable sequence
prediction (Section 6.2.2 of Hutter, 2005). For our purposes an exponentially
worse bound is still finite and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> it follows that a universal agent defined
over the space of computable sequences is self-optimising.

67

3. Taxonomy of Environments

3.6. Conclusion
In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we have defined a range of classes of environments and shown
that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of these are either special cases of other more general classes, or
are at least reducible to more elementary classes through a change of action
and perception spaces. This hierarchy of classes defines a kind of taxonomy
of environments. To the best of our <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> this analysis has not been
done before. Figure 3.1 summarises these relationships. The most general
and powerful classes of environments are at the top and the most limited and
specific classes at the bottom. As we can see, all of the more concrete classes at
the bottom of the hierarchy admit self-optimising agents. By Theorem 2.10.5
it then follows that a universal agent defined over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of these classes is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span>
self-optimising. This supports our earlier claim that universal agents are able
to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in a wide range of environments, as required by our informal
definition of intelligence.

68

3.6. Conclusion

Figure 3.1.: Taxonomy of environments. Downward arrows indicate that the
class below is a special case of the class above. Dotted horizontal
lines indicate that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> classes of environments are reducible to
each other. The greyed area contains the classes of environments
that admit self-optimising agents, that is, the environments in
which a universal agent will learn to behave optimally.

69

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
“. . . we need a definition of <span class="t20 hoverable">intelligence</span> that is applicable to machines as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as humans or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> dogs. Further, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be helpful
to have a relative measure of intelligence, that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> enable us to
judge <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> program more or less intelligent than another, rather
than identify some absolute criterion. Then it will be possible to
assess whether progress is being <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> . . . ” Johnson (1992)

In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 1 we explored the concept of <span class="t20 hoverable">intelligence</span> and proposed an informal definition of intelligence. In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 2 we introduced universal agents,
and in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 3 we detailed some of the classes of environments in which
their behaviour converges to optimal. This shows that universal agents are
highly intelligent with respect to the definition of <span class="t20 hoverable">intelligence</span> that we have
adopted. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> argue that the universal agent defined over the space of
all enumerable chronological environments, that is AIXI, is in some sense an
optimal machine intelligence.
In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we turn this idea on its head: Instead of using the theory of
universal artificial <span class="t20 hoverable">intelligence</span> to define powerful agents, we use it instead to
formally define <span class="t20 hoverable">intelligence</span> itself. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> approach is to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> AIXI and to mathematically define a performance measure under which AIXI is the maximal
agent by construction. This is the approach <span class="t19 hoverable">taken</span> by the <span class="t20 hoverable">Intelligence</span> <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">Order</span>
Relation (see Section 5.1.4 in Hutter, 2005). Although this produces a very
general relation for comparing the relative performance of agents, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to
justify calling this a formal definition of “intelligence” <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> carefully examine the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which <span class="t20 hoverable">intelligence</span> is defined, and then show how this relates
to the equation.
In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we bridge this gap by proceeding in the opposite direction.
We begin with our informal definition of <span class="t20 hoverable">intelligence</span> from <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 1 that was
based on a range of standard definitions <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by psychologists and artificial
<span class="t20 hoverable">intelligence</span> researchers. We then formalise this definition, borrowing ideas
from reinforcement learning, Kolmogorov complexity, Solomonoff induction
and universal artificial <span class="t20 hoverable">intelligence</span> theory as necessary. The result is an equation for <span class="t20 hoverable">intelligence</span> that is strongly related to existing definitions, and with
respect to which highly intelligent agents can be proven to have powerful optimality properties. We then look at some of this definition’s properties and
compare it to other tests and definitions of machine intelligence.

71

4. Universal <span class="t20 hoverable">Intelligence</span> Measure

4.1. A formal definition of machine <span class="t20 hoverable">intelligence</span>
Consider again our informal definition of intelligence:
<span class="t20 hoverable">Intelligence</span> measures an agent’s ability to achieve goals in a wide
range of environments.
This definition contains <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> essential components: An agent, environments
and goals. Clearly, the agent and the environment <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be able to interact
with each other, specifically, the agent needs to be able to send signals to
the environment and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> receive signals being sent from the environment.
Similarly, the environment <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be able to send and receive signals. In our
terminology we will adopt the agent’s perspective on these communications
and refer to the signals sent from the agent to the environment as actions, and
the signals sent from the environment to the agent as perceptions.
Our definition of an agent’s <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> requires there to be some kind
of goal for the agent to try to achieve. Perhaps an agent <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be intelligent,
in an abstract sense, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> having any objective to apply its <span class="t20 hoverable">intelligence</span>
to. Or perhaps the agent has no desire to exercise its <span class="t20 hoverable">intelligence</span> in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span>
that affects its environment. In either case, the agent’s <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be
unobservable and, more importantly, of no practical consequence. <span class="t20 hoverable">Intelligence</span>
then, at least the concrete kind that interests us, comes into effect when the
agent has an objective or goal that it actively pursues by interacting with its
environment.
The existence of a goal raises the problem of how the agent knows what the
goal is. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> possibility <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be for the goal to be <span class="t6 t10 t15 hoverable">known</span> in advance and
for this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> to be built into the agent. The problem with this is that
it limits each agent to just <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> goal. We need to allow agents that are more
flexible, specifically, we need to be able to inform the agent of what the goal
is. For humans this is easily done using language. In general however, the
possession of a sufficiently high level of language is too strong an assumption
to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> about the agent. Indeed, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> for something as intelligent as a dog
or a cat, direct explanation is not very effective.
Fortunately there is <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> possibility which is, in some sense, a blend of
the above two. We define an additional communication channel with the simplest possible semantics: a signal that indicates how <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> the agent’s current
situation is. We will call this signal the reward. The agent simply has to
maximise the amount of reward it receives, which is a function of the goal. In
a complex setting the agent <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be rewarded for winning a game or solving
a puzzle. If the agent is to succeed in its environment, that is, receive a lot of
reward, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> learn about the structure of the environment and in particular
what it needs to do in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to get reward.
This system of an agent interacting with an environment and trying to
achieve some goal is the reinforcement learning agent-environment framework
from Section 2.8. That this framework fits <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> with our informal definition
of <span class="t20 hoverable">intelligence</span> is not surprising <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> how simple and general it is. Indeed,

72

4.1. A formal definition of machine <span class="t20 hoverable">intelligence</span>
it is not only <span class="t27 hoverable">used</span> in artificial intelligence, in control theory it is <span class="t6 t10 t15 hoverable">known</span> as
the plant-controller framework. For example, the plant <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be a nuclear
<span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> plant, and the controller a system designed to keep the reactor <span class="t5 t7 hoverable">within</span>
safe operating guidelines. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which you <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> train your dog
to perform tricks by rewarding <span class="t3 hoverable">certain</span> behaviours fits into this very general
framework.
As in Section 2.8, we will include the reward signal as a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the perception
generated by the environment. The perceptions <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> contain a non-reward
part, which we will refer to as observations. The goal is implicitly defined by
the environment as this is what controls when rewards are generated. Thus,
in the framework as we have defined it, to test an agent in any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> it is
sufficient to fully define the environment.
Unfortunately, maximising reward is not sufficient to define how the agent
should behave over time. We have to define some kind of a temporal preference
that describes how <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> the agent should value near term rewards verses
rewards further into the future. As we saw in Section 2.9, a general approach
is to weight, or discount, each reward in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that depends on which cycle
it occurs in. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> γ1 , γ2 , . . . be the discounts
P∞ we apply to the reward in each
successive cycle, where ∀i : γi ≥ 0, and i=1 γi < ∞ in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to avoid infinite
weighted sums. Now define the expected future discounted reward for agent π
interacting with environment µ to be,
!
∞
X
πµ
Vγ := E
γi ri .
i=t

It is this value function that incorporates our temporal preferences that the
agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> optimise. Although this is very general, the discounting parameters γ1 , γ2 , . . . are nevertheless free parameters. In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> our formal
measure unique we want to remove these parameters, and of course we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
do so in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that is still completely general.
If we look at the value function above, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that discounting plays <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span>
roles. Firstly, it normalises rewards received so that their sum is always finite.
Secondly, it weights the rewards at <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> points in the future which in effect
defines a temporal preference. A direct <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to solve both of these problems,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> needing an external parameter, is to simply require the total reward
returned by the environment to be bounded. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Without</span> loss of generality, we
<span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> the bound to be 1. We denote this <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of reward-summable environments
by E. For any µ ∈ E, it follows that the expected value of the sum of rewards
is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> finite and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> discounting is no longer required,
!
∞
X
π
Vµ := E
ri ≤ 1.
i=1

<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> of viewing this is that the rewards returned by the environment
now have the temporal preference already factored in. Indeed, because <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span>

73

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
reward summable environment is included, in effect <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> possible temporal
preference is represented in the space of environments. The cost is that this is
an additional condition that we <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> on the space of environments. Previously
we required that each reward signal was in a subset of [0, 1] ∩ Q, now we have
the additional constraint that the reward sum is always bounded.
Next we need to quantify what we mean by “goals in a wide range of environments.” As we have argued previously, <span class="t20 hoverable">intelligence</span> is not simply the ability
to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> at a narrowly defined task; it is <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> broader. An intelligent agent is able to adapt and learn to deal with <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> situations,
kinds of problems and types of environments. In our informal definition this
was described as the agent’s general ability to perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in a “wide range
of environments.” This flexibility is a defining characteristic and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the
most important differences between humans and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> current AI systems:
while Gary Kasparov <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> still be a formidable player if we were to change
the rules of chess, IBM’s Deep Blue chess super computer <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be rendered
useless <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> significant <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intervention.
As we want our definition to be as broad and encompassing as possible, the
space of environments <span class="t27 hoverable">used</span> should be as large as possible. As the environment
is a probability measure with a <span class="t3 hoverable">certain</span> structure, an obvious possibility <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
be to consider the space of all probability measures of this form. Unfortunately, this extremely broad class of environments causes serious problems.
As the space of all probability measures is uncountably infinite, some environments <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be described in a finite <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> and so are incomputable. This <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> it impossible, by definition, to test an agent in such an environment using a computer. Further, most environments <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be infinitely complex and
have little structure for the agent to learn from.
The solution is to require the environmental probability measures to be
computable. Not only is this condition necessary if we are to have an effective
measure of intelligence, it is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> not as restrictive as it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> appear.
There are still an infinite number of environments with no upper bound on
their maximal complexity. Also, although the measures that describe the environments are computable, this does not mean that the environments are deterministic. For example, although a typical sequence of 1’s and 0’s generated
by flipping a coin is not computable, the probability measure that describes
this distribution is computable and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> it is included in our space of possible environments. Indeed, there is currently no evidence that the physical
universe <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be simulated by a Turing machine in the above sense (for further discussion of this point <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 4.4). This appears to be the largest
reasonable space of environments.
We have now formalised all the elements of our informal definition. The next
problem is how to <span class="t22 hoverable">bring</span> these <span class="t16 t21 hoverable">together</span> in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to define an overall measure
of performance; we need to find a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to combine an agent’s performance
in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> environments into a single overall measure. As there are an
infinite number of environments, we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> simply <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> a uniform distribution

74

4.1. A formal definition of machine <span class="t20 hoverable">intelligence</span>
over them. Mathematically, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> weight some environments higher than
others. But how?
Consider the agent’s perspective on this situation: there exists a probability measure that describes the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> environment, <span class="t27 t28 hoverable">however</span> this measure is not
<span class="t6 t10 t15 hoverable">known</span> to the agent. The only information the agent has are some past observations of the environment. From these, the agent can construct a list of
probability measures that are consistent with the observations. We call these
potential explanations of the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> environment hypotheses. As the number of
observations increases, the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of hypotheses shrinks and hopefully the remaining hypotheses <span class="t8 t13 hoverable">become</span> increasingly accurate at modelling the environment.
The problem is that in any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> situation there will likely be a large number of hypotheses that are consistent with the current <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of observations.
The agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> keep these in accordance with Epicurus’ principle of multiple
explanations, as we saw in Section 2.1. Because they are all consistent with
the current observations, if the agent is going to estimate which hypotheses
are the most likely to be correct it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> resort to something other than this
observational information. This is a frequently occurring problem in inductive
inference for which the most common approach is to invoke the principle of
Occam’s razor, which we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> met in Section 2.1:
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> multiple hypotheses that are consistent with the data, the
simplest should be preferred.
This is generally considered the rational and intelligent thing to do (Wallace,
2005). Indeed, as noted in Section 1.5, standard IQ tests implicitly test an
individual’s ability to use Occam’s razor. In some cases we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> consider
the correct use of Occam’s razor to be a more important demonstration of
<span class="t20 hoverable">intelligence</span> than achieving a successful outcome. Consider, for example, the
following game:
4.1.1 Example. (Dumb luck game) A questioner lays twenty $10 notes
out on a table before you and then points to the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> and asks “Yes or
No?”. If you answer “Yes” he hands you the money. If you answer “No”
he takes it from the table and puts it in his pocket. He then points to the
next $10 <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">note</span> on the table and asks the same question. Although you, as
an intelligent agent, <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> experiment with answering both “Yes” and “No”
a few times, by the 13th round you <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> have decided that the best choice
seems to be “Yes” each time. <span class="t27 t28 hoverable">However</span> what you do not <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> is that if you
answer “Yes” in the 13th round then the questioner will pull out a gun and
shoot you! Thus, although answering “Yes” in the 13th round is the most
intelligent choice, <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> what you know, it is not the most successful one. An
exceptionally dim individual <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have failed to notice the obvious relationship
between answers and getting the money, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> answer “No” in the
13th round, thereby saving his <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> due to what <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> truly be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> “dumb
luck”.
3

75

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
What is important then, is not that an intelligent agent succeeds in any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span>
situation, but rather that it takes actions that we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect to be the most
likely ones to lead to success. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> adequate experience this <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be clear,
<span class="t27 t28 hoverable">however</span> experience is often not sufficient and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> fall back on <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> prior
assumptions about the world, such as Occam’s razor. It is important then that
we test the agents in such a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that they are, at least on average, rewarded
for correctly applying Occam’s razor, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if in some cases this leads to failure.
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that this does not necessarily mean always following the simplest hypothesis that is consistent with the observations. It is just that simpler hypotheses are considered to be more likely to be correct. Thus, if there is a
simple hypothesis suggesting <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> thing, and a large number of slightly more
complex hypotheses suggesting something else, the latter <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be considered
the most likely.
There is <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> subtlety that needs to be pointed out. Often <span class="t20 hoverable">intelligence</span> is
<span class="t0 t5 t6 t14 t18 hoverable">thought</span> of as the ability to deal with complexity. Or in the <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t24 t25 t26 t27 t28 hoverable">words</span> of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> psychologist, “. . . [intelligence] is the ability to deal with cognitive complexity —
in particular, with complex information processing.”(Gottfredson, 1997b) It is
tempting then to equate the difficultly of an environment with its complexity. Unfortunately, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> are not so straightforward. Consider the following
environment:
4.1.2 Example. Imagine a very complex environment with a rich <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of
relationships between the agent’s actions and observations. The measure that
describes this will have a high complexity. However, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> imagine that the reward signal is always maximal no matter what the agent does. Thus, although
this is a very complex environment in which the agent is unlikely to be able to
predict what it will observe next, it is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> an easy environment in the sense
that all agents are optimal, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> very simple ones that do <span class="t15 t25 t26 t29 hoverable">nothing</span> at all. The
environment contains a lot of structure that is irrelevant to the goal that the
agent is trying to achieve.
3
From this perspective, a problem is <span class="t0 t5 t6 t14 t18 hoverable">thought</span> of as being difficult if the simplest <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> solution to the problem is complex. Easy problems on the other
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span> are those that have simple solutions. This is a very natural <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to think
about the difficulty of problems, or in our terminology, environments.
Fortunately, this distinction does not affect our use of Occam’s razor. This
is because Occam’s razor assigns to each hypothesis a prior probability of it
being the correct model <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to its complexity. It <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> <span class="t15 t25 t26 t29 hoverable">nothing</span> about how
relevant or useful that hypothesis <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to the agent’s goals. For example,
<span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to Occam’s razor a simple environment that always gives the agent
maximal reward <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be more likely than a complex environment that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span>
always gives the agent maximal reward, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> environments are
equally easy to succeed in. Of course from an agent’s perspective, an incorrect
hypothesis that fails to model <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> of the environment <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> if
the parts of the environment that the hypothesis fails to model are not relevant
to receiving reward. Nevertheless, if we want to reward agents on average for

76

4.1. A formal definition of machine <span class="t20 hoverable">intelligence</span>
correctly using Occam’s razor, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> weight the environments <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to
their complexity, not their difficulty.
Although we have chosen to follow a fairly strict interpretation of Occam’s
razor, the idea of weighting <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the complexity of the simplest <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span>
solution <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have some merit. For example, if we weight the <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> “experts” in a prediction with expert advice algorithm <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to their complexity, we are in effect applying this alternate principle. In practice, this can <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span>
well.
Our remaining problem now is to measure the complexity of environments.
This is a problem that we have already solved for sequences in Section 2.5, and
then generalised to active environments in Section 2.10: for any environment
µ ∈ E we define its complexity to be K(µ), that is, its Kolmogorov complexity
which is essentially just the length of the shortest program that describes µ.
Bringing all these pieces together, we can now define our formal measure of
intelligence:
4.1.3 Definition. The universal <span class="t20 hoverable">intelligence</span> of an agent π is its expected
performance with respect to the universal distribution 2−K(µ) over the space
of all computable reward-summable environments E, that is,
X
2−K(µ) Vµπ = Vξπ .
Υ(π) :=
µ∈E

The final equality above follows from the linearity of V and the definition of ξ
as a weighted mixture of environments. It shows that the universal <span class="t20 hoverable">intelligence</span>
of an agent is simply its expected performance with respect to the universal
distribution.
Consider how this equation corresponds to our informal definition. We need
to measure an agent’s ability to achieve goals in a wide range of environments.
Clearly present in the equation is the agent π, the environment µ and, implicit
in the environment, a goal. The agent’s “ability to achieve” is represented by
the value function Vµπ . By a “wide range of environments” we have <span class="t19 hoverable">taken</span> the
space of all computable reward-summable environments, where these environments have been characterised as computable chronological measures in the
<span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> E. Occam’s razor is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by the term 2−K(µ) which weights the agent’s
performance in each environment in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that decreases <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to its complexity. The definition is very general in terms of which sensors or actuators
the agent <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> have, as all information exchanged between the agent and the
environment takes <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> over very general communication channels. Finally,
the formal definition places no limits on the internal workings of the agent.
Thus, we can apply the definition to any system that is able to receive and
generate information with a view to achieving goals.
The main drawback is that the Kolmogorov complexity function K is not
computable and can only be approximated. This is acceptable as our aim has
simply been to define the concept of <span class="t20 hoverable">intelligence</span> in the most general, powerful
and elegant way. In future research we will explore ways to approximate this

77

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
ideal with a practical test. Naturally, the process of estimation will introduce
weaknesses and flaws that the current definition does not have. For example,
while the definition considers the general performance of an agent over all
computable environments with bounded reward sum, in practice a test <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span>
only <span class="t29 hoverable">ever</span> estimate this by testing the agent on a finite sample of environments.
This situation is similar to the definition of randomness for sequences: Informally, an infinite sequence is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">said</span> to be Martin-Löf random when it has no
significant regularity (Martin-Löf, 1966). This lack of regularity is equivalent
to <span class="t0 t2 t3 t4 t5 t9 t11 t12 t15 t16 t17 t22 t24 t27 hoverable">saying</span> that the sequence <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be compressed in any significant way, and
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> we can characterise randomness using Kolmogorov complexity. Naturally,
we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> test a sequence for <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> possible regularity, which is equivalent to
<span class="t0 t2 t3 t4 t5 t9 t11 t12 t15 t16 t17 t22 t24 t27 hoverable">saying</span> that we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> compute its Kolmogorov complexity. We can <span class="t27 t28 hoverable">however</span>
test sequences for randomness by checking them for a large number of statistical regularities; indeed, this is what is done in practice. Of course, just
because a sequence passes all our tests does not mean that it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be random.
There <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> always be some deeper structure to the sequence that our tests
were not able to detect. All we can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> is that the sequence seems random
with respect to our ability to detect patterns.
Some <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> argue that the definition of something should not just capture
the concept, it should <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be practical. For example, the definition of <span class="t20 hoverable">intelligence</span> should be such that <span class="t20 hoverable">intelligence</span> can be easily measured. The above
example, however, illustrates why this approach is sometimes flawed: if we
were to define randomness with respect to a particular <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of tests, then <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> specifically construct a sequence that followed a regular pattern in such
a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that it passed all of our randomness tests. This <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> completely undermine our definition of randomness. A better approach is to define the concept
in the strongest and cleanest <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> possible, and then to accept that our ability
to test for this ideal has limitations. In other words, our task is to find better
and more effective tests, not to redefine what it is that we are testing for.
This is the attitude we have <span class="t19 hoverable">taken</span> here, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> in this thesis our focus is on
the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> part, that is, establishing a strong theoretical definition of machine
intelligence.

4.2. Universal <span class="t20 hoverable">intelligence</span> of various agents
In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to gain some intuition for our definition of intelligence, in this section we will consider a range of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> agents and their relative degrees of
universal intelligence.
A random agent. The agent with the lowest intelligence, at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">among</span>
those that are not actively trying to perform badly, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> that makes
uniformly random actions. We will call this π rand . Although this is clearly a
rand
will always be
weak agent, we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> simply conclude that the value of Vµπ
low as some environments will generate high reward no matter what the agent

78

4.2. Universal <span class="t20 hoverable">intelligence</span> of various agents
does. Nevertheless, in general such an agent will not be very successful as it
will fail to exploit any regularities in the environment, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> trivial ones. It
rand
follows then that the values of Vµπ
will typically be low compared to other
agents, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> Υ(π rand ) will be low. Conversely, if Υ(π rand ) is very low, then
the equation for Υ implies that for simple environments, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> complex
rand
environments, the value of Vµπ <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be relatively low. This kind of poor
performance in general is what we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect of an unintelligent agent.

A very specialised agent. From the equation for Υ, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that an agent
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> have very low universal <span class="t20 hoverable">intelligence</span> but still perform extremely <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> at
a few very specific and complex tasks. Consider, for example, IBM’s Deep
Blue chess supercomputer, which we will represent by π dblue . When µchess
dblue
chess
describes the game of chess, Vµπchess is very high. <span class="t27 t28 hoverable">However</span> 2−K(µ ) is small,
and for µ 6= µchess the value function will be low as π dblue only plays chess.
Therefore, the value of Υ(π dblue ) will be very low. Intuitively, this is because
Deep Blue is too inflexible and narrow to have general intelligence. Current
artificial <span class="t20 hoverable">intelligence</span> systems fall into this category: powerful in some domain,
but not general and adaptable enough to be truly intelligent.
Interestingly, universal <span class="t20 hoverable">intelligence</span> becomes somewhat counter intuitive
when we use it to compare very specialised agents. Consider an agent π simple
which is only able to learn to predict sequences of the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> 0000 . . . and
1111 . . .. Obviously this agent will fail in most environments and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> will
have a low universal intelligence, as we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect. However, environments
of this <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> will have short programs and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> are <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more likely than
environments which describe, for example, chess. As π dblue can only play
chess, it <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> learn to predict these simple sequences, it follows then that
Υ(π dblue ) < Υ(π simple ). Intuitively we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect the reverse to be true.
What this shows is that the universal <span class="t20 hoverable">intelligence</span> measure strongly emphasises the ability to solve simple problems. If any system <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> do this, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if
it can do something relatively complex <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> play chess, then it is considered to
have very little intelligence. Of course extreme cases such the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> above only
occur with artificial constructions such as chess playing machines. Any <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span>
able to play chess <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> easily be able to learn to predict trivial patterns such
as 0000 . . ..
With the above in mind, it is interesting to consider the progress of artificial
<span class="t20 hoverable">intelligence</span> as a field from the perspective of universal intelligence. In the early
<span class="t0 t3 t4 t5 t11 t20 t26 t27 hoverable">days</span> of artificial <span class="t20 hoverable">intelligence</span> there was a lot of emphasis on developing machines
that were able to do simple reasoning and pattern matching etc. Extending
the <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> of these general systems was difficult and over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> the field <span class="t8 t13 hoverable">become</span>
increasingly concerned with very narrow systems that were able to solve quite
specific problems. This has lead some <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> to complain that while we now
have impressive systems for some specific things, we have not progressed <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span>
towards <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> intelligence, <span class="t6 t13 t14 t25 t27 hoverable">meaning</span> artificial general intelligence. Indeed, from

79

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
the perspective of universal intelligence, by focusing on increasingly specialised
systems we have in fact have gone backwards.
A general but simple agent. Imagine an agent that performs very basic
learning by building up a table of observation and action pairs and keeping
statistics on the rewards that follow. Each <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> an observation that it has
seen before occurs, the agent takes the action with <span class="t6 t9 t22 t29 hoverable">highest</span> estimated expected
reward in the next cycle with 0.9 probability, or a random action with 0.1
probability. We will call this agent π basic . It is clear that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> environments,
both complex and very simple, will have at least some structure that such an
basic
>
agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> advantage of. Thus, for almost all µ we will have Vµπ
rand
basic
rand
π
and so Υ(π
) > Υ(π
). Intuitively, this is what we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect
Vµ
as π basic , while very simplistic, is surely more intelligent than π rand .
Similarly, as π dblue will fail to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> advantage of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> trivial regularities in
some of the most basic environments, Υ(π basic ) > Υ(π dblue ). This is reasonable as our aim is to measure a machine’s level of general intelligence. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span>
an agent that can <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> advantage of basic regularities in a wide range of environments should rate more highly than a specialised machine that fails outside
of a very limited domain.
A simple agent with more history. The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> structure of π basic , while
very general, will miss <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> simple exploitable regularities. Consider the
following environment µalt . <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> A = {up, down} and O = {ε}. In cycle k
the environment generates a reward of 2−k each <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> the agent’s action is
<span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> to its previous action. Otherwise the reward is 0. We can define this
environment formally,

 1 if ak 6= ak−1 ∧ rk = 2−k ,
alt
1 if ak = ak−1 ∧ rk = 0,
µ (ax<k axk ) :=

0 otherwise.

Clearly the optimal strategy for an agent is simply to alternate between the
actions up and down. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> this is very simple, this strategy requires
the agent to correlate its current action with its previous action, something
that π basic <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> do. <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that we <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> the reward in cycle k to be 2k in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span>
to satisfy our bounded reward sum condition.
A natural extension of π basic is to use a longer history of actions, observations and rewards in its internal table. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> π 2back be the agent that
builds a table of statistics for the expected reward conditioned on the last
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> actions, rewards and observations. It is immediately clear that π 2back
will exploit the structure of the µalt environment. Furthermore, by definition
π 2back is a generalisation of π basic and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> it will adapt to any regularity that
2back
basic
π basic can adapt to. It follows then that in general Vµπ
> Vµπ
and so
Υ(π 2back ) > Υ(π basic ), as we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect. In the same <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> we can extend the
history that the agent utilises back further and produce <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more powerful

80

4.2. Universal <span class="t20 hoverable">intelligence</span> of various agents

top
a = rest or climb
r = 2 -k

a = rest
-k-4
r = 2

bottom

a = climb
r = 0.0

Figure 4.1.: A simple game in which the agent climbs a playground slide and
slides back down again. A shortsighted agent will always just rest
at the bottom of the slide.

agents that are able to adapt to more lengthy temporal structures and which
will have still higher universal intelligence.
A simple forward looking agent. In some environments simply trying to
maximise the next reward is not sufficient, the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> into account the rewards that are likely to follow further into the future, that is,
the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> plan ahead. Consider the following environment µslide . <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span>
A = {rest, climb} and O = {ε}. Imagine there is a slide such as you <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> in a playground. The agent can rest at the bottom of the slide, for which
it receives a reward of 2−k−4 . The alternative is to climb the slide, which gives
a reward of 0. Once at the top of the slide the agent always slides back down
no matter what action is taken; this gives a reward of 2−k . This deterministic
environment is illustrated in Figure 4.1.
Because climbing receives a reward of 0, while resting receives a reward of
2−k−4 , a very shortsighted agent that only tries to maximise the reward in
the next cycle will choose to stay at the bottom of the slide. Both π basic and
π 2back have this problem, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> they <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> random actions with 0.1
probability and so will occasionally climb the slide by chance. Clearly this is
not optimal in terms of total reward over time.
We can extend the π 2back agent again by defining a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> agent π 2forward that
with 0.9 probability chooses its next action to maximise not just the next
reward, but r̂k + r̂k+1 , where r̂k and r̂k+1 are the agent’s estimates of the next
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> rewards. As the estimate of r̂k+1 will potentially depend not only on ak ,
but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> on ak+1 , the agent assumes that ak+1 is chosen to simply maximise
the estimated reward r̂k+1 .
The π 2forward agent can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that by missing out on the resting reward of
−k−4
2
for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> cycle and climbing, a greater reward of 2−k will be had when
sliding back down the slide in the following cycle. <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that the value of the

81

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> index k will have increased by the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> the agent gets to slide down,
<span class="t27 t28 hoverable">however</span> this is not enough to change the optimal course of action.
By definition π 2forward generalises π 2back in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that more closely reflects
2back
2forward
> Vµπ . It then follows
the value function V and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> in general Vµπ
that Υ(π 2forward ) > Υ(π 2back ) as we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> intuitively expect for this more
powerful agent.
In a similar <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> agents of increasing complexity and adaptability can be
defined which will have still greater intelligence. However, with more complex
agents it is usually difficult to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> whether <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> agent has more universal <span class="t20 hoverable">intelligence</span> than another. Nevertheless, the simple examples above illustrate how
the more flexible and powerful an agent is, the higher V typically is and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
the higher its universal intelligence.
A very intelligent agent. A very intelligent agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in
simple environments, and reasonably <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> compared to most other agents in
more complex environments. From the equation for universal <span class="t20 hoverable">intelligence</span> this
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> clearly produce a high value for Υ. Conversely, if Υ was high then
the equation for Υ implies that the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in most simple
environments and reasonably <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> complex ones also. Thus, the agent
is able to achieve goals in a wide range of environments, as required by our
informal definition of intelligence.
A super intelligent agent. Consider what <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be required to maximise
the value of Υ. By definition, a “perfect” agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> always pick the action
which had greatest expected future reward. To do this, for <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> environment
µ ∈ E the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> into account how likely it is that it is facing µ,
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> the interaction history so <span class="t6 t9 t18 t26 hoverable">far</span> and the prior probability of µ, that is,
2−K(µ) . It <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> then consider all possible future interactions that <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span>
occur, how likely they are, and from this select the action in the current cycle
that maximises the expected future reward.
Essentially, this is the AIXI agent described in Section 2.10. The only difference is that AIXI was defined using discount parameters, while universal
<span class="t20 hoverable">intelligence</span> avoided these by requiring the total reward from environments to
be bounded. If we remove discounting for AIXI and define it to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> over
reward bounded environments, then the universal <span class="t20 hoverable">intelligence</span> measure is in
some sense the dual of the universal agent AIXI. It follows that agents with
very high universal <span class="t20 hoverable">intelligence</span> have powerful performance characteristics.
With our modified AIXI being the most intelligent agent by construction,
we can define the upper bound on universal <span class="t20 hoverable">intelligence</span> to be,

Ῡ := max Υ(π) = Υ π ξ .
π

This upper bounds the <span class="t20 hoverable">intelligence</span> of all future machines, no matter how
powerful their hardware and algorithms <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be.

82

4.3. Properties of universal <span class="t20 hoverable">intelligence</span>
A human. For simple environments, a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> should be able to identify their
structure and exploit this to maximise reward. However, for more complex
environments it is hard to <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> how <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> perform. <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">Much</span> of the
<span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> brain is <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> up to process <span class="t3 hoverable">certain</span> kinds of structured information from
the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> sense organs, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> is quite specialised, at least compared to the
extremely general setting considered here. Perhaps the amount of universal
machine <span class="t20 hoverable">intelligence</span> that a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> has is not that high compared to some
machine learning algorithms? It is difficult to <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> experimental
results.

4.3. Properties of universal <span class="t20 hoverable">intelligence</span>
What we have presented is a definition of machine intelligence. It is not a
practical test of machine intelligence, indeed the value of Υ is not computable
due to the use of Kolmogorov complexity. Although some of the criteria by
which we judge practical tests of <span class="t20 hoverable">intelligence</span> are not relevant to a pure definition of intelligence, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of the desirable properties are similar. Thus, to
understand the strengths and weaknesses of our definition, consider again the
desirable properties for a test of <span class="t20 hoverable">intelligence</span> from Section 1.4.
Valid. The most important property of any proposed formal definition of
<span class="t20 hoverable">intelligence</span> is that it does indeed describe something that can reasonably be
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> intelligence. Essentially, this is the core argument of this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> so far:
we have <span class="t19 hoverable">taken</span> a mainstream informal definition and step by step formalised it.
Thus, so <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> as our informal definition is acceptable, and our formalisation
argument holds, the result can reasonably be described as a formal definition
of intelligence.
As we saw in the previous section, universal <span class="t20 hoverable">intelligence</span> orders the <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span>
and adaptability of simple agents in a natural way. Furthermore, a high value
of Υ implies that the agent performs <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> on most simple and moderately
complex environments. Such an agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be an impressively powerful and
flexible piece of technology, with <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> potential uses. Clearly then, universal <span class="t20 hoverable">intelligence</span> is inherently meaningful, independent of whether or not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
considers it to be a measure of intelligence.
Informative. Υ(π) assigns to agent π a real value that is independent of the
performance of other possible agents. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> we can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> direct comparisons
between <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> agents on a single scale. This property is useful if we
want to use this measure to study <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> algorithms or modifications to existing
algorithms. In comparison, some other tests return only comparative results,
i.e. that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> algorithm is better than another, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> just a binary <span class="t5 hoverable">pass</span> or
fail.

83

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
Wide range. As we saw in the previous section, universal <span class="t20 hoverable">intelligence</span> is able
to <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> the <span class="t20 hoverable">intelligence</span> of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> the most basic agents such as π rand , π basic ,
π 2back and π 2forward . At the other extreme we have the theoretical super intelligent agent AIXI which has maximal Υ value. Thus, universal <span class="t20 hoverable">intelligence</span>
spans trivial learning algorithms <span class="t27 hoverable">right</span> up to incomputable super intelligent
agents. This seems to be the widest range possible for a measure of machine
intelligence.
General. As the agent’s performance on all <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> defined environments is factored into its Υ value, a broader performance metric is difficult to imagine.
Indeed, a <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> defined measure of <span class="t20 hoverable">intelligence</span> that is broader than universal
<span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> seem to contradict the Church-Turing thesis as it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
imply that we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> effectively measure an agent’s performance for some <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span>
defined problem that was outside of the space of computable measures.
Dynamic. Universal <span class="t20 hoverable">intelligence</span> includes environments in which the agent
has to learn and adapt its behaviour over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to maximise reward.
As such, it is a so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> “dynamic <span class="t20 hoverable">intelligence</span> test” that allows rich interaction
between the agent being tested and its environment (see Section 1.4). In
comparison, most other <span class="t20 hoverable">intelligence</span> tests are “static”, in the sense that they
only require the agent to solve isolated one-off problems. Such tests <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span>
directly measure an agent’s ability to learn and adapt over time.
Unbiased. In a standard <span class="t20 hoverable">intelligence</span> test, an individual’s performance is
judged on specific kinds of problems, and then these scores are combined to
produce an overall result. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the outcome of the test depends on which
types of problems it uses, and how each score is weighted to produce the <span class="t7 t10 t15 t25 hoverable">end</span>
result. Unfortunately, how we do this is a product of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> things, including our culture, values and the theoretical perspective on <span class="t20 hoverable">intelligence</span> that
we have taken. For example, while <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t20 hoverable">intelligence</span> test <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> contain <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span>
logical puzzle problems, <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be more linguistic in emphasis, while
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> stresses visual reasoning. Modern <span class="t20 hoverable">intelligence</span> tests <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> the StanfordBinet try to minimise this problem by covering the most important areas of
<span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> reasoning both verbally and non-verbally. This helps but it is still
very anthropocentric as we are only testing those abilities that we think are
important for <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence.
For an <span class="t20 hoverable">intelligence</span> measure for machines we have to base the test on something more general and principled: universal Turing computation. As all proposed models of computation have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t6 t9 t18 t26 hoverable">far</span> been equivalent in their expressive
power, the concept of computation appears to be a fundamental theoretical
property rather than the product of any specific culture. Thus, by weighting
<span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> environments depending on their Kolmogorov complexity, and considering the space of all computable environments, we have avoided having to
define <span class="t20 hoverable">intelligence</span> with respect to any particular culture, species etc.

84

4.3. Properties of universal <span class="t20 hoverable">intelligence</span>
Unfortunately, we have not entirely removed the problem. The environmental distribution 2−K(µ) that we have <span class="t27 hoverable">used</span> is invariant, up to a multiplicative
constant, to changes in the reference machine U . Although this affords us some
protection, the relative <span class="t20 hoverable">intelligence</span> of agents can change if we change our reference machine. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> approach to this problem is to limit the complexity of
the reference machine, for example by limiting its state-symbol complexity.
We expect that for highly intelligent machines that can deal with a wide range
of environments of varying complexity, the effect of changing from <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> simple
reference machine to <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> will be minor. For simple agents, such as those
considered in Section 4.2, the ordering of their machine <span class="t20 hoverable">intelligence</span> was <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span>
not particularly sensitive to natural choices of reference machine. Recently attempts have been <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> algorithmic probability completely unique by
identifying which universal Turing machines are, in some sense, the most simple (Müller, 2006). Unfortunately however, an elegant solution to this problem
has not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> been found.
An alternate solution, suggested by Peter Dayan (personal communication),
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to allow the agent to maintain state between <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> test environments. This <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> mitigate any bias introduced as intelligent agents <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
then be able to adapt to the test’s reference machine over multiple test trials.
Fundamental. Universal <span class="t20 hoverable">intelligence</span> is based on Turing computation, information and complexity. These are fundamental universal concepts that are
unlikely to change in the future with changes in technology. It <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span>
that universal <span class="t20 hoverable">intelligence</span> is in no <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> anthropocentric.
Formal and objective. As universal <span class="t20 hoverable">intelligence</span> is expressed as a mathematical equation, there is little space for ambiguity in the definition. In particular,
it in no <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> depends on any subjective criteria, unlike some other <span class="t20 hoverable">intelligence</span>
tests and definitions.
Fully defined. For a fixed reference machine, the universal <span class="t20 hoverable">intelligence</span> measure is fully defined. In comparison, some tests of machine <span class="t20 hoverable">intelligence</span> have
aspects which are currently unspecified and in need of further research.
Impractical. In its current <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> the definition <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be directly turned into
a test of <span class="t20 hoverable">intelligence</span> as the Kolmogorov complexity function is not computable.
Thus, in its pure form, we can only use it to analyse the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of <span class="t20 hoverable">intelligence</span>
and to theoretically examine the <span class="t20 hoverable">intelligence</span> of mathematically defined learning algorithms.
In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to use universal <span class="t20 hoverable">intelligence</span> more generally we will need to construct a workable test that approximates an agent’s Υ value. The equation
for Υ suggests how we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> approach this problem. Essentially, an agent’s
universal <span class="t20 hoverable">intelligence</span> is a weighted sum of its performance over the space of all

85

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
environments. Thus, we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> randomly generate programs that describe environmental probability measures and then test the agent’s performance against
each of these environments. After sampling sufficiently <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> environments,
the agent’s approximate universal <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be estimated by weighting
its score in each environment <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the complexity of the environment
as <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by the length of its program. <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> possibility <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to try to
approximate the sum by enumerating environmental programs from short to
long, as the short ones will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> by <span class="t6 t9 t18 t26 hoverable">far</span> the greatest contribution to the sum.
However, in this case we will need to be able to reset the state of the agent so
that it <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> cheat by learning our environmental enumeration method. In
any case, various practical challenges will need to be addressed before universal <span class="t20 hoverable">intelligence</span> can be <span class="t27 hoverable">used</span> to construct an effective <span class="t20 hoverable">intelligence</span> test. As this
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be a significant project in its own right, here we focus on the theoretical
issues surrounding universal intelligence.
Definition rather than a test. As it is not practical in its current form, universal <span class="t20 hoverable">intelligence</span> is more of a formal definition of <span class="t20 hoverable">intelligence</span> than a test of
intelligence. Some proposals we have reviewed aim to be just tests of intelligence, <span class="t1 t7 t13 hoverable">others</span> aim to be definitions, and in some cases they are intended to
be both. Often the exact classification of a proposal as a test or definition, or
both, is somewhat subjective.
Having covered the key properties of the universal <span class="t20 hoverable">intelligence</span> measure, we
now compare these properties with the properties of the other proposed tests
and definitions of machine <span class="t20 hoverable">intelligence</span> surveyed in Section 1.7. Although we
have attempted to be as fair as possible, some of our judgements will of course
be debatable. Nevertheless, we hope that it provides a rough overview of the
relative strengths and weaknesses of the proposals. The summary comparison
appears in Table 4.1.

4.4. Response to common criticisms
Attempting to mathematically define <span class="t20 hoverable">intelligence</span> is very ambitious and so,
not surprisingly, the reactions we get can be interesting. Having presented the
essence of this <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> as posters at several conferences, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> as a 30 minute
talk, we now have some idea of what the typical responses are. Most <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span>
start out sceptical but <span class="t7 t10 t15 t25 hoverable">end</span> up generally enthusiastic, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if they still have a
few reservations. This positive feedback has helped motivate us to continue
this direction of research. In this section, however, we will attempt to cover
some of the more common criticisms.
It’s obviously false, there’s <span class="t15 t25 t26 t29 hoverable">nothing</span> in your definition, just a few equations.
Perhaps the most common criticism is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> the most vacuous one: It’s obviously

86

Intelligence Test
Turing Test
Total Turing Test
Inverted Turing Test
Toddler Turing Test
Linguistic Complexity
<span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">Text</span> Compression Test
Turing Ratio
Psychometric AI
Smith’s Test
C-Test
Universal <span class="t20 hoverable">Intelligence</span>

Va
lid
In
fo
r
W mat
id iv
e
e
G Ra
en n
g
e
D ral e
yn
a
U mic
nb
i
Fu ased
nd
Fo am
rm ent
Fu al & al
lly
O
Pr D e b j .
ac fin
t ic e d
Te al
st
vs
.
D
ef
.

4.4. Response to common criticisms

•
•
•
•
•
•
•
•
•

·
·
•
·

·
·
·
·
•
•

·
·
·
·
·
•
•
•

•
•
•
•
·
·
?
?
·
·

·
·
·
·
·
•
?
•
?

·
·
·
·
·
•
?
·

·
·
·
·
•

•
•
•
·
·

•
·
•
•
•

?
•

·
•
·

?
•
•
·

T
T
T
T
T
T
T/D
T/D
T/D
T/D
D

Table 4.1.: In the table <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> “yes”, • <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> “debatable”, · <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> “no”,
and ? <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> unknown. When something is rated as unknown it is
because the test in question is not sufficiently specified.
wrong! These <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> seem to believe that defining <span class="t20 hoverable">intelligence</span> with an equation
is clearly impossible, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> there <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be very large and obvious flaws in
our work. Not surprisingly, these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> the least likely to want to
spend 10 minutes having the material explained to them. Unfortunately, none
of these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> have been able to communicate why the <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> is so obviously
flawed in any concrete <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> — despite in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> instance chasing the poor fellow
out of the conference centre and down the street begging for an explanation.
If anyone <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> to properly explain their position to us in the future, we
promise not to chase you down the street!
It’s obviously correct, indeed everybody already knows this. Curiously, the
<span class="t6 hoverable">second</span> most common criticism is the exact opposite: The <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> is obviously
right, and indeed it is already <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> known. Digging deeper, the <span class="t0 t1 t4 t8 t9 t10 t11 t12 t16 t19 t22 t23 t25 t26 hoverable">heart</span> of this
criticism comes from the perception that we have not done <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more than just
describe reinforcement learning. If you already accept that the reinforcement
learning framework is the most general and flexible <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to describe artificial
intelligence, and not everybody does, then by mixing in Occam’s razor and a
dash of complexity theory the equation for universal <span class="t20 hoverable">intelligence</span> follows in a
fairly straightforward way. While this is true, the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span>
have been <span class="t2 t4 t5 t22 t23 hoverable">brought</span> <span class="t16 t21 hoverable">together</span> is new. Furthermore, simply coming up with an
equation is not enough, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> argue that what the equation describes is in
fact <span class="t20 hoverable">intelligence</span> in a sense that is reasonable for machines.
We have addressed this question in <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> main ways: Firstly, in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 1 we explored <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> expert definitions of intelligence. Based on these,
we adopted our own informal definition of <span class="t20 hoverable">intelligence</span> in Section 1.2. In the

87

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
present <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> this informal definition was piece by piece formalised leading
to the equation for Υ. This chain of argument ties our equation for <span class="t20 hoverable">intelligence</span> to existing informal definitions and ideas on the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of intelligence.
Secondly, in Sections 4.2 and 4.3 we showed that the equation has properties
that are consistent with a definition of intelligence. Finally, in Section 4.2 it
was shown that universal <span class="t20 hoverable">intelligence</span> is strongly connected to the theory of
universally optimal learning agents, in particular AIXI. From this it follows
that machines with very high universal <span class="t20 hoverable">intelligence</span> have a wide range of powerful optimality properties. Clearly then, what we have done goes <span class="t6 t9 t18 t26 hoverable">far</span> beyond
merely restating reinforcement learning theory.
Assuming that the environment is computable is too strong. It is certainly
possible that the physical universe is not computable, in the sense that the
probability distribution over future events cannot, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> in theory, be simulated
to an arbitrary precision by a computable process. Some <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> this position on various philosophical grounds, such as the need for freewill. However,
in standard physics there is no <span class="t0 t8 t12 t15 t19 t23 t25 hoverable">law</span> of the universe that is not computable in the
above sense. Nor is there any evidence showing that such a physical <span class="t0 t8 t12 t15 t19 t23 t25 hoverable">law</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
exist. This includes quantum theory and chaotic systems, both of which can
be extremely difficult to compute for some physical systems, but are not fundamentally incomputable theories. In the case of quantum computers, they can
compute with lower <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> complexity than classical Turing machines, <span class="t27 t28 hoverable">however</span>
they are unable to compute anything that a classical Turing machine cannot,
when <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> enough time. Thus, as there is no hard evidence of incomputable
processes in the universe, our assumption that the agent’s environment has a
computable distribution is certainly not unreasonable.
If a physical process was <span class="t29 hoverable">ever</span> discovered that was not Turing computable,
then this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> likely result in a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> extended model of computation. Just as
we have based universal <span class="t20 hoverable">intelligence</span> on the Turing model of computation, it
<span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be possible to construct a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> definition of universal <span class="t20 hoverable">intelligence</span> based
on this <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> model in a natural way.
Finally, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if the universe is not computable, and we do not update our
formal definition of <span class="t20 hoverable">intelligence</span> to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> this into account, the fact that everything in physics so <span class="t6 t9 t18 t26 hoverable">far</span> is computable <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that a computable approximation
to our universe <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> still be extremely accurate over a huge range of situations. In which case, an agent that <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> deal with a wide range of computable
environments <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> most likely still function <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t5 t7 hoverable">within</span> such a universe.
Assuming that environments return bounded sum rewards is unrealistic. If
an environment µ is an artificial game, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> chess, then it seems fairly natural
for µ to meet any requirements in its definition, such as having a bounded
reward sum. However, if we think of the environment µ as being the universe
in which the agent lives, then it seems unreasonable to expect that it should
be required to respect such a bound.

88

4.4. Response to common criticisms
Strictly speaking, reward is an interpretation of the state of the environment. In this case the environment is the universe, and clearly the universe
does not have any notion of reward for particular agents. In humans this interpretation is internal, for example, the pain that is experienced when you
touch something hot. In this case, should it be a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the agent rather than
the environment? If we gave the agent complete control over rewards then our
framework <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t8 t13 hoverable">become</span> meaningless: the perfect agent <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> simply <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t28 t29 hoverable">give</span>
itself constant maximum reward. Perhaps the analogous situation for humans
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be taking the “perfect” drug.
A more accurate framework <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> consist of an agent, an environment and
a separate goal system that interpreted the state of the environment and rewarded the agent appropriately. In such a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> up the bounded rewards restriction <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the goal system and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the above problem <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> not
occur. However, for our current purposes, it is sufficient just to fold this goal
mechanism into the environment and add an easily implemented constraint to
how the environment <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> generate rewards. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> simple <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to bound an environment’s total rewards <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to use geometric discounting as discussed
in Section 2.9.
How do you respond to Block’s “Blockhead” argument? The approach
we have <span class="t19 hoverable">taken</span> is unabashedly functional. Theoretically, we desired to have a
formal, simple and very general definition. This is easier to do if we abstract
over the internal workings of the agent and define <span class="t20 hoverable">intelligence</span> only in terms
of external communications. Practically, what matters is how <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> something
works. By definition, if an agent has a high value of Υ, then it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span>
over a wide range of environments.
Block attacks this perspective by describing a machine that appears to be
intelligent as it is able to <span class="t5 hoverable">pass</span> the Turing test, but is in fact no more than
just a big look-up table of questions and answers (Block, 1981, for a related
argument <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Gunderson, 1971). Although such a look-up table <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be
unfeasibly large, the fact that a finite machine <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> in theory consistently <span class="t5 hoverable">pass</span>
the Turing test, seemingly <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> any real intelligence, intuitively seems odd.
Our formal measure of machine <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be challenged in the same
way, as <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> any test of <span class="t20 hoverable">intelligence</span> that relies only on an agent’s external
behaviour.
Our response to this is very simple: if an agent has a very high value of
Υ then it is, by definition, able to successfully operate in a wide range of
environments. We simply do not care whether the agent is efficient, due to
some very clever algorithm, or absurdly inefficient, for example by using an
unfeasibly gigantic look-up table of precomputed answers. The important
point for us is that the machine has an amazing ability to solve a huge range
of problems in a wide variety of environments.

89

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
How do you respond to Searle’s “Chinese room” argument? Searle’s Chinese room argument attacks our functional position in a similar <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> by arguing that a system <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> appear to be intelligent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> really understanding
anything (Searle, 1980). From our perspective, whether or not an agent understands what it is doing is only important to the extent that it affects the
measurable performance of the agent. If the performance is identical, as Searle
seems to suggest, then whether or not the room with Searle inside understands
the <span class="t6 t13 t14 t25 t27 hoverable">meaning</span> of what is going on is of no practical concern; indeed, it is not
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> clear to us how to define understanding if its presence has no measurable
effects. So <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> as the system as a <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span> has the powerful properties required
for universal machine intelligence, then we have the kind of extremely general
and powerful machine that we desire. On the other hand, if understanding
does have a measurable impact on an agent’s performance in some situations,
then it is of interest to us. In which case, because Υ measures performance in
all <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> defined situations, it follows that Υ is in <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> a measure of how <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span>
understanding an agent has.
But you don’t deal with consciousness (or creativity, imagination, freewill,
emotion, love, soul, etc.) We apply the same argument to consciousness,
emotions, freewill, creativity, the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">soul</span> and other such things. Our goal is to
build powerful and flexible machines and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> these somewhat vague properties
are only relevant to our goal to the extent to which they have some measurable
effect on performance in some <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> defined environment. If no such measurable
effect exists, then they are not relevant to our objective. Of course this is not
the same as <span class="t0 t2 t3 t4 t5 t9 t11 t12 t15 t16 t17 t22 t24 t27 hoverable">saying</span> that these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> do not exist. The question is whether
they are relevant or not. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> consider understanding, imagination and
creativity, appropriately defined, to have a significant impact on an agent’s
ability to adapt to challenging environments. Perhaps the same is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span>
of emotions, freewill and other qualities. If <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> accepts that these properties
affect an agent’s performance, then universal <span class="t20 hoverable">intelligence</span> is in <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> a test for
these properties.
<span class="t20 hoverable">Intelligence</span> is fundamentally an anthropocentric concept. As artificial <span class="t20 hoverable">intelligence</span> researchers our goal is not to create an “artificial human”. We are
interested in making machines that are able to process information in powerful ways in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to achieve <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> kinds of goals and solve <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> kinds of
problems. As such, a limited anthropocentric concept of <span class="t20 hoverable">intelligence</span> is not
interesting to us. Or at least, if such a definition were to be adopted, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
simply mean that we are interested in something more general and powerful
than this <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> focused concept of “intelligence”.
Perhaps this is similar to the development of heavier than air flight. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span>
of the most outspoken sceptics of this was the American astronomer Simon
Newcomb. Interestingly, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> after planes were regularly “flying” he refused
to accept defeat. He accepted that planes existed and moved around at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">great</span>

90

4.4. Response to common criticisms
speed up in the air, <span class="t27 t28 hoverable">however</span> he did not accept that what they were doing was
“flying”. To his mind, what birds did was quite different. Of course, the rest
of the population simply generalised and adapted their concept of flying to
reflect the <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> technology.
We believe that with more progress in artificial <span class="t20 hoverable">intelligence</span> the same thing
will eventually happen to the everyday concept of intelligence. At present,
most <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> think of <span class="t20 hoverable">intelligence</span> in <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> terms simply because this is the
only kind of powerful <span class="t20 hoverable">intelligence</span> they have <span class="t29 hoverable">ever</span> encountered. Indeed, from the
perspective of evolutionary psychology, it appears that we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have evolved
to expect other intelligent agents to think and act <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> ourselves.
Universal <span class="t20 hoverable">intelligence</span> does not agree with some everyday intuitions about
the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of intelligence. Everyday intuitions are not a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> guide. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">People</span>
informally use the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> “intelligence” to mean a variety of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> things, and
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> a single <span class="t9 hoverable">person</span> will use the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> in multiple ways that are not consistent
with each other. Thus, although our definition should clearly be related to the
everyday concept, it is not necessarily desirable, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> possible, for a precise
and self-consistent definition to always agree with everyday usage.
Consider a <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> that has both an everyday <span class="t6 t13 t14 t25 t27 hoverable">meaning</span> and a precise technical
meaning. When someone <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> “It’s a beautiful spring day, I am full of energy
and <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> run up a mountain”, what they mean by the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> ‘energy’ is related
to the concept of energy in physics, i.e. they need energy in the technical
sense to get up the mountain. However, the definition of energy from physics
does not entirely capture what they mean. This does not imply that there is
something wrong with the concept of energy in physics. We expect the same
in artificial intelligence: <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> will continue to use the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">word</span> “intelligence” in
an informal way, <span class="t27 t28 hoverable">however</span> in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to do research we will need to adopt a more
precise definition that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be slightly different.
An agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be intelligent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if it doesn’t achieve anything in any
environments. Consider, for example, that you are quietly sitting in a dark
room thinking about a problem you are trying to solve. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> you are
not achieving anything in your environment, some <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> argue that you are
still intelligent due to your internal process of thought. Or imagine perhaps the
famous physicist Stephen Hawking disconnected from his motorised wheelchair
and talking computer. Although his ability to achieve goals in the environment
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be limited, he is still no doubt highly intelligent. A related problem is
that an agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> simply lack the motivation to exhibit intelligent behaviour,
such as a child who wants to get an IQ test out of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> as soon as possibly
in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to go outside and play. In both of these cases the agent’s <span class="t20 hoverable">intelligence</span>
seems to be divorced from the <span class="t20 hoverable">intelligence</span> observable in its behaviour.
The above highlights the fact that when informally considering the measurement of <span class="t20 hoverable">intelligence</span> we need to be careful about exactly how the measurement
is done. Obviously, it only makes sense to measure an individual’s <span class="t20 hoverable">intelligence</span>

91

4. Universal <span class="t20 hoverable">Intelligence</span> Measure
if <span class="t3 hoverable">certain</span> essential requirements are met: a purely <span class="t23 hoverable">written</span> test for a blind
<span class="t9 hoverable">person</span> is senseless, as are the results of a test where the test subject has no
interest in performing well. Informally, we just assume that the test has been
conducted in an appropriate way. When we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that an agent is intelligent,
what we actually mean is that there exists some reasonable setup such that
the agent exhibits a high level of intelligence: a blind <span class="t9 hoverable">person</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> require an
oral test, and a disinterested child some kind of a bribe. Clearly then, if we
want to be precise about the measured <span class="t20 hoverable">intelligence</span> of an agent we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> specify
these details about exactly how the test was conducted.
When we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that an agent has low intelligence, what we mean is that there
does not exist any reasonable test setup such that the agent exhibits intelligent
behaviour. Some <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the position that an entity <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be intelligent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if it
has no measurable <span class="t20 hoverable">intelligence</span> under any test setup. However, such an agent’s
“intelligence” <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be rather meaningless as it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be a property that has
no measurable effects or consequences. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> the philosophical “brain in a vat”
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> in theory be interfaced to in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to measure its universal intelligence.
Such an agent is not intelligent because it <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> choose its goals. In the
setup we have defined the agent <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> decide what its primary goal is. It
simply tries to maximise the reward signal defined by the environment. In the
context of machines this is probably a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> idea: we want to be the ones defining what the machine’s primary objective is. However, this does not address
the question as to whether such a machine should really be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> intelligent,
or whether it is just a very powerful and general optimiser. Intelligent humans,
after all, can choose their own goals in life. But is this really true?
Obviously we can decide that we want to <span class="t8 t13 hoverable">become</span> a successful scientist, a
teacher, or maybe a rock star. So we certainly have some choice in our goals.
But are these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> our primary motivations? If we want to be a successful
scientist or a rock star, perhaps this is due to a deeper biological drive to attain
high status because this increases our chances of reproductive success. Perhaps
the desire to <span class="t8 t13 hoverable">become</span> a teacher stems from a biological drive to care for children,
again because having this drive tends to increase our probability of passing on
our genes and memes to future generations. Our interest in mating with an
attractive member of the opposite sex, avoiding intense physical pain or the
pleasure of eating energy rich foods, all of these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> have clear biological
motivations. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> a suicide bomber who kills himself and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> destroys his
future reproductive potential <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be driven by an out-of-control biological
motivation that tries to increase his societal status in an effort to improve his
chances of reproductive success.
These ideas appear in areas such as evolutionary psychology and, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
be said, attract a fair amount of controversy. Here we will not attempt to
defend them. We simply <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">note</span> that they offer <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> explanation as to how we
are able to choose our goals in life, while at the same <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> having relatively
fixed fundamental motivations.

92

4.5. Conclusion
Universal <span class="t20 hoverable">intelligence</span> is impossible due to the No-Free-Lunch theorem.
Some, such as Edmonds (2006), argue that universal definitions of <span class="t20 hoverable">intelligence</span>
are impossible due to Wolpert’s so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> “No Free Lunch” theorem (Wolpert
and Macready, 1997). <span class="t27 t28 hoverable">However</span> this theorem, or any of the standard variants
on it, <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be applied to universal <span class="t20 hoverable">intelligence</span> for the simple reason that we
have not <span class="t19 hoverable">taken</span> a uniform distribution over the space of environments. Instead
we have <span class="t27 hoverable">used</span> a highly non-uniform distribution based on Occam’s razor.
It is conceivable that there <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> exist some more general kind of “No Free
Lunch” theorem for agents that limits their maximal <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to
our definition. Clearly any such result <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> have to apply only to computable
agents <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that the incomputable AIXI agent faces no such limit. If such a
result were true, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> suggest that our definition of <span class="t20 hoverable">intelligence</span> is perhaps
too broad in its scope. Currently we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> of no such result (c.f. <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 5).

4.5. Conclusion
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> the obvious significance of formal definitions of <span class="t20 hoverable">intelligence</span> for research,
and calls for more direct measures of machine <span class="t20 hoverable">intelligence</span> to replace the problematic Turing test and other imitation based tests (Johnson, 1992), little <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span>
has been done in this area. In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we have attempted to tackle this
problem by taking an informal definition of <span class="t20 hoverable">intelligence</span> modelled on expert
definitions of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence, and then formalising it. We believe that the
resulting mathematical definition captures the concept of machine <span class="t20 hoverable">intelligence</span>
in a very powerful and elegant way. Furthermore, we have seen that agents
which are extremely intelligent with respect to this definition, such as AIXI,
can be proven to have powerful optimality properties.
The central challenge for future <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> on universal <span class="t20 hoverable">intelligence</span> is to convert
this theoretical definition of machine <span class="t20 hoverable">intelligence</span> into a workable test. The
basic structure of such a test is already apparent from the equation for Υ: the
test <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> by evaluating the performance of an agent on a large sample
of simulated environments, and then combine the agent’s performance in each
environment into an overall <span class="t20 hoverable">intelligence</span> value. This <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be done by weighting the agent’s performance in each environment <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> the environment’s
complexity.
A theoretical challenge that will need to be dealt with is to find a suitable
replacement for the incomputable Kolmogorov complexity function. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> solution <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be to use Kt complexity (Levin, 1973), <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to use
the Speed prior (Schmidhuber, 2002). Both of these consider the complexity
of an algorithm to be determined by both its minimal description length and
running time, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> forcing the complexity measures to be computable. Taking
computation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> into account <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> makes reasonable intuitive sense because
we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> not usually consider a very short algorithm that takes an enormous
amount of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> to run to be a particularly simple one. The fact that such an
approach can be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> is evidenced by the C-Test (see Section 1.7).

93

5. Limits of Computational Agents
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of the reasons for studying mathematical models such as AIXI is to gain
insights that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be useful for designing practical artificial <span class="t20 hoverable">intelligence</span> agents.
The question then arises as to how useful all this incomputable theory really is.
In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we will explore this question by looking at the theoretical limitations faced by computable sequence predictors that attempt to approximate
the <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> and generality of Solomonoff induction. As sequence prediction lies
at the <span class="t0 t1 t4 t8 t9 t10 t11 t12 t16 t19 t22 t23 t25 t26 hoverable">heart</span> of reinforcement learning, any limitations <span class="t0 t16 t25 t28 hoverable">found</span> here will carry
over to general computable agents.
As we saw in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 2, sequence prediction and compression are intimately
related. Indeed, they are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> ways of looking at the same problem.
Intuitively, if you can accurately predict what is coming next you do not need
to use <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> information to encode what the data actually is, and vice versa.
For sequence predictors based on the Minimum Description Length (MDL)
principle (Rissanen, 1996) or the Minimum Message Length (MML) principle
(Wallace and Boulton, 1968), this connection is especially evident as they
predict by attempting to find the shortest possible description, or model, of the
data. Not surprisingly then, they can be viewed as computable approximations
of Solomonoff induction (Section 5.5 of Li and Vitányi, 1997). Furthermore,
in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to produce a working sequence predictor these methods can easily
be combined with general purpose data compressors, such as the Lempel-Ziv
algorithm (Feder et al., 1992) or Context Tree Weighting (Willems et al., 1995).
Unfortunately, while useful in practice, these real <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> compressors have their
limitations: they are able to find some kinds of computable regularities in
sequences, but not others. As such, predictors based on them fall short of the
<span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> and generality of Solomonoff induction. Furthermore, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> with ideal
compression MDL and MML based predictors can <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> exponentially longer
than Solomonoff induction to learn as they use only the shortest description of
the data, rather than the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of all possible descriptions (Poland and Hutter,
2004).
Can we do better than this? Do universal and computable predictors for
computable sequences exist? Unfortunately, it is easy to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that they do not:
simply consider a sequence where the next bit is always the opposite of what
the predictor predicts. This is essentially the same as what Dawid noted when
he <span class="t0 t16 t25 t28 hoverable">found</span> that for any statistical forecasting system there exist sequences for
which the predictor is not calibrated, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be learnt (Dawid, 1985).
However, he does not deal with the complexity of the sequences themselves, nor
does he <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> a precise statement in terms of a specific measure of complexity.
The impossibility of forecasting has since been developed in considerably more

95

5. Limits of Computational Agents
depth by V’yugin (1998), in particular he proves that there is an efficient
randomised procedure producing sequences that <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be predicted, with
high probability, by computable forecasting systems.
In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we study the prediction of computable sequences from the
perspective of Kolmogorov complexity. The central question we look at is the
prediction of sequences which have bounded Kolmogorov complexity. This
leads us to a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> notion of complexity: Rather than the length of the shortest program able to generate a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> sequence, in other <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t24 t25 t26 t27 t28 hoverable">words</span> standard Kolmogorov complexity, we <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the length of the shortest program able to learn
to predict the sequence. This <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> complexity measure has the same fundamental invariance property as Kolmogorov complexity, and <span class="t3 hoverable">certain</span> strong
relationships between the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> measures are proven. Nevertheless, in some
cases the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> can diverge significantly. For example, although a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> random
string that indefinitely repeats has a very high Kolmogorov complexity, this
sequence <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> has a relatively simple structure that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> a simple predictor
can learn to predict.
We then prove that some sequences can only be predicted by very complex
predictors. This implies that very general prediction algorithms, in particular
those that can learn to predict all sequences up to a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> Kolmogorov complexity, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> themselves be complex. This puts an <span class="t7 t10 t15 t25 hoverable">end</span> to our hope of there
being an extremely general and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> relatively simple prediction algorithm. We
then use this fact to prove that although very powerful prediction algorithms
exist, they <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be mathematically discovered due to Gödel incompleteness.
This significantly constrains our ability to design and analyse powerful prediction algorithms, and indeed powerful artificial <span class="t20 hoverable">intelligence</span> algorithms in
general.

5.1. Preliminaries
For the basic notation for strings and sequences <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Appendix A. In this
<span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we will sometimes need to encode a natural number as a string. Using
simple encoding techniques it can be shown that there exists a computable
injective function f : N → B∗ where no string in the range of f is a prefix of
any other, and ∀n ∈ N : ℓ(f (n)) ≤ log2 n + 2 log2 log2 n + 1 = O(log n).
Of particular interest to us will be the class of sequences which can be
generated by an algorithm executed on the following type of machine:
5.1.1 Definition. A monotone universal Turing machine U is defined
as a universal Turing machine with <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> unidirectional input tape, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> unidirectional output tape, and some bidirectional <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> tapes. Input tapes are read
only, output tapes are write only, unidirectional tapes are those where the head
can only move from left to right. All tapes are binary (no blank symbol) and
the <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> tapes are initially filled with zeros. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that U outputs/computes
a sequence ω on input p, and write U (p) = ω, if U reads all of p but no more
as it continues to write ω to the output tape.

96

5.1. Preliminaries
We fix U and define U (p, x) by simply using a standard coding technique
to encode a program p along with a string x ∈ B∗ as a single input string for
U . For simplicity of notation we will often write p(x) to mean the function
computed by the program p when executed on U along with the input string
x, that is, p(x) is short <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span> for U (p, x).
5.1.2 Definition. A sequence ω ∈ B∞ is a computable binary sequence
if there exists a program q ∈ B∗ that writes ω to a one-way output tape when
run on a monotone universal Turing machine U , that is, ∃q ∈ B∗ : U (q) = ω.
We denote the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of all computable sequences by C.
A similar definition for strings is not necessary as all strings have finite
length and are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">therefore</span> trivially computable: all an algorithm has to do is to
copy the desired string to the output tape and then halt.
5.1.3 Definition. A computable binary predictor is a program p ∈ B∗
that on a universal Turing machine U computes a total function B∗ → B.
Having x1:n as input, the objective of a predictor is for its output, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span>
its prediction, to match the next symbol in the sequence. Formally we express
this by writing p(x1:n ) = xn+1 .
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that this is <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> to earlier chapters where a predictor generated
a distribution over the possible symbols that <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> occur next, rather than
outputting the predicted symbol. What we consider here is a special case of
probabilistic prediction as it is equivalent to a probabilistic predictor that in
each cycle always assigns probability 1 to some symbol.
As the algorithmic prediction of incomputable sequences, such as the halting sequence, is impossible by definition, we only consider the problem of
predicting computable sequences. To simplify <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> we will assume that the
predictor has an unlimited supply of computation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> and storage. We will
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> the assumption that the predictor has unlimited data to learn from,
that is, we are only concerned with whether or not a predictor can learn to
predict in the following sense:
5.1.4 Definition.
We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that a predictor p can learn to predict a
sequence ω := x1 x2 . . . ∈ B∞ if there exists m ∈ N such that ∀n ≥ m :
p(x1:n ) = xn+1 .
The existence of m in the above definition need not be constructive, that is,
we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> not <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> when the predictor will stop making prediction errors for
a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> sequence, just that this will occur eventually. This is essentially “next
value” prediction as characterised by Barzdin (1972), which follows the notion
of identifiability in the limit for languages from Gold (1967).
5.1.5 Definition.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> P(ω) be the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of all predictors able T
to learn to
predict ω. Similarly for sets of sequences S ⊂ B∞ , define P(S) := ω∈S P(ω).
97

5. Limits of Computational Agents
A standard measure of complexity for sequences is the length of the shortest
program which generates the sequence:
5.1.6 Definition. For any sequence ω ∈ B∞ the Kolmogorov complexity
of the sequence is,
K(ω) := min∗ { ℓ(q) : U (q) = ω },
q∈B

where U is a monotone universal Turing machine. If no such q exists, we define
K(ω) := ∞.
In essentially the same <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> we can define the Kolmogorov complexity of a
string x ∈ Bn , <span class="t23 hoverable">written</span> K(x), by requiring that U (q) halts after generating x
on the output tape.
It can be shown that Kolmogorov complexity depends on our choice of universal Turing machine U , but only up to an additive constant that is independent of ω. This is due to the fact that a universal Turing machine can
simulate any other universal Turing machine with a fixed length program. For
more explanation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Section 2.4, or for an extensive treatment of Kolmogorov
complexity and some of its applications <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> (Li and Vitányi, 1997) or (Calude,
2002).

5.2. Prediction of computable sequences
The most elementary result is that <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> computable sequence can be predicted
by at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> predictor, and that this predictor need not be significantly more
complex than the sequence to be predicted.
+

5.2.1 Lemma. ∀ω ∈ C, ∃p ∈ P(ω) : K(p) < K(ω).
Proof.
As the sequence ω is computable, there <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> exist at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
algorithm that generates ω. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> q be the shortest such algorithm and construct
an algorithm p that “predicts” ω as follows: Firstly the algorithm p reads x1:n
to find the value of n, then it runs q to generate x1:n+1 and returns xn+1 as its
prediction. Clearly p perfectly predicts ω and ℓ(p) < ℓ(q) + c, for some small
constant c that is independent of ω and q.
2
Not only can any computable sequence be predicted, there <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> exist very
simple predictors able to predict arbitrarily complex sequences:
5.2.2 Lemma. There exists a predictor p such that ∀n ∈ N, ∃ ω ∈ C : p ∈
P(ω) and K(ω) > n.
Proof. <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Take</span> a string x such that K(x) = ℓ(x) ≥ 2n, and from this define a
sequence ω := x0000 . . .. Clearly K(ω) > n and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> a simple predictor p that
always predicts 0 can learn to predict ω.
2

98

5.2. Prediction of computable sequences
The predictor <span class="t27 hoverable">used</span> in the above proof is very simple and can only “learn”
sequences that <span class="t7 t10 t15 t25 hoverable">end</span> with all 0’s, albeit where the initial string can have arbitrarily high Kolmogorov complexity. It <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> seem that this is due to sequences that are initially complex but where the “tail complexity”, defined
lim inf i→∞ K(ωi:∞ ), is zero. This is not the case:
5.2.3 Lemma. There exists a predictor p such that ∀n ∈ N, ∃ ω ∈ C : p ∈
P(ω) and lim inf i→∞ K(ωi:∞ ) > n.
Proof.
A predictor p for eventually periodic sequences can be defined
as follows: On input ω1:k the predictor goes through the ordered pairs
(1, 1), (1, 2), (2, 1), (1, 3), (2, 2), (3, 1), (1, 4), . . . checking for each pair (a, b)
whether the string ω1:k consists of an initial string of length a followed by
a repeating string of length b. On the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> match that is <span class="t0 t16 t25 t28 hoverable">found</span> p predicts that
the repeating string continues, and then p halts. If a + b > k before a match
is found, then p outputs a fixed symbol and halts. Clearly K(p) is a small
constant and p will learn to predict any sequence that is eventually periodic.
For any (m, n) ∈ N2 , <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> ω := x(y ∗ ) where x ∈ Bm , and y ∈ Bn is a random
string, that is, K(y) = n. As ω is eventually periodic p ∈ P(ω) and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span>
that lim inf i→∞ K(ωi:∞ ) = min{K(ωm+1:∞ ), K(ωm+2:∞ ), . . . , K(ωm+n:∞ )}.
For any k ∈ {1, . . . , n} <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> qk∗ be the shortest program that can generate ωm+k:∞ . We can define a halting program qk′ that outputs y where
this program consists of qk∗ , n and k. Thus, ℓ(qk′ ) = ℓ(qk∗ ) + O(log n) =
K(ωk:∞ )+O(log n). As n = K(y) ≤ ℓ(qk′ ), we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that K(ωk:∞ ) > n−O(log n).
As n and k are arbitrary the result follows.
2
Using a more sophisticated version of this proof it can be shown that there
exist predictors that can learn to predict arbitrary regular or primitive recursive sequences. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> wonder whether there exists a computable
predictor able to learn to predict all computable sequences. Unfortunately, no
universal predictor exists, indeed for <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> predictor there exists a sequence
which it <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> predict at all:
5.2.4 Lemma. For any predictor p there constructively exists a sequence
+
ω := x1 x2 . . . ∈ C such that ∀n ∈ N : p(x1:n ) 6= xn+1 and K(ω) < K(p).
Proof. For any computable predictor p there constructively exists a computable sequence ω = x1 x2 x3 . . . computed by an algorithm q defined as follows: <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">Set</span> x1 = 1 − p(λ), then x2 = 1 − p(x1 ), then x3 = 1 − p(x1:2 ) and so on.
Clearly ω ∈ C and ∀n ∈ N : p(x1:n ) = 1 − xn+1 .
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> p∗ be the shortest program that computes the same function as p and
define a sequence generation algorithm q ∗ based on p∗ using the procedure
above. By construction, ℓ(q ∗ ) = ℓ(p∗ ) + c for some constant c that is independent of p∗ . Because q ∗ generates ω, it follows that K(ω) ≤ ℓ(q ∗ ). By definition
+
K(p) = ℓ(p∗ ) and so K(ω) < K(p).
2

99

5. Limits of Computational Agents
Allowing the predictor to be probabilistic, as we did in previous chapters,
does not fundamentally avoid the problem of Lemma 5.2.4. In each step, rather
than generating the opposite to what will be predicted by p, instead q attempts
to generate the symbol that p is least likely to predict <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> x1:n . To do this
q <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> simulate p in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to estimate the probability that p(x1:n ) = 1. With
sufficient simulation effort, q can estimate this probability to any desired accuracy for any x1:n . This produces a computable sequence ω such that ∀n ∈ N
the probability that p(x1:n ) = xn+1 is not significantly greater than 12 , that
is, the performance of p is no better than a predictor that makes completely
random predictions. As probabilistic prediction complicates <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span>
avoiding this problem, in this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we will consider only deterministic predictors. This will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> allow us to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> the root of this problem as clearly as
possible.
With the preliminaries covered, we now move on to the central problem:
predicting sequences of limited Kolmogorov complexity.

5.3. Prediction of simple computable sequences
As the computable prediction of any computable sequence is impossible, a
weaker goal is to be able to predict all “simple” computable sequences.
5.3.1 Definition. For n ∈ N, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> Cn := {ω ∈ C : K(ω) ≤ n}. Further, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span>
Pn := P(Cn ) be the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of predictors able to learn to predict all sequences in
Cn .
Firstly, we establish that prediction algorithms exist that can learn to predict
all sequences up to a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> complexity, and that these predictors need not be
significantly more complex than the sequences they can predict:
+

5.3.2 Lemma. ∀n ∈ N, ∃p ∈ Pn : K(p) < n + O(log n).
Proof.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> h ∈ N be the number of programs of length n or less which
generate infinite sequences. Build the value of h into a prediction algorithm p
constructed as follows:
In the k th prediction cycle run in parallel all programs of length n or less until
h of these programs have each produced k + 1 symbols of output. Next predict
<span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the k + 1th symbol of the generated string <span class="t3 t4 t5 t6 t8 t10 t11 t12 t13 t14 t18 t19 t21 t23 t24 t25 t26 t27 hoverable">whose</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> k symbols
is consistent with the observed string. If more than <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> generated string is
consistent with the observed sequence, pick the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> which was generated by
the program that occurs <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> in a lexicographical ordering of the programs. If
no generated output is consistent, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t28 t29 hoverable">give</span> up and output a fixed symbol.
For sufficiently large k, only the h programs which produce infinite sequences
will produce output strings of length k + 1. As this <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of sequences is finite,
they can be uniquely identified by finite initial strings. Thus, for sufficiently

100

5.3. Prediction of simple computable sequences
large k, the predictor p will correctly predict any computable sequence ω for
which K(ω) ≤ n, that is, p ∈ Pn .

As there are 2n+1 − 1 possible strings of length n or less, h < 2n+1 and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
we can encode h with log2 h + 2 log2 log2 h = n + 1 + 2 log2 (n + 1) bits. Thus,
K(p) < n + 1 + 2 log2 (n + 1) + c for some constant c that is independent of
n.
2
Can we do better than this? Lemmas 5.2.2 and 5.2.3 show us that there
exist predictors able to predict at least some sequences vastly more complex
than themselves. This suggests that there <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> exist simple predictors able
to predict arbitrary sequences up to a high complexity. Formally, <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> there
exist p ∈ Pn where n ≫ K(p)? Unfortunately, these simple but powerful
predictors are not possible:

+

5.3.3 Theorem. ∀n ∈ N : p ∈ Pn ⇒ K(p) > n.
Proof.
For any n ∈ N <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> p ∈ Pn , that is, ∀ω ∈ Cn : p ∈ P(ω). By
Lemma 5.2.4 we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that ∃ ω ′ ∈ C : p ∈
/ P(ω ′ ) . As p ∈
/ P(ω ′ ) it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be the
′
′
case that ω ∈
/ Cn , that is, K(ω ) ≥ n. From Lemma 5.2.4 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that
+
K(p) > K(ω ′ ) and so the result follows.
2
Intuitively the reason for this is as follows: Lemma 5.2.4 guarantees that
<span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> simple predictor fails for at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> simple sequence. Thus, if we want
a predictor that can learn to predict all sequences up to a moderate level of
complexity, then clearly the predictor <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be simple. Likewise, if we want a
predictor that can predict all sequences up to a high level of complexity, then
the predictor itself <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be very complex. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> we have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> the
generous assumption of unlimited computational resources and data to learn
from, only very complex algorithms can be truly powerful predictors.
These results easily generalise to notions of complexity that <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> computation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> into consideration. As sequences are infinite, the appropriate measure of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> is the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> needed to generate or predict the next symbol in the
sequence. Under any reasonable measure of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> complexity, the operation of
inverting a single output from a binary valued function can be performed with
little cost. If C is any complexity measure with this property, it is trivial to
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the proof of Lemma 5.2.4 still holds for C. From this, an analogue of
Theorem 5.3.3 for C easily follows.
With similar arguments these results <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> generalise, in a straightforward
way, to complexity measures that <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> space or other computational resources
into account. Thus, the fact that extremely powerful predictors <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be very
complex, holds under any measure of complexity for which inverting a single
bit is inexpensive.

101

5. Limits of Computational Agents

5.4. Complexity of prediction
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> of viewing these results is in terms of an alternate notion of
sequence complexity defined as the size of the smallest predictor able to learn
to predict the sequence. This allows us to express the results of the previous
sections more concisely. Formally, for any sequence ω define the complexity
measure,
K̇(ω) := min∗ { ℓ(p) : p ∈ P(ω) },
p∈B

and K̇(ω) := ∞ if P(ω) = ∅. Thus, if K̇(ω) is high then the sequence ω is
complex in the sense that only complex prediction algorithms are able to learn
to predict it. It can easily be seen that this notion of complexity has the same
invariance to the choice of reference universal Turing machine as the standard
Kolmogorov complexity measure.
It <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be tempting to conjecture that this definition simply describes what
<span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> the “tail complexity” of a sequence, that is, K̇(ω) is equal to
lim inf i→∞ K(ωi:∞ ). This is not the case. In the proof of Lemma 5.2.3 we saw
that there exists a single predictor capable of learning to predict any sequence
that consists of a repeating string, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> for these sequences K̇ is bounded.
It was further shown that there exist sequences of this <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> with arbitrarily
high tail complexity. Clearly then tail complexity and K̇ <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be equal in
general.
Using K̇ we can now rewrite a number of our previous results <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more
succinctly. From Lemma 5.2.1 it immediately follows that,
+

∀ω : 0 ≤ K̇(ω) < K(ω).
From Lemma 5.2.2 we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that ∃c ∈ N, ∀n ∈ N, ∃ ω ∈ C such that K̇(ω) < c
and K(ω) > n, that is, K̇ can attain the lower bound above <span class="t5 t7 hoverable">within</span> a small
constant, no matter how large the value of K is. The sequences for which the
upper bound on K̇ is tight are interesting as they are the ones which demand
complex predictors. We prove the existence of these sequences and look at
some of their properties in the next section.
The complexity measure K̇ can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be generalised to sets of sequences, for
S ⊂ B∞ define K̇(S) := minp { ℓ(p) : p ∈ P(S) }. This allows us to rewrite
Lemma 5.3.2 and Theorem 5.3.3 as simply,
+

+

∀n ∈ N : n < K̇(Cn ) < n + O(log n).
This is just a restatement of the fact that the simplest predictor capable of
predicting all sequences up to a Kolmogorov complexity of n, has itself a
Kolmogorov complexity of roughly n.
Perhaps the most surprising thing about K̇ complexity is that this very natural definition of the complexity of a sequence, as viewed from the perspective
of prediction, does not appear to have been studied before.

102

5.5. Hard to predict sequences

5.5. Hard to predict sequences
We have already seen that some individual sequences, such as the repeating
string <span class="t27 hoverable">used</span> in the proof of Lemma 5.2.3, can have arbitrarily high Kolmogorov
complexity but nevertheless can be predicted by trivial algorithms. Thus,
although these sequences contain a lot of information in the Kolmogorov sense,
in a deeper sense their structure is very simple and easily learnt.
What interests us in this section is the other extreme: individual sequences
that can only be predicted by complex predictors. As we are concerned with
prediction in the limit, this extra complexity in the predictor <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be some
kind of special information which <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be learnt through observing the sequence. Our <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> task is to show that these hard to predict sequences exist.
+

+

+

5.5.1 Theorem. ∀n ∈ N, ∃ ω ∈ C : n < K̇(ω) < K(ω) < n + O(log n).
Proof. For any n ∈ N, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> Qn ⊂ B<n be the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of programs shorter than n
that are predictors, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> x1:k ∈ Bk be the observed initial string from the
sequence ω that is to be predicted. Now construct a meta-predictor p̂:
By dovetailing the computations, run in parallel <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> program of length
less than n on <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> string in B≤k . Each <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> a program is <span class="t0 t16 t25 t28 hoverable">found</span> to halt on
all of these input strings, add the program to a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of “candidate prediction
algorithms”, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> Q̃kn . As each element of Qn is a valid predictor, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
halts for all input strings in B∗ by definition, for <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> n and k it eventually
will be the case that |Q̃kn | = |Qn |. At this point the simulation to approximate
Qn terminates. It is clear that for sufficiently large values of k all of the valid
predictors, and only the valid predictors, will halt with a single symbol of
output on all tested input strings. That is, ∃r ∈ N, ∀k > r : Q̃kn = Qn .
The <span class="t6 hoverable">second</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the p̂ algorithm uses these candidate
Pk−1prediction algorithms to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> a prediction. For p ∈ Q̃kn define dk (p) := i=1 |p(x1:i ) − xi+1 |.
Informally, dk (p) is the number of prediction errors <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> by p so far. Compute this for all p ∈ Q̃kn and then <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> p∗k ∈ Q̃kn be the program with minimal
dk (p). If there is more than <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> such program, break the tie by letting p∗k be
the lexicographically <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> of these. Finally, p̂ computes the value of p∗k (x1:k )
and then returns this as its prediction and halts.
By Lemma 5.2.4, there exists ω ′ ∈ C such that p̂ makes a prediction error
for <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> k when trying to predict ω ′ . Thus, in each cycle at least <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of
the finitely <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> predictors with minimal dk makes a prediction error and
so ∀p ∈ Qn : dk (p) → ∞ as k → ∞. Therefore, ∄p ∈ Qn : p ∈ P(ω ′ ),
that is, no program of length less than n can learn to predict ω ′ and so n ≤
+
K̇(ω ′ ). Further, from Lemma 5.2.1 we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that K̇(ω ′ ) < K(ω ′ ), and from
+
Lemma 5.2.4 again, K(ω ′ ) < K(p̂).
Examining the algorithm for p̂, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that it contains some fixed length
program code and an encoding of |Qn |, where |Qn | < 2n − 1. Thus, using a
+
standard encoding method for integers, K(p̂) < n + O(log n). Chaining these,
+
+
+
+
n < K̇(ω ′ ) < K(ω ′ ) < K(p̂) < n + O(log n), which proves the theorem.
2

103

5. Limits of Computational Agents
This establishes the existence of sequences with arbitrarily high K̇ complexity which <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> have a similar level of Kolmogorov complexity. Next we
establish a fundamental property of high K̇ complexity sequences: they are
extremely difficult to compute.
For an algorithm q that generates ω ∈ C, define tq (n) to be the number of
computation steps performed by q before the nth symbol of ω is <span class="t23 hoverable">written</span> to the
output tape. For example, if q is a simple algorithm that outputs the sequence
010101 . . ., then clearly tq (n) = O(n) and so ω can be computed quickly. The
following theorem proves that if a sequence can be computed in a reasonable
amount of time, then the sequence <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> have a low K̇ complexity:
5.5.2 Lemma. ∀ω ∈ C, if ∃q : U (q) = ω and ∃r ∈ N, ∀n > r : tq (n) < 2n ,
+
then K̇(ω) = 0.
Proof. Construct a prediction algorithm p̃ as follows:
On input x1:n , run all programs of length n or less, each for 2n+1 steps. In
a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> Wn collect <span class="t16 t21 hoverable">together</span> all generated strings which are at least n + 1 symbols
<span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> and where the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> n symbols match the observed string x1:n . Now <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span>
the strings in Wn <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to a lexicographical ordering of their generating
programs. If Wn = ∅, then just return a prediction of 1 and halt. If |Wn | > 1
then return the n + 1th symbol from the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> sequence in the above ordering.
Assume that ∃q : U (q) = ω such that ∃r ∈ N, ∀n > r : tq (n) < 2n . If q is
not unique, <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> q to be the lexicographically <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> of these. Clearly ∀n > r
the initial string from ω generated by q will be in the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> Wn . As there
is no lexicographically lower program which can generate ω <span class="t5 t7 hoverable">within</span> the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span>
constraint tq (n) < 2n for all n > r, for sufficiently large n the predictor p̃ <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
converge on using q for each prediction and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> p̃ ∈ P(ω). As ℓ(p̃) is clearly
+
a fixed constant that is independent of ω, it follows then that K̇(ω) < ℓ(p̃) =
0.
2
We <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> replace the 2n bound in the above result with any monotonically
n
growing computable function, for example, 22 . In any case, this does not
change the fundamental result that sequences which have a high K̇ complexity
are practically impossible to compute. However, from our theoretical perspective, these sequences present no problem as they can be predicted, albeit with
immense difficulty.

5.6. The limits of mathematical analysis
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to interpret the results of the previous sections is in terms of constructive theories of prediction. Essentially, a constructive theory of prediction
expressed in some sufficiently rich formal system F, is in effect a description
of a prediction algorithm with respect to a universal Turing machine which
implements the required parts of F. Thus, from Theorems 5.3.3 and 5.5.1,
it follows that if we want to have a predictor that can learn to predict all

104

5.6. The limits of mathematical analysis
sequences up to a high level of Kolmogorov complexity, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> just predict
individual sequences which have high K̇ complexity, the constructive theory of
prediction that we base our predictor on <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be very complex. Elegant and
highly general constructive theories of prediction simply do not exist, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if
we assume unlimited computational resources. This is in marked contrast to
Solomonoff’s highly elegant but non-constructive theory of prediction.
Naturally, highly complex theories of prediction will be very difficult to
mathematically analyse, if not practically impossible. Thus, at some point
the development of very general prediction algorithms <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> <span class="t8 t13 hoverable">become</span> mainly
an experimental endeavour due to the difficulty of working with the required
theory. Interestingly, an <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> stronger result can be proven showing that
beyond some point the mathematical analysis is in fact impossible, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> in
theory:
5.6.1 Theorem. In any consistent formal axiomatic system F that is sufficiently rich to express statements of the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> “p ∈ Pn ”, there exists m ∈ N
such that for all n > m and for all predictors p ∈ Pn the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> statement
“p ∈ Pn ” <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be proven in F.
In other words, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> we have proven that very powerful sequence
prediction algorithms exist, beyond a <span class="t3 hoverable">certain</span> complexity it is impossible to
find any of these algorithms using mathematics. The proof has a similar
structure to Chaitin’s information theoretic proof (Chaitin, 1982) of Gödel’s
incompleteness theorem for formal axiomatic systems (Gödel, 1931).
Proof.
For each n ∈ N <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> Tn be the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of statements expressed in the
formal system F of the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> “p ∈ Pn ”, where p is filled in with the complete
description of some algorithm in each case. As the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of programs is denumerable, Tn is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> denumerable and each element of Tn has finite length. From
Lemma 5.3.2 and Theorem 5.3.3 it follows that each Tn contains infinitely
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> statements of the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> “p ∈ Pn ” which are true.
Fix n and create a search algorithm s that enumerates all proofs in the
formal system F searching for a proof of a statement in the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> Tn . As the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span>
Tn is recursive, s can always recognise a proof of a statement in Tn . If s finds
any such proof, it outputs the corresponding program p and then halts.
By <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> of contradiction, assume that s halts, that is, a proof of a theorem
in Tn is <span class="t0 t16 t25 t28 hoverable">found</span> and p such that p ∈ Pn is generated as output. The size of
the algorithm s is a constant (a description of the formal system F and some
proof enumeration code) as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as an O(log n) term needed to describe n. It
+
follows then that K(p) < O(log n). However, from Theorem 5.3.3 we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span>
+
that K(p) > n. Thus, for sufficiently large n, we have a contradiction and
so our assumption of the existence of a proof <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be false. That is, for
sufficiently large n and for all p ∈ Pn , the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> statement “p ∈ Pn ” <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be
proven <span class="t5 t7 hoverable">within</span> the formal system F.
2

105

5. Limits of Computational Agents
The exact value of m depends on our choice of formal system F and which
reference machine U we measure complexity with respect to. However, for
reasonable choices of F and U the value of m <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be in the <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> of 1000.
That is, the bound m is certainly not so large as to be vacuous.

5.7. Conclusion
We have shown that there does not exist an elegant constructive theory of
prediction for computable sequences, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if we assume unbounded computational resources, unbounded data and learning time, and <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> moderate
bounds on the Kolmogorov complexity of the sequences to be predicted. Very
powerful computable predictors are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">therefore</span> necessarily complex. We have
further shown that the source of this problem is the existence of computable
sequences which are extremely expensive to compute. While we have proven
that very powerful prediction algorithms which can learn to predict these sequences exist, we have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> proven that, unfortunately, mathematical analysis
<span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be <span class="t27 hoverable">used</span> to discover these algorithms due to Gödel incompleteness.
These results can be extended to more general settings, specifically to those
problems which are equivalent to, or depend on, sequence prediction. Consider,
for example, a reinforcement learning agent interacting with an environment,
as described in Chapters 2 and 3. In each interaction cycle the agent <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
choose its actions so as to maximise the future rewards that it receives from
the environment. Of course the agent <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> for <span class="t3 hoverable">certain</span> if some action
will lead to rewards in the future. Whether explicitly or implicitly, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span>
somehow predict these. Thus, at the <span class="t0 t1 t4 t8 t9 t10 t11 t12 t16 t19 t22 t23 t25 t26 hoverable">heart</span> of reinforcement learning lies a
prediction problem, and so the results for computable predictors presented in
this paper <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> apply to computable reinforcement learners. More specifically,
from Theorem 5.3.3 it follows that very powerful computable reinforcement
learners are necessarily complex, and from Theorem 5.6.1 it follows that it is
impossible to discover any of these extremely powerful reinforcement learning
algorithms mathematically. These relationships are illustrated in Figure 5.1.
It is reasonable to ask whether the assumptions we have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">made</span> in our model
need to be changed. If we increase the <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> of the predictors further, for
example by providing them with some kind of an oracle, this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> the
predictors <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more unrealistic than they currently are. This goes against
our goal of finding an elegant, powerful and general prediction theory that is
more realistic in its assumptions than Solomonoff’s incomputable model. On
the other hand, if we weaken our assumptions about the predictors’ resources
to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> them more realistic, we are in effect taking a subset of our current
class of predictors. As such, all the same limitations and problems will still
apply, as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as some <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> ones.
It seems then that the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> forward is to further restrict the problem space.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> possibility <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to bound the amount of computation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> needed to
generate the next symbol in the sequence. However, if we do this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> re-

106

5.7. Conclusion
stricting the predictors’ resources then the simple predictor from Lemma 5.5.2
easily learns to predict any such sequence and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the problem of prediction
in the limit has <span class="t8 t13 hoverable">become</span> trivial. <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> possibility <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to bound the
memory of the machine <span class="t27 hoverable">used</span> to generate the sequence, <span class="t27 t28 hoverable">however</span> this makes the
generator a finite state machine and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> bounds its computation time, again
making the problem trivial.
Perhaps the only reasonable solution <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to add additional restrictions
to both the algorithms which generate the sequences to be predicted, and to
the predictors. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> want to consider not just learnability in the limit,
but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> how quickly the predictor is able to learn. Of course we are then
facing a <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more difficult analysis problem.

107

5. Limits of Computational Agents

Figure 5.1.: Theorem 5.3.3 rules out simple but powerful artificial <span class="t20 hoverable">intelligence</span>
algorithms, as indicated by the greyed out region in the upper
left. Theorem 5.6.1 upper bounds how powerful an algorithm can
be before it can no longer be proven to be a powerful algorithm.
This is indicated by the horizontal line separating the region of
provable algorithms from the region of Gödel incompleteness.

108

6. Temporal Difference Updating
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> a Learning Rate
In Chapters 2 and 3 we saw how universal agents are able to learn to behave
optimally across a wide range of environments. Unfortunately, these agents
are incomputable as they are based on incomputable universal prior distributions. Thus, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to use the theory of universal artificial <span class="t20 hoverable">intelligence</span> to
design and build practical algorithms, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> find a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to scale the
theory down. In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 5 we investigated some of the constraints faced when
attempting this. What we uncovered was a number of fundamental negative
results. In short, computable predictors capable of predicting all sequences up
to a moderate Kolmogorov complexity are both highly complex and mathematically impossible to find due to Gödel incompleteness. The only <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> out
of this bind, it seems, is to move to a more sophisticated measure of complexity that takes not only information content into account, but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> and
space. Unfortunately, the theory of resource bounded complexity is notoriously difficult to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with and has <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> unsolved fundamental questions.
Furthermore, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if the universal prior distribution <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be replaced by a
suitable computable prior, perhaps something <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> the Speed prior (Schmidhuber, 2002), there still remains the fact that AIXI’s search through possible
futures requires computation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> that is exponential in the depth of the look
ahead (see the equation for AIXI in Section 2.10).
Despite these difficulties, several attempts at scaling AIXI down have been
made. The most theoretically founded of these is AIXItl (Chapter 7 of Hutter,
2005). In this model, proof search is <span class="t27 hoverable">used</span> to limit the size and computation
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> of the algorithm. Unfortunately, although technically computable, the
resulting agent still requires impossibly vast computational resources. <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span>
more drastic scaling down of AIXI did produce a usable algorithm (Poland and
Hutter, 2006). Here the problem domain was limited to games that <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be
described by 2 × 2 matrices, and the look ahead was bounded to 8 interaction
cycles. The resulting algorithm was able to learn simple game theoretic interaction strategies. While this proves that some kind of scaling down of AIXI is
possible, the problem space of 2 × 2 matrix games falls <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> short of what is
needed for a useful artificial <span class="t20 hoverable">intelligence</span> algorithm.
In this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we present what started out as <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> attempt to scale
AIXI down. As we have seen in previous chapters, at the core of the reinforcement learning problem lies the problem of estimating the expected future
discounted reward. We begin by expressing this estimation problem as a loss
function, more specifically, as the squared difference between the empirical

109

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE
future discounted reward and our estimate. We then derive an equation for
this estimator by minimising the loss function. Although the resulting learning
equations no longer bear <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> resemblance to AIXI, they do strongly resemble
the standard equation for temporal difference learning with eligibility traces,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t6 t10 t15 hoverable">known</span> as the TD(λ) algorithm. Interestingly, while the standard algorithm has a free learning rate parameter, in our <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> equation there is none. In
its <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> there is an equation that automatically sets the learning rate in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span>
that is specific to each state transition. We have experimentally tested this <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span>
learning rule against TD(λ) and <span class="t0 t16 t25 t28 hoverable">found</span> that it offers superior performance in
various settings. We have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> extended the algorithm to reinforcement learning and again <span class="t0 t16 t25 t28 hoverable">found</span> encouraging results. This <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> covers the derivation
of this algorithm and our experimental results.
<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that while the notation <span class="t27 hoverable">used</span> in this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> is fairly standard for the
temporal difference learning literature, it is a little <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> to what we <span class="t27 hoverable">used</span>
to define AIXI. For example, we now talk of states rather than observations,
and index the value function in a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> way.

6.1. Temporal difference learning
In the field of reinforcement learning, perhaps the most popular <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to estimate the future discounted reward of states is the method of temporal difference learning. It is unclear who exactly introduced this first, <span class="t27 t28 hoverable">however</span> the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span>
explicit version of temporal difference as a learning rule appears to be Witten
(1977). The idea is as follows: The expected future discounted reward of a state
s is,

	
V s := E rk + γrk+1 + γ 2 rk+2 + · · · |sk = s ,

where the rewards rk , rk+1 , . . . are geometrically discounted into the future by
γ < 1. From this definition it follows that,

	
V s = E rk + γV sk+1 |sk = s .
(6.1)

Our task, at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t, is to compute an estimate Vst of V s for each state s.
The only information we have to base this estimate on is the current history
of state transitions, s1 , s2 , . . . , st , and the current history of observed rewards,
r1 , r2 , . . . , rt . Equation (6.1) suggests that at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t + 1 the value of rt + γVst+1
provides us with information on what Vst should be: if it is higher than Vstt
then perhaps this estimate should be increased, and vice versa. This intuition
gives us the following estimation heuristic for state st ,


t
t
t
Vst+1
:=
V
+
α
r
+
γV
−
V
t
st
st+1
st ,
t

where α is a parameter that controls the rate of learning. This type of temporal
difference learning is <span class="t6 t10 t15 hoverable">known</span> as TD(0).
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> shortcoming of this method is that at each <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> step the value of only
the last state st is updated. States before the last state are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> affected by

110

6.1. Temporal difference learning
Algorithm 1 TD(λ)
Initialise V (s) arbitrarily and E(s) = 0 for all s
Initialise s
repeat
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Make</span> state transition and observe r, s′
∆ ← r + γV (s′ ) − V (s)
E(s) ← E(s) + 1
for all s do
V (s) ← V (s) + αE(s)∆
E(s) ← γλE(s)
<span class="t7 t10 t15 t25 hoverable">end</span> for
s ← s′
until <span class="t7 t10 t15 t25 hoverable">end</span> of run
changes in the last state’s value and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> these <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be updated too. This is
what happens with so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> temporal difference learning with eligibility traces,
where a history, or trace, is kept of which states have been recently visited.
Under this method, when we update the value of a state we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> go back
through the trace updating the earlier states as well. Formally, for any state
s its eligibility trace is computed by,

γλEst−1
if s 6= st ,
Est :=
γλEst−1 + 1 if s = st ,
where λ is <span class="t27 hoverable">used</span> to control the rate at which the eligibility trace is discounted.
The temporal difference update is then, for all states s,


Vst+1 := Vst + αEst r + γVstt+1 − Vstt .
(6.2)

This more powerful version of temporal <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> learning is <span class="t6 t10 t15 hoverable">known</span> as TD(λ)
(Sutton, 1988). The complete algorithm appears in Algorithm 1.
Although it has been successfully applied to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> simple problems, plain
TD(λ) has a number of drawbacks. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of these is that the learning rate
parameter α has to be experimentally tuned by hand. Indeed, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> this is
not always enough, for optimal performance some monotonically decreasing
function has to be experimentally <span class="t0 t16 t25 t28 hoverable">found</span> that decreases the learning rate over
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> at the <span class="t27 hoverable">right</span> rate. If the learning rate decreases too quickly, the system
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> <span class="t8 t13 hoverable">become</span> stuck at a level of sub-optimal performance, if it decreases too
slowly then convergence will be unnecessarily late. The main contribution of
this <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> is to solve this problem by deriving a temporal difference rule
from statistical principles that automatically sets its learning rate.
Perhaps the closest <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> to ours is the LSTD(λ) algorithm (Bradtke and
Barto, 1996; Boyan, 1999; Lagoudakis and Parr, 2003). LSTD(λ) is concerned
with finding a least-squares linear function approximation to the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> value

111

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE
function. The unknown expected rewards and transition probabilities are replaced by empirical averages up to current <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t. In contrast, we consider
finite state spaces and no function approximation. We derive a least-squares
estimate of the empirical values including future rewards by bootstrapping.
The computation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> for our update is linear in the number of states, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span>
TD(λ), while LSTD is quadratic (even in the case of state-indicator features
and no function approximation). Indeed our algorithm exactly coincides with
TD/Q/Sarsa(λ) but with a novel learning rate derived from statistical principles. LSTD has not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> been developed for general λ and γ. Our algorithm and
LSTD both get rid of the learning rate and the necessity to initialise V . Since
LSTD has primarily been developed for linear function approximation and has
a <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more expensive update rule, we focused our experimental comparison
to the algorithms for which we determined the learning rate (finite state space,
linear <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> TD/Q/Sarsa algorithms). It remains to be seen how our approach
generalises to (linear) function approximation.

6.2. Derivation
The empirical future discounted reward of a state sk is the sum of actual
rewards following from state sk in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> steps k, k + 1, . . ., where the rewards
are discounted as they go into the future. Formally, the empirical value of
state sk at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> k for k = 1, ..., t is,
vk :=

∞
X

γ u−k ru ,

(6.3)

u=k

where the future rewards ru are geometrically discounted by γ < 1. In practice
the exact value of vk is always unknown to us as it depends not only on rewards
that have been already observed, but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> on unknown future rewards. <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span>
that if sm = sn for m 6= n, that is, we have visited the same state twice at
<span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> times m and n, this does not imply that vn = vm as the observed
rewards following the state visit <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> each time.
Our goal is that for each state s the estimate Vst should be as close as possible
to the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> expected future discounted reward V s . Thus, for each state s we
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> Vs to be close to vk for all k such that s = sk . Furthermore, in
non-stationary environments we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> to discount <span class="t25 hoverable">old</span> evidence by some
parameter λ ∈ (0, 1]. Formally, we want to minimise the loss function,
t

L :=

2
1 X t−k
λ
vk − Vstk .
2
k=1

For stationary environments we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> simply <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> λ = 1 a priori.

112

(6.4)

6.2. Derivation
As we wish to minimise this loss, we <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the partial derivative with respect
to the value estimate of each state and <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> to zero,
t

t

t

k=1

k=1

k=1

X
X
X

∂L
λt−k vk − Vstk δsk s = Vst
=−
λt−k δsk s vk = 0,
λt−k δsk s −
t
∂Vs
where we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> change Vstk into Vst due to the presence of the Kronecker δsk s ,
defined δxy := 1 if x = y, and 0 otherwise. By defining a discounted state visit
Pt
counter Nst := k=1 λt−k δsk s we get
Vst Nst =

t
X

λt−k δsk s vk .

(6.5)

k=1

Since vk depends on future rewards rk , Equation (6.5) can not be <span class="t27 hoverable">used</span> in
its current form. Next we <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">note</span> that vk has a self-consistency property with
respect to the rewards. Specifically, the tail of the future discounted reward
sum for each state depends on the empirical value at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t in the following
way,
t−1
X
γ u−k ru + γ t−k vt .
vk =
u=k

Substituting this into Equation (6.5) and exchanging the <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> of the double
sum,
Vst Nst

=

u
t−1 X
X

λt−k δsk s γ u−k ru +

u=1 k=1

=

t−1
X

u=1

=

λt−u

t
X

λt−k δsk s γ t−k vt

k=1
u
X

(λγ)u−k δsk s ru +

t
X

(λγ)t−k δsk s vt

k=1

k=1

Rst + Est vt ,

Pt
where Est := k=1 (λγ)t−k δsk s is the eligibility trace of state s, and Rst :=
Pt−1 t−u
Esu ru is the discounted reward with eligibility.
u=1 λ
Est and Rst depend only on quantities <span class="t6 t10 t15 hoverable">known</span> at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t. The only unknown
quantity is vt , which we have to replace with our current estimate of this value
at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t, which is Vstt . In other words, we bootstrap our estimates. This gives
us,
(6.6)
Vst Nst = Rst + Est Vstt .
For state s = st , this simplifies to
Vstt =

Rst t
.
− Estt

Nstt

113

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE
Substituting this back into Equation (6.6) we obtain,
Vst Nst = Rst + Est

Rst t
.
Nstt − Estt

(6.7)

This gives us an explicit expression for our V estimates. However, from an
algorithmic perspective an incremental update rule is more convenient. To
derive this we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> use of the relations,
Nst+1 =

λNst + δst+1 s ,

Ns0 = 0,

Est+1 =
Rst+1 =

λγEst + δst+1 s ,
λRst + λEst rt ,

Es0 = 0,
Rs0 = 0,

Inserting these into Equation (6.7) with t replaced by t + 1,
Vst+1 Nst+1

=
=

Rst+1 + Est+1

Rst+1
t+1

t+1
Nst+1
t+1 − Est+1
Rst + Estt+1 rt
.
λRst + λEst rt + Est+1 tt+1
Nst+1 − γEstt+1

By solving Equation (6.6) for Rst and substituting back in,
Nst Vst − Estt+1 Vstt + Estt+1 rt

Vst+1 Nst+1 = λ Vst Nst − Est Vstt + λEst rt + Est+1 t+1 t+1t
Nst+1 − γEstt+1

= λNst + δst+1 s Vst − δst+1 s Vst − λEst Vstt + λEst rt
+ Est+1

Nstt+1 Vstt+1 − Estt+1 Vstt + Estt+1 rt
Nstt+1 − γEstt+1

.

Dividing through by Nst+1 (= λNst + δst+1 s ),
Vst+1 = Vst +

+

−δst+1 s Vst − λEst Vstt + λEst rt
λNst + δst+1 s
(λγEst + δst+1 s )(Nstt+1 Vstt+1 − Estt+1 Vstt + Estt+1 rt )
(Nstt+1 − γEstt+1 )(λNst + δst+1 s )

.

Making the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> denominator the same as the second, then expanding the
numerator,
Vst+1 = Vst +

+

114

λEst rt Nstt+1 − λEst Vstt Nstt+1 − δst+1 s Vst Nstt+1 − λγEstt+1 Est rt
(Nstt+1 − γEstt+1 )(λNst + δst+1 s )

λγEstt+1 Est Vstt + γEstt+1 Vst δst+1 s + λγEst Nstt+1 Vstt+1 − λγEst Estt+1 Vstt
(Nstt+1 − γEstt+1 )(λNst + δst+1 s )

6.3. Estimating a small Markov process
+

λγEst Estt+1 rt + δst+1 s Nstt+1 Vstt+1 − δst+1 s Estt+1 Vstt + δst+1 s Estt+1 rt
(Nstt+1 − γEstt+1 )(λNst + δst+1 s )

.

After cancelling equal terms (keeping in <span class="t0 t1 t2 t3 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t24 t25 t27 t28 t29 hoverable">mind</span> that in <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> term with a Kronecker δxy factor we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> assume that x = y as the term is always zero otherwise), and factoring out Est the <span class="t27 hoverable">right</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span> side becomes,

Est λrt Nstt+1−λVstt Nstt+1+ γVst δst+1 s +λγNstt+1Vstt+1−δst+1 s Vstt +δst+1 s rt
t
Vs +
(Nstt+1 − γEstt+1 )(λNst + δst+1 s )
Finally, by factoring out λNstt+1 + δst+1 s we obtain our update rule,

Vst+1 = Vst + Est βt (s, st+1 ) rt + γVstt+1 − Vstt ,

(6.8)

where the learning rate is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by,
βt (s, st+1 ) :=

Nstt+1
1
.
Nstt+1 − γEstt+1 Nst

(6.9)

Examining Equation (6.8), we find the usual update equation for temporal
difference learning with eligibility traces (see Equation (6.2)), <span class="t27 t28 hoverable">however</span> the
learning rate α has now been replaced by βt (s, st+1 ). This learning rate was
derived by minimising the squared loss between the estimated and <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> state
value. In the derivation we have exploited the fact that the latter <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be
self-consistent and then bootstrapped to get Equation (6.6). This gives us an
equation for the learning rate for each state transition at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> t, as opposed to
the standard temporal difference learning where the learning rate α is either
a fixed free parameter for all transitions, or is decreased over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> by some
monotonically decreasing function. In either case, the learning rate is not
automatic and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be experimentally tuned for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> performance. The above
derivation appears to theoretically solve this problem.
The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> term in βt seems to provide some type of normalisation to the
learning rate, <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> the intuition behind this is not clear to us. The <span class="t6 t13 t14 t25 t27 hoverable">meaning</span>
of <span class="t6 hoverable">second</span> term <span class="t27 t28 hoverable">however</span> can be understood as follows: Nst measures how often
we have visited state s in the recent past. Therefore, if Nst ≪ Nstt+1 then
state s has a value estimate based on relatively few samples, while state st+1
has a value estimate based on relatively <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> samples. In such a situation,
the <span class="t6 hoverable">second</span> term in βt boosts the learning rate so that Vst+1 moves more aggressively towards the presumably more accurate rt + γVstt+1 . In the opposite
situation when st+1 is a less visited state, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the reverse occurs and
the learning rate is reduced in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to maintain the existing value of Vs .

6.3. Estimating a small Markov process
For our <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> test we consider a small Markov process with 21 states. In each
step the state number is either incremented or decremented by <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> with equal

115

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE

0.30

0.30
HL(1.0)
TD(0.7) a = 0.07
TD(0.7) a = 0.13

0.25

0.20
RMSE

RMSE

0.20
0.15

0.15

0.10

0.10

0.05

0.05

0.00
0.0

HL(1.0)
TD(0.7) a = 0.07
TD(0.7) a = 0.13

0.25

0.2

0.4

0.6
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

0.8

1.0

x1e+4

Figure 6.1.: 21 state Markov process,
average performance over
10 runs.

0.00
0.0

0.2

0.4

0.6
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

0.8

1.0

x1e+4

Figure 6.2.: 21 state Markov process,
average performance over
100 runs.

probability, unless the system is in state 0 or 20 in which case it always transitions to state 10 in the following step. When the state transitions from 0 to
10 a reward of 1.0 is generated, and for a transition from 20 to 10 a reward of
-1.0 is generated. All other transitions have a reward of 0. We <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> the discount
value γ = 0.9 and then computed the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> discounted value of each state by
running a brute force Monte Carlo simulation.
For our <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> test we ran our algorithm 10 times on the above Markov chain
and computed the root mean squared error in the value estimate across the
states at each <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> step averaged across each run. The optimal value of λ for
our algorithm, which we will call HL(λ), was 1.0. This was to be expected
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that the environment is stationary and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> discounting <span class="t25 hoverable">old</span> experience
is not helpful. Setting this parameter correctly was important, for example if
we reduced the value of λ to 0.98 performance became poor.
For TD(λ) the optimal value of λ was about 0.7. This algorithm was <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span>
less sensitive to the setting of λ. The other important parameter for TD(λ)
was the learning rate α. We tested a variety of values, the effect of which is
illustrated by the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> values α = 0.07 and α = 0.13 on Figure 6.1.
When α was high, TD(λ) learnt more quickly, as we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect, but then
became unstable as the learning rate was too high for fine tuning the value
estimates. With the lower learning rate of 0.07, TD(λ) learnt more slowly, but
eventually achieved a more accurate value estimate before becoming stuck. In
fact rather than just becoming stuck, what we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> is that the error reaches
a minimum at around t = 4,000 and then actually becomes worse for the
remainder of the run. This is a <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> undesirable characteristic of TD(λ)
(see for example Section 6.2 of Sutton and Barto, 1998). With a fixed learning
rate we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that the variability in the error estimate does not improve
towards the <span class="t7 t10 t15 t25 hoverable">end</span> of the run.

116

6.4. A larger Markov process
In comparison, HL(λ) had very fast learning initially, combined with a more
accurate final estimate of the discounted state values, and the mean squared
error in the <span class="t6 hoverable">second</span> half of the experiment was very stable. This is significant
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that TD(λ) required <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> parameters to be tuned for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> performance,
while HL(λ) had just λ which <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> a priori to 1.0.
Figure 6.2 shows the same experiment averaged over 100 runs. This obscures
the better stability of HL(λ), but more clearly illustrates its faster learning and
better convergence.

6.4. A larger Markov process
In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to understand how <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> HL(λ) scales as the number of states increases,
we ran the previous experiment again but with 51 states. As the movement
through the states is almost entirely a random walk with reward on just <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span>
transitions, estimating the value function on this Markov chain is significantly
more difficult than before, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> the total number of states has not
grown all that much. In the <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> experiment the return state was still in
the middle of the chain, i.e. state 25. As most of the state space was a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span>
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> from the rewards, we increased the γ value to 0.99 so that states in the
middle of the chain <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> not have values too close to 0. The <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> discounted
value of each state was again computed by running a brute force Monte Carlo
simulation.
We ran our algorithm 10 times on the above Markov chain and computed
the root mean squared error in the value estimate across the states at each
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> step averaged across each run. The optimal value of λ for HL(λ) was 1.0,
which was to be expected <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that the environment is stationary and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span>
discounting <span class="t25 hoverable">old</span> experience is not helpful.
For TD(λ) we tried various <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> learning rates and values of λ. We
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> find no settings where TD(λ) was competitive with HL(λ). If the learning rate α was <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> too high the system <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> learn as fast as HL(λ) briefly
before becoming stuck. With a lower learning rate the final performance was
improved, <span class="t27 t28 hoverable">however</span> the initial performance was now <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> worse than HL(λ).
The results of these tests appear in Figure 6.3.
Similar tests were performed with larger and smaller Markov chains, and
with <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> values of γ. HL(λ) was consistently superior to TD(λ) across
these tests. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> wonders whether this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be due to the fact that the implicit learning rate that HL(λ) uses is not fixed. To test this we explored the
performance of a number of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> learning rate functions on the 51 state
Markov chain described above. We <span class="t0 t16 t25 t28 hoverable">found</span> that functions of the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> κt always
performed poorly, <span class="t27 t28 hoverable">however</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> performance was possible by setting κ corκ
rectly for functions of the <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> √κt and √
3 . As the results were <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> closer,
t
we averaged over 300 runs. These results appear in Figure 6.4.
With a variable learning rate TD(λ) is performing <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> better, <span class="t27 t28 hoverable">however</span> we
were still unable to find an equation that reduced the learning rate in such

117

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE

0.40

0.40
HL(1.0)
TD(0.9) a = 0.1
TD(0.9) a = 0.2

0.30

0.30

0.25

0.25

0.20

0.20

0.15

0.15

0.10

0.10

0.05
0.0

0.5

1.0
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

1.5

HL(1.0)
TD(0.9) a = 8.0/sqrt(t)
TD(0.9) a = 2.0/cbrt(t)

0.35

RMSE

RMSE

0.35

2.0

x1e+4

Figure 6.3.: 51 state Markov process
averaged over 10 runs.
The parameter a is the
learning rate α.

0.05
0.0

0.5

1.0
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

1.5

2.0

x1e+4

Figure 6.4.: 51 state Markov process
averaged over 300 runs.

a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that TD(λ) <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> outperform HL(λ). This is evidence that HL(λ) is
adapting the learning rate optimally <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> the need for manual equation
tuning.

6.5. Random Markov process
To test on a Markov process with a more complex transition structure, we
created a random 50 state Markov process. We did this by creating a 50 by 50
transition matrix where each element was <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> to 0 with probability 0.9, and a
uniformly random number in the interval [0, 1] otherwise. We then scaled each
row to sum to 1. Then to transition between states we interpret the ith row
as a probability distribution over which state follows state i. To compute the
reward associated with each transition we created a random matrix as above,
but <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> normalising. We <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> γ = 0.9 and then ran a brute force Monte
Carlo simulation to compute the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> discounted value of each state.
The λ parameter for HL(λ) was simply <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> to 1.0 as the environment is
stationary. For TD we experimented with a range of parameter settings and
learning rate decrease functions. We <span class="t0 t16 t25 t28 hoverable">found</span> that a fixed learning rate of α = 0.2,
√
and a decreasing rate of 1.5
performed reasonable well, but <span class="t0 t10 t11 t14 t18 hoverable">never</span> as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as
3
t
HL(λ). The results were generated by averaging over 10 runs, and are shown
in Figure 6.5.
Although the structure of this Markov process is quite <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> to that <span class="t27 hoverable">used</span>
in the previous experiment, the results are again similar: HL(λ) preforms as
<span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> or better than TD(λ) from the beginning to the <span class="t7 t10 t15 t25 hoverable">end</span> of the run. Furthermore, stability in the error towards the <span class="t7 t10 t15 t25 hoverable">end</span> of the run is better with HL(λ)
and no manual learning tuning was required for these performance gains.

118

6.6. Non-stationary Markov process

0.7

0.30
HL(1.0)
TD(0.9) a = 0.2
TD(0.9) a = 1.5/cbrt(t)

0.6

0.20

0.4

RMSE

RMSE

0.5

0.3

0.15
0.10

0.2

0.05

0.1
0.0
0

HL(0.9995)
TD(0.8) a = 0.05
TD(0.9) a = 0.05

0.25

1000

2000

3000

4000

5000

0.00
0.0

0.5

<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

Figure 6.5.: Random 50 state Markov
process. The parameter a
is the learning rate α.

1.0
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

1.5

2.0

x1e+4

Figure 6.6.: 21 state non-stationary
Markov process.

6.6. Non-stationary Markov process
The λ parameter in HL(λ), introduced in Equation (6.4), reduces the importance of <span class="t25 hoverable">old</span> observations when computing the state value estimates. When the
environment is stationary this is not useful and so we can <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> λ = 1.0, <span class="t27 t28 hoverable">however</span>
in a non-stationary environment we need to reduce this value so that the state
values adapt properly to changes in the environment. The more rapidly the
environment is changing, the lower we need to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> λ in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to more rapidly
forget <span class="t25 hoverable">old</span> observations.
To test HL(λ) in such a setting we reverted back to the 21 state Markov
chain from Section 6.3 in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to speed up convergence. We <span class="t27 hoverable">used</span> this Markov
chain for the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> 5,000 <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> steps. At that point, we changed the reward
when transitioning from the last state to the middle state from -1.0 to be 0.5.
At <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> 10,000 we then switched back to the original Markov chain, and so on
alternating between the models of the environment <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span> 5,000 steps. At each
switch, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> changed the target state values that the algorithm was trying
to estimate to match the current configuration of the environment. For this
experiment we <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> γ = 0.9.
As expected, the optimal value of λ for HL(λ) fell from 1 down to about
0.9995. This is about what we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> that each phase is 5,000
steps long. For TD(λ) the optimal value of λ was around 0.8 and the optimum
learning rate was around 0.05. As we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect, for both algorithms when
we pushed λ above its optimal value this caused poor performance in the
periods following each switch in the environment (these bad parameter settings
are not shown in the results). On the other hand, setting λ too low produced
initially fast adaption to each environment switch, but poor performance after
that until the next environment change. To get accurate statistics we averaged
over 200 runs. The results of these tests appear in Figure 6.6.

119

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE
Algorithm 2 HLS(λ)
Initialise Q(s, a) = 0, N (s, a) = 1 and E(s, a) = 0 for all s, a
Initialise s and a
repeat
<span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Take</span> action a, observed r, s′
Choose a′ by using ǫ-greedy selection on Q(s′ , ·)
∆ ← r + γQ(s′ , a′ ) − Q(s, a)
E(s, a) ← E(s, a) + 1
N (s, a) ← N (s, a) + 1
for all s, a do
N (s′ ,a′ )
1
β((s, a), (s′ , a′ )) ← N (s′ ,a′ )−γE(s
′ ,a′ )
N (s,a)
<span class="t7 t10 t15 t25 hoverable">end</span> for
for all s, a do

Q(s, a) ← Q(s, a) + β (s, a), (s′ , a′ ) E(s, a)∆
E(s, a) ← γλE(s, a)
N (s, a) ← λN (s, a)
<span class="t7 t10 t15 t25 hoverable">end</span> for
s ← s ′ ; a ← a′
until <span class="t7 t10 t15 t25 hoverable">end</span> of run
For some reason HL(0.9995) learns faster than TD(0.8) in the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> half
of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle, but only equally fast at the start of each following cycle.
Furthermore, its performance in the <span class="t6 hoverable">second</span> half of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> cycle is poor. We
are not sure why this is happening. We <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> improve the initial speed at
which HL(λ) learnt in the last <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> cycles by reducing λ, <span class="t27 t28 hoverable">however</span> that comes
at a performance cost in terms of the lowest mean squared error attained at
the <span class="t7 t10 t15 t25 hoverable">end</span> of each cycle. In any case, in this non-stationary situation HL(λ) again
performed <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> in general.

6.7. Windy Gridworld
Reinforcement learning algorithms such as Watkins’ version of Q(λ) (Watkins,
1989) and Sarsa(λ) (Rummery and Niranjan, 1994; Rummery, 1995) are based
on temporal difference updates. This suggests that <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> reinforcement learning
algorithms based on HL(λ) should be possible.
For our <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> experiment we <span class="t4 hoverable">took</span> the standard Sarsa(λ) algorithm and modified it in the obvious <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to use an HL temporal difference update. In the
presentation of this algorithm we have changed notation slightly to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span>
more consistent with that typical in reinforcement learning. Specifically, we
have dropped the t super script as this is implicit in the algorithm specification, and have defined Q(s, a) := V(s,a) , E(s, a) := E(s,a) and N (s, a) := N(s,a) .
Our <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> reinforcement learning algorithm, which we call HLS(λ) is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> in
Algorithm 2. Essentially the only changes to the standard Sarsa(λ) algorithm

120

6.7. Windy Gridworld

Figure 6.7.: [Windy Gridworld] S marks the start state and G the goal state,
at which the agent jumps back to S with a reward of 1. Small
arrows indicate an upward wind of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> row per <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> step. The
large arrows indicate a wind of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> rows per <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> step.

have been to add code to compute the visit counter N (s, a), add a loop to
compute the β values, and replace α with β in the temporal difference update.
To test HLS(λ) against standard Sarsa(λ) we <span class="t27 hoverable">used</span> the Windy Gridworld
environment described on page 146 of (Sutton and Barto, 1998). This <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> is
a grid of 7 by 10 squares that the agent can move through by going either up,
down, left or right. If the agent attempts to move off the grid it simply stays
where it is. The agent starts in the 4th row of the 1st column and receives
a reward of 1 when it finds its <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to the 4th row of the 8th column. To
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> more difficult, there is a “wind” blowing the agent up 1 row in
columns 4, 5, 6, and 9, and a strong wind of 2 rows in columns 7 and 8. This
is illustrated in Figure 6.7. Unlike in the original version, we have <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> up this
problem to be a continuing discounted task with an automatic transition from
the goal state back to the start state. This is because we have not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> derived
an episodic version of our learning rule.
We <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> γ = 0.99 and in each run computed the empirical future discounted
reward at each point in time. As this value oscillated we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> ran a moving
average through these values with a window length of 50. Each run lasted for
50,000 <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> steps as this allowed us to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> at what level each learning algorithm
topped out. These results appear on the left of Figure 6.8 and were averaged
over 500 runs to get accurate statistics.
Despite putting considerable effort into tuning the parameters of Sarsa(λ),
we were unable to achieve a final future discounted reward above 5.0. The

121

6

6

5

5

Future Discounted Reward

Future Discounted Reward

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE

4
3
2
1

4
3
2
1

HLS(0.995) e = 0.003
Sarsa(0.5) a = 0.4 e = 0.005
0
0

10000

20000
30000
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

40000

50000

HLQ(0.99) e = 0.01
Q(0.75) a = 0.99 e = 0.01
0
0

10000

20000
30000
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Time</span>

40000

50000

Figure 6.8.: Windy Gridworld performance tests. The left graph shows
HLS(λ) vs. Sarsa(λ), while the <span class="t27 hoverable">right</span> graph shows HLQ(λ) vs.
Q(λ). In both tests the algorithm based on HL learning performed
best. e represents the exploration parameter ǫ, and a represents
the learning rate α. For both tests the performance was averaged
over 500 runs.

settings shown on the graph represent the best final value we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> achieve.
In comparison HLS(λ) easily beat this result at the <span class="t7 t10 t15 t25 hoverable">end</span> of the run, while being
slightly slower than Sarsa(λ) at the start. By setting λ = 0.99 we were able
to achieve the same performance as Sarsa(λ) at the start of the run, <span class="t27 t28 hoverable">however</span>
the performance at the <span class="t7 t10 t15 t25 hoverable">end</span> of the run was then only slightly better than
Sarsa(λ). This combination of superior performance and fewer parameters
to tune suggest that the benefits of HL(λ) carry over into the reinforcement
learning setting. In terms of computational cost, HL(λ) was about 1.7 times
slower than Sarsa(λ) per <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> step due to the cost of computing the β values.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> popular reinforcement learning algorithm is Watkins’ Q(λ). Similar
to Sarsa(λ) above, we simply inserted the HL(λ) temporal difference update
into the usual Q(λ) algorithm in the obvious way. We call this <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> algorithm
HLQ(λ) and it is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> in Algorithm 3. The test environment was exactly the
same as we <span class="t27 hoverable">used</span> with Sarsa(λ) above.
The results this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> were more competitive and appear on the <span class="t27 hoverable">right</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span>
side of Figure 6.8. Nevertheless, despite spending a considerable amount of
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> fine tuning the parameters of Q(λ), we were unable to beat HLQ(λ). As
the performance advantage was relatively modest, the main benefit of HLQ(λ)
was that it achieved this level of performance <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> having to manually tune
a learning rate.

122

6.8. Conclusion
Algorithm 3 HLQ(λ)
Initialise Q(s, a) = 0, N (s, a) = 1 and E(s, a) = 0 for all s, a
Initialise s and a
repeat
<span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Take</span> action a, observed r, s′
Choose a′ by using ǫ-greedy selection on Q(s′ , ·)
a∗ ← arg maxb Q(s′ , b)
∆ ← r + γQ(s′ , a∗ ) − Q(s, a)
E(s, a) ← E(s, a) + 1
N (s, a) ← N (s, a) + 1
for all s, a do
N (s′ ,a′ )
1
β((s, a), (s′ , a′ )) ← N (s′ ,a′ )−γE(s
′ ,a′ )
N (s,a)
<span class="t7 t10 t15 t25 hoverable">end</span> for
for all s, a do

Q(s, a) ← Q(s, a) + β (s, a), (s′ , a′ ) E(s, a)∆
N (s, a) ← λN (s, a)
if a′ = a∗ then
E(s, a) ← γλE(s, a)
else
E(s, a) ← 0
<span class="t7 t10 t15 t25 hoverable">end</span> if
<span class="t7 t10 t15 t25 hoverable">end</span> for
s ← s ′ ; a ← a′
until <span class="t7 t10 t15 t25 hoverable">end</span> of run

6.8. Conclusion
We have derived a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> equation for setting the learning rate in temporal difference learning with eligibility traces. The equation replaces the free learning
rate parameter α, which is normally experimentally tuned by hand. In <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">every</span>
setting tested, be it stationary Markov chains, non-stationary Markov chains
or reinforcement learning, our <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> method produced superior results.
To further our theoretical understanding, the next step <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to try
to prove that the method converges to correct estimates. This can be done
for TD(λ) under <span class="t3 hoverable">certain</span> assumptions on how the learning rate decreases over
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> (Dayan, 1992; Peng, 1993). Hopefully, something similar can be proven
for our <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> method. In terms of experimental results, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be interesting
to try <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> types of reinforcement learning problems and to more clearly
identify where the ability to <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> the learning rate differently for <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> state
transition pairs helps performance.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> extensions to the algorithm should <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be possible. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be
to generalise the learning rule to episodic tasks, <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to merge
our update rule with Peng’s version of Q(λ) (Peng and Williams, 1996), as we

123

TD UPDATING <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">WITHOUT</span> A LEARNING RATE
have done with Sarsa(λ) and Watkins’ version of Q(λ). Finally, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be
useful to extend the algorithm to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with function approximation methods
so that it <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> deal with larger state spaces.

124

7. Discussion
The title of this thesis is deliberately provocative. It asks the reader to consider
not just intelligent machines, but the possibility of machines that are super
intelligent. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> find this idea difficult to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> seriously. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Among</span> researchers
the topic is almost taboo: it belongs in science fiction. The most intelligent
computer in the world, they assure the public, is perhaps as smart as an ant,
and that’s on a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> day. <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">True</span> machine intelligence, if it is <span class="t29 hoverable">ever</span> developed,
lies in the distant future.
This was not always the case. In the 1960’s pioneering artificial <span class="t20 hoverable">intelligence</span>
researchers had initial successes in a range of areas. Emboldened, they predicted that more powerful systems were not <span class="t6 t9 t18 t26 hoverable">far</span> off, and that truly intelligent
machines <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> follow. As the researcher Herbert Simon wrote in 1965, “machines will be capable, <span class="t5 t7 hoverable">within</span> twenty years, of doing any <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">man</span> can
do.” (quoted in Crevier, 1993) The field was alight with ambition and, not
surprisingly, attracted plenty of attention.
Over the decade that followed a <span class="t14 hoverable">series</span> of high profile failures <span class="t2 t4 t5 t22 t23 hoverable">brought</span> this
dream crashing back to earth. Although progress was being made, it was
<span class="t6 t9 t18 t26 hoverable">far</span> slower than <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> had expected. Naturally the public, and more importantly the funding agencies, wanted to <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> where the intelligent machines
were. The cuts that ensued marked the beginning of the so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> ‘AI winter’, although usage of this label varies considerably (Crevier, 1993; Russell
and Norvig, 1995). During the early 80’s there was a brief reprieve as expert
systems became popular, <span class="t27 t28 hoverable">however</span> by the late 80’s these too had failed to live
up to expectations. Over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> some areas distanced themselves from the label
‘artificial intelligence’, emphasising that their <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> was more practical and
limited in scope. Any talk of building machines with <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> level <span class="t20 hoverable">intelligence</span>
was frowned upon.
Since the early 90’s steady progress has slowly begun to reinvigorate the
field, particularly since the late 90’s. More powerful algorithms coupled with
dramatically improved hardware has produced countless advances in robotics,
speech recognition, natural language processing, image processing, clustering,
classification, prediction, various types of optimisation, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> other areas.
As a result the reputation of artificial <span class="t20 hoverable">intelligence</span> has started to recover and
funding has improved, leading some to believe that the ‘AI spring’ <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> have
finally arrived (Havenstein, 2005).
With the mood becoming more positive, the grand dream of artificial <span class="t20 hoverable">intelligence</span> is starting to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> back. A number of books predicting
the arrival of advanced artificial <span class="t20 hoverable">intelligence</span> have been published and major
conferences have held workshops on ‘Human level AI’. Small conferences on

125

7. Discussion
‘Artificial General Intelligence’ and on the safety issues surrounding powerful
machine <span class="t20 hoverable">intelligence</span> have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> appeared. Perhaps over the next few <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> these
ideas will <span class="t8 t13 hoverable">become</span> more mainstream, <span class="t27 t28 hoverable">however</span> for now they are at the fringe.
Most researchers remain very sceptical about the idea of truly intelligent machines <span class="t5 t7 hoverable">within</span> their lifetime.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> goal of this thesis is to promote the idea that intelligent machines, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span>
super intelligent machines, is a topic that is both important and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> that can
be scientifically studied, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if just theoretically for now. In Chapters 2 and 3
we described Hutter’s model of an intelligent machine and examined some of
its remarkable properties. In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 4 we saw how this model can be <span class="t27 hoverable">used</span> to
construct a general measure of machine <span class="t20 hoverable">intelligence</span> that formalises <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of
the standard perspectives on the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of <span class="t20 hoverable">intelligence</span> outlined in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 1.
In <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 5 we then examined some of the theoretical limitations faced by
powerful artificial <span class="t20 hoverable">intelligence</span> algorithms. Thus, although highly intelligent
machines do not exist yet, theoretical tools are starting to emerge to allow us
to study their properties. While this thesis makes some contributions to this
effort, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> fundamental questions remain open (see for example the open
problems listed in Hutter, 2005).
None of this theoretical <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be of <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> importance if intelligent
machines were impossible in practice, or exceedingly unlikely in any reasonable
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> frame. In this last <span class="t12 t19 t25 t26 t28 hoverable">chapter</span> we will argue that this is not the case. Our
goal is not to conclusively argue that this will happen, or exactly when it
will happen, but simply to argue that the possibility <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be completely
discounted. This is important because if a super intelligent machine <span class="t29 hoverable">ever</span>
did exist the implications for humanity <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be immense. Thus, if there is
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> a small probability that intelligent machines <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be developed in the
foreseeable future, it is important that we start to think seriously about the
<span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of these machines and what the implications <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be.

7.1. Are super intelligent machines possible?
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> outside of the field are deeply sceptical about the idea that machines, mere physical objects of our construction, <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t29 hoverable">ever</span> have anything
resembling real intelligence: machines can only <span class="t29 hoverable">ever</span> be strictly logical; they
<span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> do anything they were not programed to do; and they certainly <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span>
not be superior to their own creator — that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be a paradox! However,
as anybody working in the field knows, these common beliefs are baseless
myths. Artificial <span class="t20 hoverable">intelligence</span> algorithms regularly find solutions to problems
using heuristics and forms of reasoning that are not strictly logical. They discover powerful <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> designs for problems that the system’s programmers had
<span class="t0 t10 t11 t14 t18 hoverable">never</span> <span class="t0 t5 t6 t14 t18 hoverable">thought</span> of (Koza et al., 2003). They <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> learn to play games such as
chess (Hsu et al., 1995) and backgammon (Tesauro, 1995) at levels superior
to that of any human, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> alone the researchers who designed and created the

126

7.1. Are super intelligent machines possible?
system. Indeed, in the case of checkers, computers are now literally unbeatable
as they can play a provably perfect game (Schaeffer et al., 2007).
The persistence of these beliefs seems to be due to a number of things. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span>
is that algorithms from artificial <span class="t20 hoverable">intelligence</span> are not consumer products: they
are hidden in the magic of sophisticated technology. For example, when <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t29 hoverable">hand</span>
writing the address on a card most <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> do not <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that it will likely be
read by a computer rather than a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> at the sorting office. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">People</span> do
not think about the learning algorithms that are monitoring their credit card
transactions looking for fraud, filtering spam to their email address, automatically trading their retirement savings on international markets, monitoring
their behaviour on the internet in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to decide which ads should appear on
web pages they view, or <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> just the vision processing algorithms that graded
the apples at the supermarket. The steady progress that artificial <span class="t20 hoverable">intelligence</span>
algorithms are making is out of sight, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> generally out of mind.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> common objection is that we humans have something mysterious
and special that makes us tick, something that machines, by definition, do not
have. Perhaps some type of non-physical consciousness or feelings, qualia, or
‘quantum field’ etc. Of course it is impossible to rule out mysterious possibilities until an intelligent machine has been constructed <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> needing anything
particularly mysterious. Nonetheless, we should view such objections for what
they are: a <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> of vitalism. Throughout history, whenever science <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> not
explain some unusual phenomenon, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> readily assumed that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">God</span> or
magic was at work. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> distinguished scientists have fallen into this, only
to be embarrassed once more sceptical and curious scientists worked out what
was actually going on. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Things</span> ranging from the motion of <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span> galaxies to the
behaviour of sub-atomic particles are now <span class="t6 t10 t15 hoverable">known</span> to follow extremely precise
physical laws. To conjecture that our brains are somehow special and <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
in some strange <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> is to speculate based on <span class="t15 t25 t26 t29 hoverable">nothing</span> but our own feelings of
specialness.
If the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> brain is merely a ‘meat machine’, as some have <span class="t0 t2 t16 t18 t21 t22 t23 hoverable">put</span> it, it is
certainly not the most powerful <span class="t20 hoverable">intelligence</span> possible. To start with, there
is the issue of scale: a typical adult <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> brain weights about 1.4 kg and
consumes just 25 watts of <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> (Kandel et al., 2000). This is ideal for a mobile
intelligence, <span class="t27 t28 hoverable">however</span> an artificial <span class="t20 hoverable">intelligence</span> need not be mobile and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span>
be orders of magnitude larger and more energy intensive. At present a large
supercomputer can fill a room twice the size of a basketball court and consume
10 megawatts of power. With a few billion dollars <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> larger machines <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span>
be built. Google, for example, is currently constructing a data centre next to
a <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> station in Oregon that will cover <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> football fields and have cooling
towers four stories high (Markoff and Hansell, 2005). Biology <span class="t0 t10 t11 t14 t18 hoverable">never</span> had the
option of building brains on such an enormous scale.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> point is that brains use fairly large and slow components. Consider
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the simpler of these, axons: essentially the wiring of the nervous system.
These are typically around 1 micrometre wide, carry spike signals at up to 75
metres per <span class="t6 hoverable">second</span> at a frequency of at most a few hundred hertz (Kandel et al.,

127

7. Discussion
2000). Compare these characteristics with those of a wire that carries signals
on a microchip. Currently these are 45 nanometres wide, propagate signals at
300 million metres per <span class="t6 hoverable">second</span> and can easily operate at 4 billion hertz. Some
<span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> debate whether an electrochemical spike travelling down an axon is so
directly comparable to an electrical pulse travelling down a wire, <span class="t27 t28 hoverable">however</span> it
is <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> established that at least the primary role of an axon is simply to carry
this information. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that present <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">day</span> technology produces wires which are
20 times thinner, propagate signals 4 million times faster and operate at 20
million times the frequency, it is hard to believe that the performance of axons
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> not be improved by at least a few orders of magnitude.
Of course, the above assumes that the brain’s design is what we should
replicate. Perhaps the brain’s algorithm is close to optimal for some things,
but it certainly is not optimal for all problems. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> the most outstanding savants <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> store information anywhere near as quickly, accurately and in the
quantities that are possible for a computer. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Also</span> savants’ impressive ability to
perform fast mental calculations is insignificant next to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> a basic calculator.
Brains are poorly designed for such feats. A machine, however, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> have no
such limitations: it <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> employ a range of specialised algorithms for <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
types of problems. Concepts <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> education <span class="t8 t13 hoverable">become</span> obsolete when <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
and understanding can simply be copied from <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> intelligent machine to another. It is easy to think up <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> more advantages.
Most likely improvements over brains are possible in algorithms, hardware
and scale. This is not to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> <span class="t0 t1 t2 t3 t4 t6 t8 t10 t11 t12 t13 t14 t15 t16 t17 t20 t21 t22 t24 t25 t26 t27 t28 hoverable">away</span> from the amazing system that the brain
is, something that we are still unable to match in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> ways. All we wish to
point out is that if the brain is essentially just a machine, which appears to be
the case, then it certainly is not the most intelligent machine that <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> exist.
This idea is reasonable once you think about it: machines can easily carry
more, fly higher, move faster and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> further than <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> the most able animals
in each of these categories. Why <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span> be any different? Of
course, just because systems with greater than <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> <span class="t20 hoverable">intelligence</span> are possible
in principle, this does not mean that we will be able to build one. Designing
and constructing such an advanced machine <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be beyond our capabilities.

7.2. How <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> intelligent machines be developed?
There are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> ways in which machine <span class="t20 hoverable">intelligence</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be developed. Unfortunately, it is difficult to estimate how likely any of these approaches are to
succeed. In this section we speculate on a few of them.

Theoretical approaches
The approach most closely related to this thesis <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the AIXI
model and find a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> to usefully scale it down. A number of attempts to
do this have been made: the HL(λ) algorithm presented in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 6, the

128

7.2. How <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> intelligent machines be developed?
AIXItl algorithm (Chapter 7 of Hutter, 2005), the AIXI based algorithm for

repeated matrix games (Poland and Hutter, 2006) and Fitness Uniform Optimisation (Hutter and Legg, 2006) all originate in efforts to scale down AIXI.
Although not deriving from AIXI, the Shortest and Fastest algorithm in (Hutter, 2002a), the Speed prior (Schmidhuber, 2002), the Optimal Ordered Problem Solver (Schmidhuber, 2004), and the Gödel Machine (Schmidhuber, 2005)
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> from a related background in algorithmic probability and Kolmogorov
complexity theory. While each of these has their strengths, as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> none <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span>
close to bringing the <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> of theoretical models such as AIXI into reality.

The key question is how to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> a general and powerful artificial <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> AIXI <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> efficiently. The results of <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 5 offer some hints
on the direction that such a project <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> take. For certain, the prediction
of general computable sequences is out of the question (Lemma 5.2.4), as is
the prediction of all computable sequences <span class="t3 t4 t5 t6 t8 t10 t11 t12 t13 t14 t18 t19 t21 t23 t24 t25 t26 t27 hoverable">whose</span> Kolmogorov complexity is
below some moderate bound. Otherwise serious problems arise with the necessary complexity of the prediction algorithms (Theorem 5.3.3), and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> worse
Gödel incompleteness (Theorem 5.6.1). Nevertheless, we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that <span class="t3 hoverable">certain</span>
types of complex sequences can be predicted by relatively simple algorithms
(Lemma 5.2.3), and that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> theoretical problems go <span class="t0 t1 t2 t3 t4 t6 t8 t10 t11 t12 t13 t14 t15 t16 t17 t20 t21 t22 t24 t25 t26 t27 t28 hoverable">away</span> when <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> weak
bounds are placed on the computation <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> of the sequences to be predicted
(Lemma 5.5.2). Thus, what we need to aim for is the ability to efficiently predict computable sequences that have <span class="t3 hoverable">certain</span> computational resource bounds.
How best to characterise such sequences is an open problem, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> alone how to
efficiently learn to predict them. Perhaps any breakthrough is more likely to
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> from the opposite direction: somebody discovers a theoretically elegant
and very powerful algorithm that is able to efficiently predict <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> kinds of sequences. The structure of this algorithm and how easily it can model <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
sequences will then implicitly define a natural measure of resource bounded
sequence complexity.

When a breakthrough in this area <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> occur is impossible to predict,
and the same is <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> of other theoretical approaches. Perhaps with the <span class="t27 hoverable">right</span>
theoretical insight Bayesian networks, prediction with expert advice, artificial
neural networks, reasoning engines, or any <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of a dozen other techniques
will suddenly advance in a dramatic way. Although some progress in all of
these areas is a near certainty, <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> breakthroughs by their very <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> are
rare and highly unpredictable. Furthermore, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if a huge breakthrough did
occur, whether existing computer hardware <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be sufficient to support the
<span class="t14 hoverable">creation</span> of highly intelligent machines <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> depend on the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of this
<span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> algorithm. In summary, although we <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> rule out the possibility of a
large breakthrough leading to intelligent machines, there is little we can do to
estimate how likely this is.

129

7. Discussion

Brain simulation
Rather than taking abstract theories <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> AIXI and trying to scale them down to
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> them practical, <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> approach is to proceed in the opposite direction:
start by studying the details of how real biological brains <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> and then try to
abstract from this a design for an artificial intelligence. Although this idea is
simple in principle, studying and understanding the brain is very challenging.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> of the most basic problems is that it is currently impossible to observe a
brain in action with sufficient spatial and temporal resolution to really <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> how
it works. Methods to study the brain include: PET scans, fMRI scans, arrays
of probes, marking neurons with chemicals which <span class="t1 t3 t9 t15 t20 t21 t26 t29 hoverable">cause</span> them to fluoresce when
they fire, placing extracted neural tissue on microchips that can sense when
neurons fire, and the use of staining and microscopy to study the anatomical
structure of the brain. The problem is that none of these methods allows a
researcher to <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> a sizable area of a brain and simultaneously observe exactly
which neurons are firing, precisely when they fire, what type of neurons they
are, which other neurons they are connected to, the types of these connections,
how these connections change over short intervals of time, and so on. Instead,
researchers have at their disposal a range of methods, each of which provides
only a limited view of what is going on. Indeed, what is <span class="t6 t10 t15 hoverable">known</span> about the brain
is largely dictated by the strengths and weaknesses of the <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> methods of
study.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> major problem is the sheer complexity of the system. A <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span>
brain consists of hundreds of billions of neurons, and hundreds of trillions
of synapses. There are over a hundred <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> types of neurons, <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
synapses employ <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> combinations of neurotransmitters, connection patterns vary from <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the brain to another, and so on (see any standard
<span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">text</span> book, for example Kandel et al., 2000). When looking at slices of brain
tissue where a small percentage of neurons and their dendritic trees have been
stained, the brain’s wiring starts to look about as comprehensible as an ocean
of tangled spaghetti. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> worse, all these elements interact with each other
in highly dynamic ways. Via some kind of a miracle, from this monstrous
cacophony emerges the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> mind. With so <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> complexity, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if the
perfect brain scanning technology existed it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> still be very difficult to
understand how the brain actually works.
<span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> the scale of these difficulties, is building a brain simulation feasible?
Perhaps the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> point to <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">note</span> is that building a working simulation of something does not require understanding everything about how the system works.
What is required is that the basic units which comprise the system can be
faithfully reproduced and connected together. If this is done properly, the resulting dynamics will be the same as the dynamics in the real system — <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span>
if we do not fully understand what these higher level dynamics are, or why
they are important to the system’s overall functioning. Indeed, simulations
are often constructed for the very purpose of better understanding a system’s
emergent dynamics once its low level dynamics are understood. Thus, rather

130

7.2. How <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> intelligent machines be developed?
than needing to understand all the mysteries of how the brain works, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span>
to build a functional simulation it is sufficient to understand the <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> and organisation of the brain’s elementary units: neurons, dendrites, axons, synapses
etc. There is already a large <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">body</span> of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> about how these basic units
<span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> and how they are wired <span class="t16 t21 hoverable">together</span> in various parts of the brain. Literally
thousands of researchers around the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> are refining and developing this
knowledge.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> important point, at least for <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> interested in developing artificial intelligence, is that <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> of the brain’s complexity is not relevant. A
significant <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> brain is a jumble of <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> subsystems that
<span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> care of basic instinctive <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> breathing, <span class="t0 t1 t4 t8 t9 t10 t11 t12 t16 t19 t22 t23 t25 t26 hoverable">heart</span> beat, blood pressure,
reproduction, hunger, thirst, rhythms such as sleeping and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">body</span> temperature,
fight or flight response and so on. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the largest parts of the brain, the
cerebellum which is involved in movement and precision timing, is not needed
for an artificial intelligence: individuals <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> are still intellectually
and emotionally able. The key, it seems, lies in understanding the neocortex,
and its interaction with <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> smaller structures, namely, the thalamus and the
hippocampus. It is <span class="t6 t10 t15 hoverable">known</span> that the neocortex is the <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the brain that
is primarily responsible for processing vision, sound, touch, proprioception,
understanding and generating language, planning, spatial reasoning, coordinating and executing movement, and logical <span class="t0 t5 t6 t14 t18 hoverable">thought</span> (Fuster, 2003). Clearly
then, the key to artificial <span class="t20 hoverable">intelligence</span> via brain simulation lies in understanding
the neocortex and related structures.
As <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> regions of the neocortex perform <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> functions, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span>
expect that they <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> have significantly <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> anatomical structures.
Amazingly, this is not the case. Essentially the <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span> neocortex has the same
six layer structure, or up to 12 layers depending on how you count them. Each
layer is characterised by the types of neurons present, where axons from neurons in the layer project to, and where axons <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> from that <span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> synapses
on the dendritic trees of these neurons. Besides some thickening and thinning
of the layers in <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> regions, and the fact that primary visual cortex actually has an extra layer, this six layer structure is consistent across the <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span>
neocortex (Fuster, 2003; Abeles, 1991). What this suggests is that the same
information processing mechanism is being applied across the neocortex, and
that the variations in function across <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> regions are actually adaptations
to the information passing through each region (Creutzfeldt, 1977; Mountcastle, 1978).
A number of results back up this hypothesis. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> is that with increased use
the region of cortex responsible for performing some action tends to expand,
in the sense that neighbouring cortex is recruited. In extreme cases, such as
the congenitally blind, the unused areas of visual cortex start to perform other
roles, such as helping touch processing for reading braille. In a more dramatic
example, the brain of a ferret was physically altered at birth so that visual
neocortex received auditory input and vice versa. Each region of neocortex

131

7. Discussion
then learnt to process the input it was receiving (Melchner et al., 2000). Similar
experiments with rats have produced consistent results.
If the hypothesis of a single underlying learning and adaptation dynamic
across the neocortex is correct, then the key to understanding <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> of the intellectual capacity of the brain lies in understanding how these six layers work,
and the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> in which they interact with the thalamus and hippocampus. In
recent <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> this approach to artificial <span class="t20 hoverable">intelligence</span> has been popularised by Jeff
Hawkins (2004), <span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> most of these ideas have been <span class="t6 t10 t15 hoverable">known</span> in neuroscience
for some time. The main vehicle for Hawkins’ artificial <span class="t20 hoverable">intelligence</span> <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> is
his company Numenta, with related neuroscience research being carried out at
the Redwood Center for Theoretical Neuroscience, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> founded by Hawkins.
They are certainly not alone in trying to understand the cortex, indeed it is
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the largest areas of neuroscience research with <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span> journals dedicated
to the topic.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> group of researchers <span class="t3 t4 t5 t6 t8 t10 t11 t12 t13 t14 t18 t19 t21 t23 t24 t25 t26 t27 hoverable">whose</span> <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be useful to brain modelling is
currently cutting a cortical column into 30 nanometre slices and then scanning these using both electron and <span class="t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t26 t27 t28 t29 hoverable">light</span> based microscopes. As this produces
enormous quantities of data, machine learning algorithms are being developed
to automatically identify the structures in these images. Over the next few
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> this process should produce an extremely detailed <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> dimensional
anatomical model of the cortex (Singer, 2007).
A group with more of a simulation emphasis is the BlueBrain project based
at EPFL. Using information on the structure and behaviour of cortical columns
collected from a wide range of sources, they have built a computer model of
a column that they run on an IBM BlueGene supercomputer. To calibrate
their model they use segments of rat cortex which they stimulate in <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span>
ways and then compare the resulting dynamics with what their model predicts.
They now claim to have succeeded in accurately modelling the dynamics of
a cortical column, and are working on ways to expand this model to be able
to simulate groups of columns working <span class="t16 t21 hoverable">together</span> (Graham-Rowe, 2007). Their
goal is to eventually be able to simulate an entire <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> neocortex.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> group working at the IBM Almaden Research Lab recently announced that they had simulated a mouse scale neocortex, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> on an IBM
BlueGene supercomputer. Their model consisted of 8 million neurons and
50 billion synapses and ran at <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> seventh real <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> speed. They claim that
this simulation produced dynamical properties consistent with what is observed in a real mouse brain, including EEG <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> waves (Ananthanarayanan
and Modha, 2007). Their aim is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> to scale up to a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> sized neocortex as
more powerful supercomputers <span class="t8 t13 hoverable">become</span> available in the coming years. Unlike
the BlueBrain project, their core goals are less neuroscience orientated: as
their model starts to do more interesting things, their aim is to extract from
this useful algorithms for artificial intelligence.
Obviously these simulations are pushing the limits of what is <span class="t6 t10 t15 hoverable">known</span> about
the cortex, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> what is possible with current computer technology. Nevertheless, the fact that these simulations are being attempted at all illustrates

132

7.2. How <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> intelligent machines be developed?
how the gap between the <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> of supercomputers and brains is perhaps not
as large as some <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> think. At present the world’s fastest machine is the
IBM Roadrunner supercomputer at the US department of defence which has
an official LINPACK benchmark performance of 1015 floating point operations
per <span class="t6 hoverable">second</span> (FLOPS). Machines capable of 1016 FLOPS are being designed and
should appear in a few years. No doubt these will be superseded by <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> more
powerful machines.
To <span class="t0 t2 t16 t18 t21 t22 t23 hoverable">put</span> these numbers in perspective, a <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> cortex has on the <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> of
1010 neurons and 1014 synapses (Koch, 1999). <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that neurons can <span class="t1 t2 t3 t5 t6 t7 t10 t11 t13 t15 t16 t17 t18 t19 t20 t21 t22 t24 t28 t29 hoverable">fire</span> on
the <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> of 100 Hz, this gives a crude estimate of the computational capacity
of the brain: 1016 operations per <span class="t6 hoverable">second</span> (Moravec, 1998; Kurzweil, 2000). Of
course, until a simulation succeeds in producing intelligence, nobody knows
for sure how <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> computer <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> will be needed. Researchers working in
molecular neuroscience tend to think it is <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> more, while some working in
theoretical neuroscience think it <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be less. If the estimate of 1016 FLOPS is
in fact 100 times too low, then we will have to wait 10 <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> before a sufficiently
powerful computer exists.

Evolution
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> approach that is becoming more attractive with increasing computer
<span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> is artificial evolution. After all, natural evolution produced the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span>
brain so we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that the approach does <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> — at least as a planet wide
phenomenon over billions of years! No computer in the foreseeable future
<span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> hope to simulate evolution on such a scale. Fortunately, evolving an
artificial <span class="t20 hoverable">intelligence</span> via an evolutionary algorithm is a <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> smaller problem.
To begin with, it is not necessary to start from scratch the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> did.
Of the approximately 4 billion <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> since simple cellular <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">came</span> into
existence, about 3 billion <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> of this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> was required just to get to the level
of multicellular life. Only then <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> more complex organisms start to evolve.
We can short circuit this by building a virtual <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">body</span> for the agent. Of course,
evolving an <span class="t20 hoverable">intelligence</span> for a complex <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">body</span> <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be too difficult, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span>
want to begin with trying to evolve a simple <span class="t20 hoverable">intelligence</span> for a simple <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">body</span> in
a simple environment, and then scale up. In any case, all the evolutionary
algorithm has to do is to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> on the design of the agent’s intelligence: <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span>
of the remaining complexity can be <span class="t19 hoverable">taken</span> care of by us.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> important point is that natural evolution does not seek to maximise intelligence: its effect is to optimise agents’ ability to spread their genes.
<span class="t20 hoverable">Intelligence</span> is a secondary feature that is more useful in some ecological niches
than others. In artificial evolution this does not need to be the case. So <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span>
as we can define and measure <span class="t20 hoverable">intelligence</span> in a sufficiently general way, we
can use this to evaluate the fitness of individuals. With evolution explicitly
directed towards maximising intelligence, progress towards more intelligent
agents should be <span class="t6 t9 t18 t26 hoverable">far</span> more rapid.

133

7. Discussion
The universal <span class="t20 hoverable">intelligence</span> measure described in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 4 lays the foundation for such a measure. Essentially, what the universal <span class="t20 hoverable">intelligence</span> measure
<span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> is that we should test agents initially over extremely trivial pattern recognition and interaction problems as these are the most important. As agents
learn to solve these, we should slowly include more complex problems, always
ensuring that agents are still able to solve the simple problems that <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">came</span>
before. This ensures that the agents’ abilities remain very general as they
develop. As the universal <span class="t20 hoverable">intelligence</span> measure is still a theoretical definition,
some further <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be needed to figure out how best to convert it into a
practical test for evolving agents.
The importance of having a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> fitness function <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be stressed enough:
with a fitness function that is reliable, smooth and has a gradient that generally
points in the <span class="t27 hoverable">right</span> direction, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> high dimensional optimisation problems
can <span class="t8 t13 hoverable">become</span> relatively easy. With an unreliable or deceptive fitness function
seemingly simple problems can <span class="t8 t13 hoverable">become</span> practically unsolvable. It is currently
unknown how <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> universal <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> as a fitness function for
evolving agents with general intelligence.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> major issue concerns the model that we use for the agents’ intelligence: should the agents be neural networks, programs in some language,
or some other kind of object? In theory any Turing equivalent representation
should do, <span class="t27 t28 hoverable">however</span> in practice the choice of representation is important as
it has the effect of biasing the space of possibilities towards <span class="t3 hoverable">certain</span> types of
agents. Often choosing the <span class="t27 hoverable">right</span> representation is critical to making artificial
evolution work. For an intelligent agent there are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> possible systems of
representation, each with their respective proponents. Unfortunately, nobody
really knows which of these is the best. About the only thing we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> for sure
is that <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> was able to evolve <span class="t20 hoverable">intelligence</span> working with networks of spiking neurons. On the other hand, with a computer system perhaps something
closer to a traditional programing language <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be more efficient.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> important problem faced in large scale artificial evolution is diversity
control. Essentially, if you apply too <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> selection pressure the population
tends to collapse around a small group of individuals that are all related to
the fittest individual. At this point evolution becomes stuck due to a lack
of genetic diversity. If you reduce the selection pressure this helps, <span class="t27 t28 hoverable">however</span>
now the speed at which the population fitness rises is reduced. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> with no
selection pressure at all population diversity tends to collapse over <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> due to
the phenomenon of genetic drift. For problems where the evolving individuals
are fairly simple this can easily be dealt with by creating a distance metric
to evaluate how similar individuals are to each other. This can then be <span class="t27 hoverable">used</span>
to ensure that similar individuals tend not to mate with each other, or have
reduced fitness (see for example Goldberg and Richardson, 1987; Jong, 1975).
In more complex problems, however, it can <span class="t8 t13 hoverable">become</span> very difficult to judge
how similar <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> individuals really are. For example, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> neural networks
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> compute the same function, but have completely <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> weights and

134

7.3. Is building intelligent machines a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> idea?
topologies. Indeed, due to Rice’s theorem it is impossible in general to decide
whether <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> algorithms compute the same function.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> solution is to use diversity control methods that do not rely on comparing the genotypes of the individuals, but rather by comparing properties
of their phenotypes. A simple example of this is Fitness Uniform Optimisation where the evolutionary algorithm tries to increase the diversity in the
population by increasing the diversity of fitness (Hutter and Legg, 2006). In
the case of the Fitness Uniform Selection Scheme (FUSS) this is achieved
through selection pressure (Hutter, 2002b; Legg et al., 2004), while for the
Fitness Uniform Deletion Scheme it is achieved by deletion (Legg and Hutter,
2005a). Although these methods have not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> been applied to the evolution
of programs or neural networks, they have proven to be effective on a number
of deceptive optimisation problems.
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">Another</span> possibility <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to mix biologically and theoretically derived
designs with evolution. For example, as a starting point <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> a model of the
neocortex based on biological studies, such as those in the previous section,
and then apply artificial evolution to modify and tune the model. Although we
<span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> not get the initial design <span class="t27 hoverable">right</span> due to limitations in our understanding of
the cortex, it is reasonable to suppose that in the space of all neural networks
this initial design is relatively close to the correct design, or a related design
that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> works. Starting with individuals that are reasonably close to the
target makes the optimisation problem that evolution has to solve orders of
magnitude easier.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> if each of the above suggestions succeeded in reducing the difficulty of
evolving <span class="t20 hoverable">intelligence</span> by several orders of magnitude, perhaps the biggest problem is still computer power. Advancing technology over the coming decades
will help, but this <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> still be too little. Perhaps the closest we <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span>
to nature’s planet scale evolution <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be to construct a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">world</span> wide network of machines donating computation time. After all, more than 2 million
<span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> of computer <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> have so <span class="t6 t9 t18 t26 hoverable">far</span> been donated to the Search for Extraterrestrial <span class="t20 hoverable">Intelligence</span> (SETI), why not something similar to search for artificial
intelligence?

7.3. Is building intelligent machines a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> idea?
It is impossible to <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> whether any of the approaches discussed in the previous section, or other approaches, will succeed in producing truly intelligent
machines. But this is not the point we want to make: the point is that it is
not obvious that they will all fail. This is important, because the impact of
this event <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be huge. Following the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> credible demonstration of <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span>
general <span class="t20 hoverable">intelligence</span> in a machine, for sure a <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> larger and more powerful
machine will be constructed shortly thereafter. This leads to what I. J. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Good</span>
referred to as an <span class="t20 hoverable">intelligence</span> explosion:

135

7. Discussion
“Let an ultraintelligent machine be defined as a machine that can
<span class="t6 t9 t18 t26 hoverable">far</span> surpass all the intellectual activities of any <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">man</span> <span class="t27 t28 hoverable">however</span> clever.
Since the design of machines is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of these intellectual activities, an ultraintelligent machine <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> design <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> better machines;
there <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> then unquestionably be an ‘intelligence explosion,’ and
the <span class="t20 hoverable">intelligence</span> of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">man</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be left <span class="t6 t9 t18 t26 hoverable">far</span> behind. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span>
ultraintelligent machine is the last invention that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">man</span> need <span class="t29 hoverable">ever</span>
make.” (Good, 1965)
The defining characteristic of our species is intelligence. It is not by superior
size, strength or speed that we dominate <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> on earth, but by our intelligence.
If our <span class="t20 hoverable">intelligence</span> were to be significantly surpassed, it is difficult to imagine
what the consequences of this <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be. It <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> certainly be a source of
enormous power, and with enormous <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> comes enormous responsibility.
Machine <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t22 hoverable">bring</span> unprecedented wealth and opportunity if
<span class="t27 hoverable">used</span> constructively and safely. Alternatively, it <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t22 hoverable">bring</span> about some kind
of a nightmare scenario. The latter possibility is certainly <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> known, being
a staple of science fiction. Positive fictional depictions are rare, probably
because casting the machines as villains is a convenient plot device. Outside
of <span class="t9 t16 t26 hoverable">works</span> of fiction, however, the implications of powerful machine <span class="t20 hoverable">intelligence</span>
are rarely encountered. Indeed, the <span class="t3 t5 t7 t9 t20 t28 t29 hoverable">whole</span> subject of truly intelligent machines
is generally avoided by academics, as noted at the start of this chapter.
If <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> accepts that the impact of truly intelligent machines is likely to be
profound, and that there is at least a small probability of this happening in
the foreseeable future, it is only prudent to try to prepare for this in advance.
If we wait until it seems very likely that intelligent machines will soon appear,
it will be too late to thoroughly discuss and contemplate the issues involved.
Historically technology has advanced in leaps and bounds, while social and
ethical considerations have developed more slowly, often only as a reaction to
the problems created by a technology after it arrived. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Even</span> what now seem to
be obvious moral principles, such as gender and racial equality, were debated
for centuries and are still not accepted in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> parts of the world. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that
the implications of powerful machine <span class="t20 hoverable">intelligence</span> are likely to be complex, we
<span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> expect to find <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> answers quickly. We need to be seriously working
on these <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> now.
A small but growing number of forward thinking individuals and organisations are thinking about these issues. Perhaps the premier organisation
dedicated to the safe and beneficial development of powerful artificial <span class="t20 hoverable">intelligence</span> is the Singularity Institute for Artificial <span class="t20 hoverable">Intelligence</span> (SIAI). In 2007 SIAI
organised a conference that attracted <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> speakers including Rodney
Brooks (director of the computer science and AI laboratory at MIT), Barney Pell (AI researcher and CEO of Powerset), Wendell Wallach (bioethics
lecturer at Yale), Sam Adams (IBM distinguished engineer), Paul Saffo (lecturer at Sanford), Peter Norvig (director of research at Google), Peter Thiel
(founder of Clarium Capital and co-founder of Paypal) and Ray Kurzweil (fu-

136

7.3. Is building intelligent machines a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> idea?
turologist, inventor and entrepreneur). Although this event was <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of the
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> of its kind, the calibre of these speakers makes it clear that issues surrounding the development of advanced artificial <span class="t20 hoverable">intelligence</span> are starting to be
<span class="t19 hoverable">taken</span> seriously. Other notable <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> who have spoken and <span class="t23 hoverable">written</span> about the
potential dangers of advanced artificial <span class="t20 hoverable">intelligence</span> in recent <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> include Bill
Joy (co-founder of <span class="t0 t8 t11 t14 t23 t25 t28 hoverable">Sun</span> Microsystems), Nick Bostrom (director of the Future of
Humanity Institute at Oxford), and Sir Martin Rees (professor at Cambridge
and president of the Royal Society). Hopefully this trend will continue.
At the SIAI itself the principle research fellow is Eliezer Yudkowsky. He
has <span class="t23 hoverable">written</span> a number of documents that deal mostly with safety and ethical
issues surrounding the development of powerful artificial intelligence, as <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> as
ideas on how he thinks so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> ‘Friendly AI’ should be developed. Although
these can be accessed through the SIAI website, none of his writings have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span>
appeared in mainstream peer reviewed journals. As AIXI is currently the only
comprehensive mathematical theory of machine super intelligence, SIAI follows
this <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with interest and lists (Hutter, 2007b) <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">among</span> their core readings.
PhD candidate Nick Hay, who is associated with SIAI, is examining whether
AIXI theory can be <span class="t27 hoverable">used</span> to study the safety of intelligent machines. Hopefully
the gentle introduction to AIXI in <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> 2 will encourage more <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> to
explore some of these research directions.
In 1887 <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Lord</span> Acton famously wrote, “Power tends to corrupt, and absolute
<span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> corrupts absolutely.” It is not that <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> itself is inherently <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> or
evil, rather it grants the ability to be so: <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> amplifies intention. Although
Acton’s quote has a ring of <span class="t3 t7 t9 t10 t15 t19 t22 hoverable">truth</span> to it, perhaps it is excessively pessimistic
about <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> nature. In any case, if there is <span class="t29 hoverable">ever</span> to be something approaching
absolute power, a super intelligent machine <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">come</span> close. By definition,
it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> be capable of achieving a vast range of goals in a wide range of
environments. If we carefully prepare for this possibility in advance, not only
<span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> we avert disaster, we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> <span class="t22 hoverable">bring</span> about an age of prosperity unlike
anything seen before.

137

A. Notation and Conventions
When defining a symbol or equation we use := to stress that the item on the
left is newly defined. When we just need to assert that <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> are equal
we use the plain = symbol. Not equal is 6=, and approximately equal ≈. By
x ≫ y we mean that x is <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> greater than y, and similarly for ≪.
The cardinality of a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> S is <span class="t23 hoverable">written</span> |S|. The empty <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> is <span class="t23 hoverable">written</span> ∅, subset
with the possbility of equality ⊆, and proper subset ⊂. The natural numbers
are denoted N := {1, 2, . . .}, naturals with 0 included N0 := {0, 1, 2, 3, . . .},
n
the integers Z := {. . . − 2, −1, 0, 1, 2, . . .}, the rational numbers Q := { m
:
n, m ∈ Z}, and the real numbers R. We use standard notation for intervals
on the real line. Specifically, we define [x, y] := {z ∈ R : x ≤ z ≤ y}, and
(x, y) := {z ∈ R : x < z < y}. Intervals such as (0, 1] have the obvious
meaning.
loga x is the logarithm of x base a. ln x := loge x where e = 2.71828 . . ..
When the specific base <span class="t27 hoverable">used</span> makes no difference we simply write log x. The
factorial of n, <span class="t23 hoverable">written</span> n!, is defined 0! = 1 and n! := n(n − 1)(n − 2) · · · 1 for
n!
. Although 00 is technically
n ∈ N. The binomial is <span class="t23 hoverable">written</span> nr := (n−r)!r!
indeterminate, in derivations we follow the standard convention and <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> its
value to be 1. The Kronecker delta symbol δab is defined to be 1 if a = b, and
0 otherwise.
An alphabet is a finite <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of elements which are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> symbols. For example,
{a, b, c, . . . , z} is an alphabet, as is {up, down, left, right}. Mostly we use the
binary alphabet B := {0, 1}, in which context the symbols are <span class="t6 t10 t15 hoverable">known</span> as bits. A
binary string is a finite ordered n-tuple of bits. This is denoted x := x1 x2 . . . xn
where ∀i ∈ {1, . . . , n} : xi ∈ B, or more succinctly, x ∈ Bn . The 0-tuple is
denoted ǫ and is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> the null string. The expression
B≤n represents the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span>
S
of binary strings of length n or less, and B∗ := n∈N Bn is the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of all binary
strings. A substring of a string x is itself a string defined xj:k := xj xj+1 . . . xk
where 1 ≤ j ≤ k ≤ n. Concatenation is indicated by juxtaposition, for
example, if x = x1 x2 ∈ B2 and y = y1 y2 y3 ∈ B3 , then xy = x1 x2 y1 y2 y3 . In
some cases we will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> concatenate constants, for example if x ∈ B∗ then x101
is the string x with 101 added on the end. By ℓ(x) we mean the length of the
string x, for example, if x ∈ Bn then ℓ(xx) = 2ℓ(x) = 2n, and for k < n we
have ℓ(xj:k ) = k − j + 1.
Unlike strings which always have finite length, a binary sequence ω is an
infinite list of bits, for example ω := x1 x2 x3 . . . ∈ B∞ . For a sequence ω ∈ B∞
we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be interested in the prediction of the (t+1)th bit, denoted ωt+1 , <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span>
that we have so <span class="t6 t9 t18 t26 hoverable">far</span> observed only the finite initial string ω1:t ∈ Bt . Obviously
139

A. Notation and Conventions
a string <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be concatenated onto the <span class="t7 t10 t15 t25 hoverable">end</span> of a sequence as a sequence has
no end, <span class="t27 t28 hoverable">however</span> a sequence can be concateded onto the <span class="t7 t10 t15 t25 hoverable">end</span> of a string.
Our notation and usage of measure theory in this thesis is non-standard
as it makes working with strings and sequences easier. Although the usage
is explained in the text, for the mathematically inclined the relationship to
standard measure theory is as follows: For a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> sample space Ω, a probability
measure ν is a type of [0, 1] valued function defined over a <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of subsets of Ω,
<span class="t6 t10 t15 hoverable">known</span> as a σ-algebra. For prediction we need to measure the probability of
sets of sequences that begin with a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> string, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> we need a σ-algebra that
contains these sets, the so <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> cylinder sets defined, Γx := {xω : ω ∈ B∞ }
for x ∈ B∗ . Such a σ-algebra can easily be constructed by considering the
smallest σ-algebra that contains the cylinder sets. In this thesis we do not
need to be able to measure arbitrary sets <span class="t5 t7 hoverable">within</span> this σ-algebra and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> we
adopt a simplified notation for measures by defining ν(x) to be shorthand for
ν(Γx ). In other words, ν(x) is the probability that a binary sequence sampled
<span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to the distribution ν begins with the string x ∈ B∗ . This shorthand
does not get us into trouble as it can be proven that measures defined on the
<span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> B∗ correspond uniquely to measures defined on the full σ-algebra (Calude,
2002). As we are often interested in the probability that a string x ∈ B∗ follows
a string y ∈ B∗ in a sequence sampled <span class="t0 t1 t3 t4 t5 t7 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t19 t21 t22 t24 t26 t27 t29 hoverable">according</span> to some distribution µ, we
further define the shorthand notation µ(yx) := µ(yx)/µ(y).
Probability distributions, measures and semi-measures are usually denoted
by a lowercase greek letter, for example, µ, ν, ̺. For an unnamed probabilty
distribution over a random variable X, we write P (X). Eµ (X) is the expected
value of X with respect to µ. When µ is the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> distribution we can omit
this from the notation for expectations.
Throughout the thesis we refer to an agent, usually denoted by the conditional measure π, that is interacting with some kind of an environment, usually
denoted by the conditional measure µ. This interaction occurs by having action symbols from the alphabet A being sent by the agent to the environment,
and perception symbols from the alphabet X being sent back in the other direction. Each perception consists of an observation from the alphabet O and
a reward from the alphabet R. When referring to the full measure defined by
π interacting with µ we use the symbol πµ .
To denote symbols being sent we use the lower case variable names a, o,
r and x for actions, observations, rewards and perceptions respectively. We
index these in the <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> in which they occur, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> a1 is the agent’s <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> action,
a2 is the <span class="t6 hoverable">second</span> action and so on. The agent and the environment <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> turns at
sending symbols, starting with the agent. This produces a history of actions,
observations and rewards which can be written, a1 o1 r1 a2 o2 r2 a3 o3 r3 . . .. As
we refer to interaction histories a lot, we need to be able to represent these
compactly. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> trick that we use is to squeeze symbols <span class="t16 t21 hoverable">together</span> and then
index them as blocks of symbols. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> for the complete interaction history
up to and including cycle t, we can write ax1:t := a1 x1 a2 x2 a3 . . . at xt . For the
history before cycle t we use ax<t := ax1:t−1 . <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that xt := ot rt .

140

Some of our results will have the property of holding <span class="t5 t7 hoverable">within</span> an additive
constant that is independent of the variables in the expression. We indicate
this by placing a small plus above the equality or inequality symbol. For
+
+
+
example, f < g <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that ∃c ∈ R, ∀x : f (x) < g(x) + c. If f < g < f , we
+
write f = g. When using standard “big O” notation this is superfluous as
expressions are already understood to hold <span class="t5 t7 hoverable">within</span> an independent constant,
<span class="t27 t28 hoverable">however</span> we will sometimes still use it for consistency of notation. Similarly,
×
×
×
we define f ≤ g to mean that ∃c ∈ R, ∀x : f (x) ≤ c · g(x), and if f ≤ g ≤ f we
×
write f = g.

141

B. Ergodic MDPs admit
self-optimising agents
A number of results similar to Theorem 3.4.5 exist, such as the proof that with
probability 1 the Q-learning algorithm converges to optimal in an ergodic MDP
environment (Watkins and Dayan, 1992). Unfortunately, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to establish
the necessary chain of results we need something a little different: we require
the convergence to hold for any history and with an effective horizon that
goes to infinity. For a precise statement of the theorem, necessary technical
conditions and proof, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Theorem 5.38 in (Hutter, 2005). Although Hutter’s
proof is straight forward, it assumes a <span class="t3 hoverable">certain</span> continuity condition on value
functions based on estimated transition probabilities for ergodic MDPs. This
is left as a (large!) exercise for the reader (Problem 5.12 in Hutter, 2005). In
this appendix we follow the plan of attack suggested in Problem 5.12 to prove
the missing continuity result, namely, Theorem B.4.3 that appears at the <span class="t7 t10 t15 t25 hoverable">end</span>
of this section.

B.1. Basic definitions
For definitions of agents, environments, ergodic and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> other <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> used,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> Chapters 2 and 3. Rather than just talking about agents, as we do in the
rest of this thesis, here we will use the slightly more refined notion of a policy.
Essentially, a policy is some rule or mode of operation that an agent has. For
example, when navigating a maze an agent’s policy <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to always turn left
at each intersection. The distinction is useful when we wish to consider agents
which have multiple <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> modes of operation. That is, an agent which
follows some policy for a while and then, perhaps due to <span class="t3 hoverable">certain</span> conditions
such as a lack of success, switches to a <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> policy.
The proofs in this section will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> extensive use of results from linear
algebra. The mathematical notation <span class="t27 hoverable">used</span> is fairly standard: we will represent
real valued matrices with capital letters, for example A ∈ Rn×m . By Aij we
mean the single scalar element of A on the ith row and j th column. By A∗j we
mean the j th column of A and similarly for Ai∗ . We represent vectors with a
bold lowercase variable, for example a ∈ Rn . Similar to the case for matrices,
by ai we mean the ith element of a. In some situations a matrix or vector <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span>
already have other indexes, in this case we <span class="t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">place</span> square brackets around it and
then index so as to avoid confusion. For example, [bπ ]i is the ith element of

143

B. Ergodic MDPs admit self-optimising agents
the vector bπ . We represent the classical adjoint of a matrix A by adj(A) and
the determinant by det(A).
In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to express the Markov chains as matrices, we need to be able
to index the actions and perceptions with natural numbers. This <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> be
achieved by a simple numbering scheme; here we will just assume that this
has already been done. That is, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> loss of generality we assume that
X := {1, . . . , n1 } and A := {1, . . . , n2 } for n1 , n2 ∈ N. The difficulty with this
is that each perception x is still associated with an observation o and a reward
r. To recover the reward associated with x we write r(x) ∈ R, and similarly
for o(x) ∈ O. Both R and O are finite, as always, but otherwise unspecified.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> π be a stationary policy such that ∀ax<k ak : π(ax<k ak ) = π(o(x)k−1 ak ).
That is, under the policy π the distribution of actions depends on only the last
observation in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that is independent of k. It follows that the equation for
the k th perception xk <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> history ax<k is,
π(ax<k ak ) µ(ax<k axk ) = π(o(xk−1 )ak ) µ(o(xk−1 )axk ).
Thus, for a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> µ and π the next perception xk depends on only the previous
observation o(x)k−1 in a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> that is independent of k, that is, π and µ <span class="t16 t21 hoverable">together</span>
<span class="t6 t7 t8 t9 t13 t14 t16 t17 t19 t20 t21 t28 t29 hoverable">form</span> a stationary Markov chain. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that everything is stationary, we can
drop the index k and write x, a and x′ for the perception, action and the
following perception.
From the definition of an MDP it is clear that we can represent a (stationary)
MDP as a <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">three</span> dimensional Cartesian tensor D ∈ Rn1 ×n2 ×n1 defined ∀x, a, x′ ,
Dxax′ := µ(o(x)ax′ ).

<span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">Note</span> that the only <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of x that plays a <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> in defining the structure of D is
the associated observation o(x), as required by our definition of an MDP. The
reward associated with x, that is r(x), has no role.
We can now express the interaction of the policy and the environment as a
square stochastic matrix T ∈ Rn1 ×n1 defined,
X
X
Txx′ :=
π(o(x)a) µ(o(x)ax′ ) =
π(o(x)a)Dxax′ .
(B.1)
a∈A

a∈A

It should be noted that this characterisation of the interaction between µ
and π as a stochastic matrix T is only possible if µ is a stationary MDP and
π is a stationary policy. Fortunately this is all we will need for optimality,
<span class="t0 t1 t2 t6 t7 t8 t9 t11 t12 t13 t14 t16 t17 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">though</span> we will briefly have to consider non-stationary policies in Section B.3
in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to prove this. It is worth keeping in <span class="t0 t1 t2 t3 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t24 t25 t27 t28 t29 hoverable">mind</span> that when we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> a matrix T
this represents the complete system of an agent and environment interacting,
rather than just an environment. That is, it represents the interaction measure
π
µ.
Define a matrix Rxa ∈ Rn1 ×n2 to be the expected reward when choosing
action a after perception x. Further define the column vector rπ ∈ Rn1 where,
X
[rπ ]x := E(Rxa |x) =
π(xa)Rxa .
a∈A

144

B.1. Basic definitions
The advantage of expressing everything in matrix notation is that the full
range of linear algebra techniques is now easy to <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> with. For example, the
probability of transiting between any <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> perceptions can be easily computed
by taking powers of T : if we have a perception i then the probability that
exactly m cycles later the perception will be j, is <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by [T m ]ij .
B.1.1 Definition.
For an environment µ and a policy π the expected
average value in cycles k to m <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> history ax<k is defined to be,
πµ
Vkm
(ax<k ) :=

1 X
m ax

π
µ (ax<k axk:m )

m
X

r(xi ).

i=k

k:m

Additionally we define the expected <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> run average value to be,
πµ
πµ
Vk∞
(ax<k ) := lim Vkm
(ax<k ),
m→∞

when this limit exists.
When k = 1 there is no history, that is, ax<k = ǫ, the null string. In this
πµ
πµ
case we simplify the notation slightly by defining V1m
:= V1m
(ǫ). Similarly
πµ
for V1∞ .
In matrix notation we can express the expected <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> run average value for
each initial perception x1 ∈ X = {1, . . . , n1 } as a vector of value functions,
πµ
V1∞

if the limit exists.


πµ
V1∞
(1)


..
= 
=
.
πµ
V1∞ (n1 )


m−1
1 X k
lim
T
m→∞ m
k=0

!

rπ ,

(B.2)

B.1.2 Definition. For an environment µ the optimal policy, denoted π µ ,
is defined as:
πµ
π µ := arg max V1∞
,
π

where the maximum is <span class="t19 hoverable">taken</span> over all policies, including non-stationary ones.
In some sense the optimal policy is the ideal policy. However, the optimal
policy is usually only optimal with respect to the specific environment for which
it was defined. If we do not <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> the specific details of the environment that
the policy will face in advance, the best we can do is to have a policy which will
adapt to the environment based on experience. In such a situation the policy
is unlikely to be optimal as it will probably <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> some non-optimal actions as
it learns about the environment it faces. In this situation the following concept
is useful:

145

B. Ergodic MDPs admit self-optimising agents
B.1.3 Definition. We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that a policy π is self-optimising in an environment µ if its expected average value converges to the optimal expected average
value as m → ∞, that is,
πµ
πµ µ
V1m
−→ V1m
.
Intuitively this <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that the expected performance of the policy in the
<span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> run is as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> as an optimal policy which was designed with complete
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> of the environment in advance. Classes of environments which
admit self-optimising policies are important because they are environments in
which it is possible for general purpose policies to adapt their behaviour until
eventually their actions <span class="t8 t13 hoverable">become</span> optimal.

B.2. Analysis of stationary Markov chains
In this section we will establish some of the properties of Markov chains that
we will require. Our <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> lemma shows that the key term (I − αT )−1 can be
expanded using a Taylor series. The proof of this lemma and the following
lemma and theorem are based on the proof of Proposition 1.1 from Section 4.1
of (Bertsekas, 1995).
B.2.1 Lemma. For a stochastic matrix T ∈ Rn×n and scalar α ∈ (0, 1) there
exist stochastic matrices T ∗ ∈ Rn×n and H ∈ Rn×n such that
(I − αT )−1 = (1 − α)−1 T ∗ + H + O(|1 − α|)
where limα→1 O(|1 − α|) = 0.
Proof. Define the n × n matrix,
M (α) := (1 − α)(I − αT )−1 .
Applying the matrix inversion formula we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that
M (α) = (1 − α)

adj(I − αT )
,
det(I − αT )

where the determinant det(I − αT ) is an nth <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> polynomial in α and the
classical adjoint adj(I − αT ) is an n × n matrix of n−1th <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> polynomials in
α. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Therefore</span> M (α) can be expressed as an n × n matrix where each element is
either zero or a fraction of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> polynomials in α that have no common factors.
We <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that the denominator polynomials of M (α) <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> have 1 as a
root as this <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> imply that the corresponding element of M (α) → ∞ as
α → 1. This <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> happen because,
(1 − α)−1 M (α)rπ = (I − αT )−1 rπ
146

B.2. Analysis of stationary Markov chains




where ∀i ∈ {1, . . . , n} :  (I − αT )−1 rπ i  ≤ (1 − α)−1 maxk [rπ ]k . Clearly


then the absolute value of the elements of M (α)rπ are bounded by maxk [rπ ]k 
for α < 1. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Therefore</span> we can express the ij th element of M (α) as,
Mij (α) =

γ(α − φ1 ) · · · (α − φp )
(α − ψ1 ) · · · (α − ψq )

where γ, φi , ψj ∈ R for all i ∈ {1, . . . p} and j ∈ {1, . . . q}.
Using this expression we can <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> a Taylor expansion of M (α) about 1 as
follows. Firstly, define the matrix T ∗ ∈ Rn×n as,
T ∗ := lim M (α)
α→1

and the matrix H ∈ Rn×n as


∂
Hij := − Mij (α)]
.
∂α
α=1

(B.3)

That is, H is a matrix having as it ij th element the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> derivative of −Mij (α)
with respect to α evaluated at α = 1.
From the equation for a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> Taylor expansion,
M (α) = T ∗ + (1 − α)H + O((1 − α)2 )
where O((1 − α)2 ) is an α-dependent matrix such that
O((1 − α)2 )
= 0.
α→1
1−α
lim

Dividing through by (1 − α) we get
(1 − α)−1 M (α) = (1 − α)−1 T ∗ + H + O(|1 − α|)
where limα→1 O(|1 − α|) = 0. The result then follows as (I − αT )−1 = (1 −
α)−1 M (α) by definition.
2
We will soon show that T ∗ as defined above plays a significant role in the
analysis. Before looking at this more closely, we will firstly prove some useful
identities.
B.2.2 Lemma. It follows from the definitions of T ∗ and M (α) that,
T ∗ = T ∗T = T T ∗ = T ∗T ∗
and for k ∈ N

(T − T ∗ )k = T k − T ∗ .

147

B. Ergodic MDPs admit self-optimising agents
Proof. By subtracting the identity αI = α(I − αT )(I − αT )−1 from the
identity I = (I − αT )(I − αT )−1 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that,
(1 − α)I = (I − αT )(1 − α)(I − αT )−1

and thus,
αT (1 − α)(I − αT )−1 = (1 − α)(I − αT )−1 + (α − 1)I.
Taking α → 1 gives,

lim αT · lim (1 − α)(I − αT )−1 = lim (1 − α)(I − αT )−1 + lim (α − 1)I

α→1

α→1

α→1

α→1

which, using the definition of M (α), becomes,
T · lim M (α) = lim M (α).
α→1

α→1

∗

Finally using the definition of T this reduces to just,
T T ∗ = T ∗.
Using essentially the same argument it can <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> be shown that T ∗ T = T ∗ . It
then immediately follows that ∀k ∈ N : T k T ∗ = T ∗ T k = T ∗ .
From the relation T T ∗ = T ∗ it follows that T ∗ − αT T ∗ = T ∗ − αT ∗ and so
(I − αT )T ∗ = (1 − α)T ∗ and thus,
T ∗ = (1 − α)(I − αT )−1 T ∗ .

Taking α → ∞ gives,

lim T ∗ = lim (1 − α)(I − αT )−1 · lim T ∗ ,

α→1

α→1

α→1

∗

which by the definition of T is just,
T ∗ = T ∗T ∗.
This establishes the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> result.
The <span class="t6 hoverable">second</span> result will be proven by induction. Trivially (T −T ∗ )1 = T 1 −T ∗
which establishes the case k = 1. Now assume that the induction hypothesis
holds for the k th case and consider the (k + 1)th case:
(T − T ∗ )k+1

=
=
=
=

(T − T ∗ )k (T − T ∗ )
(T k − T ∗ )(T − T ∗ )

T k+1 − T k T ∗ − T ∗ T k + T ∗ T ∗
T k+1 − T ∗ .

The <span class="t6 hoverable">second</span> line follows from the induction assumption and the final line from
the results above.
2
We will use these simple relations frequently in the proofs that follow <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span>
further comment. Now we can prove a key result about the structure of T ∗ :
it is the limiting average distribution for the matrix T .

148

B.2. Analysis of stationary Markov chains
B.2.3 Theorem. For a stochastic matrix T ∈ Rn×n and ∀m ∈ N,
T∗ =

m−1
1 X k
1
T + (T m − I)H.
m
m
k=0

where H ∈ Rn×n is the matrix that satisfies Lemma B.2.1.
Proof. As T is a stochastic matrix, from Lemma B.2.1 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that there
exist matrices H ∈ Rn×n and T ∗ ∈ Rn×n such that,
H = (I − αT )−1 − (1 − α)−1 T ∗ − O(|1 − α|)

(B.4)

where α ∈ (0, 1) and limα→1 O(|1 − α|) = 0.
<span class="t27 t28 hoverable">However</span> from the geometric <span class="t14 hoverable">series</span> equations it follows that,
(I − αT )−1 − (1 − α)−1 T ∗

=

∞
X

k=0

=

αk T k − T ∗
∞
X

I − T∗ +

k=1

∞
X

αk =

∞
X

k=0

k=0

(α(T − T ∗ ))

αk (T k − T ∗ )

k

α(T − T ∗ )
I − α(T − T ∗ )

=

I − T∗ +

=

(I − α(T − T ∗ ))−1 − T ∗ .

Substituting this result into Equation (B.4) and taking α → 1,


H = lim (I − α(T − T ∗ ))−1 − T ∗ − O(|1 − α|)
α→1

= (I − T + T ∗ )−1 − T ∗ .

Multiplying by (I − T + T ∗ ) and then T ∗ we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that,
(I − T + T ∗ )H
H − T H − T ∗H

T ∗H − T ∗H − T ∗H
T ∗H

=
=
=
=

I − (I − T + T ∗ )T ∗
I − T ∗ + T T ∗ − T ∗T ∗ = I − T ∗

T∗ − T∗
0.

It now <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> follows that H − T H = I − T ∗ and so T ∗ + H = I + T H.
Multiplying by T k on the left for k ∈ N0 now gives,
T ∗ + T k H = T k + T k+1 H.

Summing over k = 0, 1, . . . , m − 1 and cancelling equal terms and dividing
through by m produces,
mT ∗ +

m−1
X

T kH

=

k=0

k=0

mT ∗ + H

m−1
X

=

m−1
X

Tk +

m−1
X

T k+1 H

k=0

T k + T mH

k=0

149

B. Ergodic MDPs admit self-optimising agents
from which the result follows as m 6= 0.

2
Pm−1

1
k
∗
This establishes bounds on the convergence of m
k=0 T to T that we
will need. As H is bounded, simply taking m → ∞ yields the following:

B.2.4 Corollary. For a stochastic matrix T ∈ Rn×n ,
m−1
1 X k
T .
m→∞ m

T ∗ = lim

k=0

By applying this result to Equation (B.2) we can now express the expected
<span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> run average value very simply in terms of T ∗ ,
!
m−1
1 X k
πµ
V1∞ :=
lim
T
rπ = T ∗ rπ .
(B.5)
m→∞ m
k=0

∗

<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> by the existence of T we can infer that the expected <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> run average
value <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> exists in this case.
B.2.5 Corollary. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> µ be a stationary MDP environment and π a stationary
policy. ∀m ∈ N,
 
1
πµ
πµ
.
|V1∞
− V1m
|=O
m
Proof. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> T ∈ Rn×n represent the Markov chain formed by the interaction
of µ and π. From Theorem B.2.3 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that ∀m ∈ N,
T∗ −

m−1
1 X k
1
T = (T m − I)H.
m
m
k=0

Multiplying by rπ on the <span class="t27 hoverable">right</span> gives
!
m−1
1
1 X k
∗
(T m − I) Hrπ ,
T
rπ =
T rπ −
m
m
k=0

and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the result follows as the elements of both T m and H are bounded. 2
Of course this result is not surprising as we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> expect the expected
average value to converge to its limit in a reasonable <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">way</span> when both the
environment and policy are stationary.
Finally <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> us <span class="t1 t2 t3 t4 t6 t8 t9 t12 t14 t15 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">note</span> some technical results on the relationship between T and
T ∗.
B.2.6 Lemma. For an ergodic stochastic matrix T ∈ Rn×n the row vectors
of T ∗ are all the same and define a stationary distribution under T .

150

B.2. Analysis of stationary Markov chains
This is a standard result in the theory of ergodic Markov chains. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">See</span> for example <span class="t12 t19 t25 t26 t28 hoverable">Chapter</span> V of (Doob, 1953) or any <span class="t25 hoverable">book</span> on discrete stochastic processes
for a proof.
The following result shows that the limiting matrix T ∗ is in some sense
continuous with respect to small changes in T . This will be important because
it <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that if we have an estimate of T that converges in the limit then our
estimate of T ∗ will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> converge.
B.2.7 Theorem. For an ergodic stochastic matrix T ∈ Rn×n the matrix
T ∗ ∈ Rn×n is continuous in T in the following sense: If T̂ ∈ Rn×n is a
stochastic matrix where maxij Tij − T̂ij  is small, then ∃cT > 0 which depends




on T , such that maxij Tij∗ − T̂ij∗  ≤ cT maxij Tij − T̂ij .

Proof. For an ergodic square matrix T ∈ Rn×n the row vectors of T ∗ are all
the same and correspond to the stationary distribution row vector t∗ ∈ R1×n .
That is,
 ∗ 
t
 .. 
∗
(B.6)
T = . 
t∗

where t∗ T = t∗ and for all distribution vectors t ∈ R1×n we have tT ∗ = t∗ .
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> T has an eigenvalue of 1 with t∗ being the corresponding left eigenvector.
From linear algebra we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that ∀T ∈ Rn×n ,
adj(I − T ) (I − T ) = det(I − T )I.
<span class="t27 t28 hoverable">However</span> as T has an eigenvalue of 1, det(I − T ) = 0 and thus,
adj(I − T ) T = adj(I − T ),
or equivalently, ∀i ∈ {1, . . . , n},
[adj(I − T )]i∗ T = [adj(I − T )]i∗ .
Because {t∗ } is a basis for the eigenspace corresponding to the eigenvalue 1,
[adj(I − T )]i∗ <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">must</span> be in this eigenspace. That is, ∀i ∈ {1, . . . , n}, ∃ci ∈ R:

[adj(I − T )]i∗ = cof 1i (I − T ), . . . , cof ni (I − T ) = ci t∗

where the cofactor is defined cof ji (I − T ) := (−1)j+i det(minorji (I − T )).
As T has an eigenvalue of 1 with geometric multiplicity 1 it follows that
I − T has an eigenvalue of 0 <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> with geometric multiplicity 1. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> the
nullity of I − T is 1 and so rank(I − T ) = n − 1. While we define the rank
of a matrix to be the dimension of its column or row space, it <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> can be
defined as the size of the largest non-zero minor and the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> defintions can be
proven to be equivalent. As the adjoint is composed of <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> n − 1 minors it
151

B. Ergodic MDPs admit self-optimising agents
immediately follows that adj(I − T ) 6= 0 and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> ∃k, which depends on T ,
such that ck > 0.
As minorjk (I −T ) is an (n−1)×(n−1) sub-matrix of (I −T ) the determinant
of this is an <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> n−1 polynomial in the elements of T . Thus, by the continuity
of polynomials, ∃c′ > 0 such that for a sufficiently small ε > 0 change in any
element of T we will get at most a c′ ε change in each cof jk (I − T ). <span class="t27 t28 hoverable">However</span>
we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that t∗ = c1k [adj(I − T )]k∗ , and so an ε change in the elements of
T results in at most a

cT :=

′

c
ck

c′
ck ε

change in the elements of t∗ and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> T ∗ . Define

to indicate that this constant depends on T and we are done.

2

B.3. An optimal stationary policy
We now turn our attention to optimal policies. While our analysis so <span class="t6 t9 t18 t26 hoverable">far</span>
has only dealt with stationary policies, in general optimal policies need not
be stationary. As non-stationary policies are more difficult to analyse our
preference is to deal with only stationary policies if possible. In this section
we prove that for the class of ergodic finite stationary MDP environments an
optimal policy can indeed be chosen so that it is stationary. This will simplify
our analysis in later sections. However, in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to show this result we will
need to briefly consider policies which are potentially non-stationary. The
proofs in this section follow those of Section 4.2 in (Bertsekas, 1995).
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> us assume that the policy π is deterministic but not necessarily stationary, that is, π := {π1 , π2 , . . .}. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> in the k th cycle we apply πk . Define
pi (x) := arg maxy∈A πi (xa) to be the action chosen by policy π in cycle i.
Clearly this is unique for deterministic π.
In <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> some of the equations that follow more manageable we
need to define the following <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> mappings. For any function f : X → R
and deterministic policy π := {π1 , π2 , . . .} we define the mapping Bπk for any
k ∈ N to be ∀x ∈ X ,
X
(Bπk f )(x) := Rxpk (x) +
µ( x pk (x) x′ )f (x′ ).
x′ ∈X

Of interest will be the policy that simply selects the action which maximises
this expression in each cycle for any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> x. For this we define for any function
f : X → R and ∀x ∈ X ,
#
"
X
′
′
(Bf )(x) := max Rxa +
µ(xax )f (x ) .
a∈A

x′ ∈X

Clearly this policy is stationary as the maximising a depends only on x and is
independent of which cycle the system is in. By (B 2 f )(x) we mean (B(Bf ))(x)
and similarly higher powers such as (B i f )(x) and (Bπi k f )(x). The equation
Bf = f is the <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> <span class="t6 t10 t15 hoverable">known</span> Bellman equation (Bellman, 1957).

152

B.3. An optimal stationary policy
An elementary property of the mappings Bπk and B is their monotonicity
in the following sense.
B.3.1 Lemma. For any f, f ′ : X → R such that ∀x ∈ X : f (x) ≤ f ′ (x) and
for any possibly non-stationary deterministic policy π := {π1 , π2 , . . .}, we have
∀x ∈ X , ∀i, k ∈ N,
(Bπi k f )(x) ≤ (Bπi k f ′ )(x)
and
(B i f )(x) ≤ (B i f ′ )(x).
Proof.
Clearly the cases B 1 and Bπ1k are <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> from their definitions. A
simple induction argument establishes the general result.
2
Define the column vector e := (1, . . . , 1)t ∈ Rn×1 . Using these mappings
we can now prove that the optimal policy can be chosen stationary if <span class="t3 hoverable">certain</span>
conditions hold.
B.3.2 Theorem. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> µ be a finite stationary MDP environment. If λ ∈ R is
a scalar and h ∈ Rn×1 a column vector such that ∀x ∈ X ,
#
"
X
′
(B.7)
λ + [h]x = max Rxa +
µ(xax )[h]x′
a∈A

x′ ∈X

or equivalently,
λe + h = Bh,
then
∗µ
πµ
λ = V1∞
:= max V1∞
.
π

µ

Furthermore, if a stationary policy π attains the maximum in Equation (B.7)
πµ µ
for each x then this policy is optimal, that is, V1∞
= λ.
Proof.
We have λ ∈ R and h ∈ Rn×1 such that for any (possibly nonstationary) policy π = {π1 , π2 , . . .} and cycle m ∈ N and ∀xm ∈ X ,
λ + [h]xm ≥ Rxm pm (xm ) +

X

µ( xm pm (xm ) xm+1 )[h]xm+1 .

xm+1 ∈X

Furthermore, if πm attains the maximum in Equation (B.7) for each xm ∈ X
then equality holds in the mth cycle and pm (xm ) is optimal for this single
cycle. The main idea of this proof is to extend this result so that we get a
policy which is optimal across all cycles.
Using the mapping Bπm we can express the above equation more compactly
as,
λe + h ≥ Bπm h.
153

B. Ergodic MDPs admit self-optimising agents
Applying now Bπm−1 to both sides and using the monotonicity property from
Lemma B.3.1 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that,
λe + Bπm−1 h ≥ Bπm−1 Bπm h.
<span class="t27 t28 hoverable">However</span> we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that λe + h ≥ Bπm−1 h and so it follows that,
2λe + h ≥ Bπm−1 Bπm h.
Repeating this m times we get
mλe + h ≥ Bπ1 Bπ2 · · · Bπm h,
where equality continues to hold in the case where πk attains the maximum
in Equation (B.7) in each cycle k ∈ {1, . . . , m}. When this is the case we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span>
that π is optimal for the cycles 1 to m.
From the definition of Bπk we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that,
(
)

m
X


Rxk pk (xk ) x1 , π, µ
[Bπ1 Bπ2 · · · Bπm h]x1 = E [h]xm+1 +
k=1

is the total expected reward over m cycles from the initial perception x1 to
the final perception xm+1 under policy π and environment µ. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> ∀x1 ∈ X ,
(
)

m
X


Rxk pk (xk ) x1 , π, µ
mλ + [h]x1 ≥ E [h]xm+1 +
k=1

where equality holds if πk attains the maximum in Equation (B.7) in each
cycle.
Dividing by m, gives ∀x1 ∈ X ,
)
(m

X

	
1 
1
1

Rxk pk (xk ) x1 , π, µ . (B.8)
λ + [h]x1 ≥ E [h]xm+1 |x1 , π, µ + E
m
m
m
k=1

Taking m → ∞ this reduces to ∀x1 ∈ X ,
)
(m

X

1

λ ≥ lim
E
Rxk pk (xk ) x1 , π, µ ,
m→∞ m
k=1

or equivalently,

πµ
λ ≥ V1∞
,

where equality holds if πk attains the maximum in Equation (B.7) for each
πµ
πµ
cycle. When this is the case, π is optimal and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> V1∞
= maxπ V1∞
= λ.
Furthermore, we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that this optimal policy is stationary because in Equation
(B.7) the action a only depends on the current perception x and is independent
of the cycle number. We call this optimal stationary policy π µ .
2

154

B.4. Convergence of expected average value
The above result only guarantees the existence of an optimal stationary
policy π µ for a stationary MDP environment µ in the case where there is
a solution to the Bellman equation λe + h = Bh. Fortunately for ergodic
MDPs it can be shown that such a solution always exists (our definition of
ergodicity implies condition (2) of Proposition 2.6 in (Bertsekas, 1995) where
the existence of a solution is proven). It now follows that:
B.3.3 Theorem. For any ergodic finite stationary MDP environment µ there
exists an optimal stationary policy π µ .
This is a useful result because the interaction between a stationary MDP
environment and a stationary policy is <span class="t0 t1 t5 t10 t12 t13 t15 t19 t20 t28 hoverable">much</span> simpler to analyse than the nonstationary case. We will refer back to this result a number of times when we
need to assert the existence of an optimal stationary policy.
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">One</span> thing that we have not shown is that the optimal policy with respect
to a <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> MDP can be computed. <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">Given</span> that our MDP is finite and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">therefore</span>
the number of possible stationary deterministic policies is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> finite we <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span>
expect that this problem should be solvable. Indeed, it can be shown that the
Policy Iteration algorithm is able to compute an optimal stationary policy in
this situation (see Section 4.3 of Bertsekas, 1995).

B.4. Convergence of expected average value
Our goal is to find a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> policy for an unknown stationary MDP µ. Because
we do not <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> the structure of the MDP, that is µ, we create an estimate
µ̂ and then find the optimal policy with respect to this estimate, which we
will call π µ̂ . Our hope is that if our estimate µ̂ is sufficiently close to µ, then
π µ̂ will perform <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> compared to the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> optimal policy π µ . Specifically we
 πµ̂ µ

πµ µ 
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> V1∞
− V1∞
= 0.
In the analysis that follows we will need to be careful about whether we are
talking about the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> environment µ, our estimate of this µ̂, or various combinations of environments interacting with various policies. Sometimes policies
will be optimal with respect to the environment that they are interacting with,
sometimes they will only be optimal with respect to an estimate of the environment that they are actually interacting with, and in some cases the policy
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span> be arbitrary. Needless to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">say</span> that care is required to avoid mixing <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span>
up.
As defined previously, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">let</span> D ∈ RX ×A×X represent the chronological system
µ and D̂ ∈ RX ×A×X the chronological system µ̂. From Equation (B.1) we
<span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that the matrix T ∈ RX ×X representing the Markov chain formed by a
policy π interacting with µ is defined by,
X
Txx′ :=
π(xa)Dxax′ .
a∈A

155

B. Ergodic MDPs admit self-optimising agents
We can similarly define T̂ from π and D̂. It now follows that if D̂ is close to D,
in the sense that ε := maxxax′ |Dxax′ − D̂xax′ | is small, then for any stationary
policy π the associated matrices T and T̂ are close:




X

 ′



′
′
′
D
−
D̂
T
−
T̂
=
max
max
π(xa)


xax
xax 
xx
xx
xx′ 
xx′
a∈A



X
≤ max 
π(xa)ε = ε.
x

a∈A

This is important as it <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> that we can <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> bounds on the accuracy of our
estimate of the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> MDP and imply from this bounds on the accuracy of the
estimate T̂ for any stationary policy.
For any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> stationary policy this bound <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> carries over to the associated
expected <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> run average value functions in a straightforward way:
B.4.1 Lemma. For stationary finite MDPs such that ε := maxxax′ |Dxax′ −
D̂xax′ | it follows that for any stationary policy π,


 πµ
π µ̂ 
V1∞ − V1∞  = O(ε).

Proof. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> T and T̂ be the Markov chains defined by D and D̂ interacting
with a stationary policy π. By the argument above we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that maxxx′ |Txx′ −
T̂xx′ | ≤ ε. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Thus</span> by Theorem B.2.7 we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that there exists cT such that
∗
∗
maxxx′ |Txx
′ − T̂xx′ | ≤ cT ε, where cT depends on T . By Equation (B.5) we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span>
that,

 

 πµ


π µ̂ 
(B.9)
V1∞ − V1∞  = (T ∗ − T̂ ∗ )rπ  = O(ε).
2

From this lemma we can show that the optimal policies with respect to µ
and µ̂ are bounded:
B.4.2 Theorem. For a stationary finite MDP such that ε := maxxax′ |Dxax′ −
D̂xax′ | it follows that,
 µ

 π µ
π µ̂ µ̂ 
V1∞ − V1∞  = O(ε),
where π µ and π µ̂ are optimal policies that are <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> stationary.

Proof. For any <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> functions f, f ′ : D → R such that ∀x ∈ D : |f (x) −
f ′ (x)| ≤ δ it follows that | maxx∈D f (x) − maxx′ ∈D f ′ (x′ )| ≤ δ. From Lemma
B.4.1 it then follows that,



πµ
π ′ µ̂ 
V
max V1∞ − max
 = O(ε)
1∞
′
π

π

where π and π ′ belong to the <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of stationary policies. <span class="t27 t28 hoverable">However</span> by Theorem
B.3.3 we <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> that the optimal policies for µ and µ̂ can be chosen stationary
and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">thus</span> the result follows.
2

156

B.4. Convergence of expected average value
We now have all the necessary results to show that if µ̂ is a <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> estimate
of µ then our policy π µ̂ that is based on µ̂ will perform near optimally with
respect to the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> environment in the limit.
B.4.3 Theorem. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Let</span> µ and µ̂ be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">two</span> ergodic stationary finite MDP environments that are close in the sense that ε := maxxax′ |Dxax′ − D̂xax′ | is small.
It can be shown that for m ∈ N,
 

 µ̂
1
 π µ
πµ µ 
=
O
−
V
+ O(ε)
V1m
1m 
m

where π µ is an optimal policy for the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> distribution µ, and π µ̂ is an optimal
policy with respect to the estimate of the <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t19 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">true</span> distribution µ̂.
Proof.
From Theorem B.3.3 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that π µ̂ can be chosen stationary. From the
triangle inequality and the results of Corollary B.2.5 (with π
π µ̂ ), Lemma
µ̂
B.4.1 (with π
π ) and Theorem B.4.2 we <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">see</span> that,

 µ̂

 µ̂
 π µ
 π µ
π µ̂ µ
π µ̂ µ
π µ̂ µ̂
π µ̂ µ̂
πµ µ 
πµ µ 
V1m − V1∞  = V1m − V1∞ + V1∞ − V1∞ + V1∞ − V1∞ 
  µ̂
  µ̂

 µ̂
 π µ
 π µ
 π µ̂
π µ̂ µ 
π µ̂ µ̂ 
πµ µ 
− V1∞
≤ V1m
 + V1∞ − V1∞  + V1∞ − V1∞ 
 
1
= O
+ O(ε).
m
Now from Corollary B.2.5 (with π
π µ ) and the triangle inequality again,

 µ̂

 µ̂
 π µ
 π µ
πµ µ
πµ µ
πµ µ 
πµ µ 
V1m − V1m  = V1m − V1∞ + V1∞ − V1m 
 

  µ
 µ̂
1
 π µ
 π µ
πµ µ 
πµ µ 
=
O
−
V
+
− V1∞
≤ V1m
+ O(ε).
 V1∞
1m 
m

2

157

C. Definitions of <span class="t20 hoverable">Intelligence</span>
“Viewed narrowly, there seem to be almost as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> definitions of
<span class="t20 hoverable">intelligence</span> as there were experts asked to define it.” R. J. Sternberg quoted in (Gregory, 1998)
Despite a <span class="t1 t6 t21 t24 t26 t27 t29 hoverable">long</span> history of research and debate, there is still no standard
definition of intelligence. This has lead some to believe that <span class="t20 hoverable">intelligence</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">may</span>
be approximately described, but <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be fully defined. We believe that
this degree of pessimism is too strong. Although there is no single standard
definition, if <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> surveys the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> definitions that have been proposed, strong
similarities between <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> of the definitions quickly <span class="t8 t13 hoverable">become</span> obvious.
Here we <span class="t0 t1 t2 t3 t4 t5 t7 t8 t9 t10 t11 t13 t14 t15 t16 t17 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">take</span> the opportunity to present the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> informal definitions that
we have collected over the years. Naturally, compiling a complete list <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span>
be impossible as <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> definitions of <span class="t20 hoverable">intelligence</span> are buried deep inside articles
and books. Nevertheless, the 70 odd definitions presented below are, to the
best of our knowledge, the largest and most <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> referenced collection there is.

C.1. Collective definitions
In this section we present definitions that have been proposed by groups or
organisations. In <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> cases definitions of <span class="t20 hoverable">intelligence</span> <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> in encyclopedias
have been either contributed by an individual psychologist or quote an earlier
definition <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> by a psychologist. In these cases we have chosen to attribute
the quote to the psychologist, and have placed it in the next section. In this
section we only list those definitions that either <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be attributed to specific
individuals, or represent a collective definition agreed <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">upon</span> by <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> individuals. As <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> dictionaries source their definitions from other dictionaries, we
have endeavoured to always list the original source.
1. “The ability to use memory, knowledge, experience, understanding, reasoning, imagination and judgement in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to solve problems and adapt
to <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> situations.” AllWords Dictionary, 2006
2. “The capacity to acquire and apply knowledge.” The American Heritage
Dictionary, fourth edition, 2000
3. “Individuals differ from <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> <span class="t0 t1 t2 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t18 t19 t21 t23 t24 t25 t27 t28 t29 hoverable">another</span> in their ability to understand complex ideas, to adapt effectively to the environment, to learn from experience, to engage in various forms of reasoning, to overcome obstacles

159

C. Definitions of <span class="t20 hoverable">Intelligence</span>
by taking thought.” American Psychological Association (Neisser et al.,
1996)
4. “The ability to learn, understand and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> judgments or have opinions
that are based on reason” Cambridge Advance Learner’s Dictionary, 2006
5. “Intelligence is a very general mental capability that, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">among</span> other things,
involves the ability to reason, plan, solve problems, think abstractly,
comprehend complex ideas, learn quickly and learn from experience.”
Common statement with 52 expert signatories (Gottfredson, 1997a)
6. “The ability to learn facts and skills and apply them, especially when this
ability is highly developed.” Encarta <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">World</span> English Dictionary, 2006
7. “. . . ability to adapt effectively to the environment, either by making a
change in oneself or by changing the environment or finding a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span>
. . . <span class="t20 hoverable">intelligence</span> is not a single mental process, but rather a combination
of <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> mental processes directed toward effective adaptation to the
environment.” Encyclopedia Britannica, 2006
8. “the general mental ability involved in calculating, reasoning, perceiving
relationships and analogies, learning quickly, storing and retrieving information, using language fluently, classifying, generalizing, and adjusting
to <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> situations.” Columbia Encyclopedia, sixth edition, 2006
9. “Capacity for learning, reasoning, understanding, and similar forms of
mental activity; aptitude in grasping truths, relationships, facts, meanings, etc.” Random <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">House</span> Unabridged Dictionary, 2006
10. “The ability to learn, understand, and think about things.” Longman
Dictionary or Contemporary English, 2006
11. “: the ability to learn or understand or to deal with <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> or trying situations : . . . the skilled use of reason (2) : the ability to apply <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>
to manipulate one’s environment or to think abstractly as measured by
objective criteria (as tests)” Merriam-Webster Online Dictionary, 2006
12. “The ability to acquire and apply <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and skills.” Compact Oxford English Dictionary, 2006
13. “. . . the ability to adapt to the environment.” <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">World</span> <span class="t25 hoverable">Book</span> Encyclopedia,
2006
14. “Intelligence is a property of <span class="t0 t1 t2 t3 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t24 t25 t27 t28 t29 hoverable">mind</span> that encompasses <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> related mental
abilities, such as the capacities to reason, plan, solve problems, think
abstractly, comprehend ideas and language, and learn.” Wikipedia, 4
October, 2006

160

C.2. Psychologist definitions
15. “Capacity of mind, especially to understand principles, truths, facts or
meanings, acquire knowledge, and apply it to practise; the ability to
learn and comprehend.” Wiktionary, 4 October, 2006
16. “The ability to learn and understand or to deal with problems.” <span class="t0 t1 t2 t3 t4 t6 t7 t8 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t21 t22 t23 t24 t25 t26 t27 t28 hoverable">Word</span>
Central Student Dictionary, 2006
17. “The ability to comprehend; to understand and profit from experience.”
Wordnet 2.1, 2006
18. “The capacity to learn, reason, and understand.” Wordsmyth Dictionary, 2006

C.2. Psychologist definitions
This section contains definitions from psychologists. In some cases we have
not <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">yet</span> managed to locate the exact reference and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">would</span> appreciate any help
in doing so.
1. “Intelligence is not a single, unitary ability, but rather a composite of several functions. The term denotes that combination of abilities required
for survival and advancement <span class="t5 t7 hoverable">within</span> a particular culture.” Anastasi
(1992)
2. “. . . that facet of <span class="t0 t1 t2 t3 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t24 t25 t27 t28 t29 hoverable">mind</span> underlying our capacity to think, to solve novel
problems, to reason and to have <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> of the world.” Anderson
(2006)
3. “It seems to us that in <span class="t20 hoverable">intelligence</span> there is a fundamental faculty, the
alteration or the lack of which, is of the utmost importance for practical life. This faculty is judgement, otherwise <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">called</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> sense, practical sense, initiative, the faculty of adapting ones <span class="t0 t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">self</span> to circumstances.”
Binet and Simon (1905)
4. “We <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">shall</span> use the term ‘intelligence’ to mean the ability of an organism
to solve <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> problems . . . ” Bingham (1937)
5. “Intelligence is what is measured by <span class="t20 hoverable">intelligence</span> tests.” Boring (1923)
6. “. . . a quality that is intellectual and not emotional or moral: in measuring it we try to rule out the effects of the child’s zeal, interest, industry,
and the like. Secondly, it denotes a general capacity, a capacity that
enters into everything the child <span class="t1 t7 t9 t14 t15 t17 t25 t28 hoverable">says</span> or does or thinks; any want of ‘intelligence’ will <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">therefore</span> be revealed to some degree in almost all that he
attempts;” Burt (1957)

161

C. Definitions of <span class="t20 hoverable">Intelligence</span>
7. “A <span class="t9 hoverable">person</span> possesses <span class="t20 hoverable">intelligence</span> insofar as he has learned, or can learn,
to adjust himself to his environment.” S. S. Colvin quoted in (Sternberg,
2000)
8. “. . . the ability to plan and structure one’s behavior with an <span class="t7 t10 t15 t25 hoverable">end</span> in view.”
J. P. Das
9. “The capacity to learn or to profit by experience.” W. F. Dearborn
quoted in (Sternberg, 2000)
10. “. . . in its lowest terms <span class="t20 hoverable">intelligence</span> is present where the individual animal, or <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> being, is aware, <span class="t27 t28 hoverable">however</span> dimly, of the relevance of his
behaviour to an objective. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Many</span> definitions of what is indefinable have
been attempted by psychologists, of which the least unsatisfactory are 1.
the capacity to meet novel situations, or to learn to do so, by <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> adaptive responses and 2. the ability to perform tests or tasks, involving the
grasping of relationships, the degree of <span class="t20 hoverable">intelligence</span> being proportional to
the complexity, or the abstractness, or both, of the relationship.” Drever
(1952)
11. “Intelligence A: the biological substrate of mental ability, the brain’s
neuroanatomy and physiology; <span class="t20 hoverable">Intelligence</span> B: the manifestation of <span class="t20 hoverable">intelligence</span> A, and everything that influences its expression in real <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span>
behavior; <span class="t20 hoverable">Intelligence</span> C: the level of performance on psychometric tests
of cognitive ability.” H. J. Eysenck.
12. “Sensory capacity, capacity for perceptual recognition, quickness, range
or flexibility or association, facility and imagination, span of attention,
quickness or alertness in response.” F. N. Freeman quoted in (Sternberg,
2000)
13. “. . . adjustment or adaptation of the individual to his total environment,
or limited aspects thereof . . . the capacity to reorganize one’s behavior
patterns so as to act more effectively and more appropriately in novel
situations . . . the ability to learn . . . the extent to which a <span class="t9 hoverable">person</span> is educable . . . the ability to carry on abstract thinking . . . the effective use of
concepts and symbols in dealing with a problem to be solved . . . ” W.
Freeman
14. “An <span class="t20 hoverable">intelligence</span> is the ability to solve problems, or to create products,
that are valued <span class="t5 t7 hoverable">within</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> or more cultural settings.” Gardner (1993)
15. “. . . performing an operation on a specific type of content to produce a
particular product.” J. P. Guilford
16. “Sensation, perception, association, memory, imagination, discrimination, judgement and reasoning.” N. E. Haggerty quoted in (Sternberg,
2000)

162

C.2. Psychologist definitions
17. “The capacity for knowledge, and <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> possessed.” Henmon (1921)
18. “. . . cognitive ability.” Herrnstein and Murray (1996)
19. “. . . the resultant of the process of acquiring, storing in memory, retrieving, combining, comparing, and using in <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> contexts information and
conceptual skills.” Humphreys
20. “Intelligence is the ability to learn, exercise judgment, and be imaginative.” J. Huarte
21. “Intelligence is a general factor that runs through all types of performance.” A. Jensen
22. “Intelligence is assimilation to the extent that it incorporates all the <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span>
data of experience <span class="t5 t7 hoverable">within</span> its framework . . . There can be no doubt either,
that mental <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> is <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">also</span> accommodation to the environment. Assimilation
can <span class="t0 t10 t11 t14 t18 hoverable">never</span> be pure because by incorporating <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> elements into its earlier
schemata the <span class="t20 hoverable">intelligence</span> constantly modifies the latter in <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> to adjust
them to <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> elements.” Piaget (1963)
23. “Ability to adapt oneself adequately to relatively <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> situations in life.”
R. Pinter quoted in (Sternberg, 2000)
24. “A biological mechanism by which the effects of a complexity of stimuli
are <span class="t2 t4 t5 t22 t23 hoverable">brought</span> <span class="t16 t21 hoverable">together</span> and <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> a somewhat unified effect in behavior.”
J. Peterson quoted in (Sternberg, 2000)
25. “. . . <span class="t3 hoverable">certain</span> <span class="t1 t2 t4 t7 t15 t17 t18 t19 hoverable">set</span> of cognitive capacities that enable an individual to adapt
and thrive in any <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> environment they find themselves in, and those
cognitive capacities include <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">things</span> <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">like</span> memory and retrieval, and problem solving and so forth. There’s a cluster of cognitive abilities that lead
to successful adaptation to a wide range of environments.” Simonton
(2003)
26. “Intelligence is <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the internal environment that shows through at
the interface between <span class="t9 hoverable">person</span> and external environment as a function of
cognitive task demands.” R. E. Snow quoted in (Slatter, 2001)
27. “. . . I prefer to refer to it as ‘successful intelligence.’ And the reason is
that the emphasis is on the use of your <span class="t20 hoverable">intelligence</span> to achieve success in
your life. So I define it as your skill in achieving whatever it is you want
to attain in your <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">life</span> <span class="t5 t7 hoverable">within</span> your sociocultural context — <span class="t6 t13 t14 t25 t27 hoverable">meaning</span> that
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">people</span> have <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> goals for themselves, and for some it’s to get very
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> grades in school and to do <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> on tests, and for <span class="t1 t7 t13 hoverable">others</span> it <span class="t1 t2 t5 t8 t10 t11 t12 t13 t14 t15 t18 t21 t22 t23 t25 t26 t27 t28 t29 hoverable">might</span> be to
<span class="t8 t13 hoverable">become</span> a very <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">good</span> basketball player or actress or musician.” Sternberg
(2003)

163

C. Definitions of <span class="t20 hoverable">Intelligence</span>
28. “. . . the ability to undertake activities that are characterized by (1) difficulty, (2) complexity, (3) abstractness, (4) economy, (5) adaptedness to
goal, (6) social value, and (7) the emergence of originals, and to maintain
such activities under conditions that demand a concentration of energy
and a resistance to emotional forces.” Stoddard
29. “The ability to carry on abstract thinking.”
in (Sternberg, 2000)

L. M. Terman quoted

30. “Intelligence, considered as a mental trait, is the capacity to <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t27 t28 t29 hoverable">make</span> impulses focal at their early, unfinished stage of formation. <span class="t20 hoverable">Intelligence</span> is
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">therefore</span> the capacity for abstraction, which is an inhibitory process.”
Thurstone (1924)
31. “The capacity to inhibit an instinctive adjustment, the capacity to redefine the inhibited instinctive adjustment in the <span class="t1 t2 t3 t4 t6 t7 t8 t9 t10 t11 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t26 t27 t28 t29 hoverable">light</span> of imaginally experienced trial and error, and the capacity to realise the modified instinctive
adjustment in overt behavior to the advantage of the individual as a
social animal.” L. L. Thurstone quoted in (Sternberg, 2000)
32. “A global concept that involves an individual’s ability to act purposefully,
think rationally, and deal effectively with the environment.” Wechsler
(1958)
33. “The capacity to acquire capacity.” H. Woodrow quoted in (Sternberg,
2000)
34. “. . . the term <span class="t20 hoverable">intelligence</span> designates a complexly interrelated assemblage
of functions, no <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">one</span> of which is completely or accurately <span class="t6 t10 t15 hoverable">known</span> in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">man</span>
. . . ” Yerkes and Yerkes (1929)
35. “. . . that faculty of <span class="t0 t1 t2 t3 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t24 t25 t27 t28 t29 hoverable">mind</span> by which <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> is perceived in a situation previously considered disordered.” R. W. Young quoted in (Kurzweil, 2000)

C.3. AI researcher definitions
This section lists definitions from researchers in artificial intelligence.
1. “. . . the ability of a system to act appropriately in an uncertain environment, where appropriate action is that which increases the probability
of success, and success is the achievement of behavioral subgoals that
support the system’s ultimate goal.” Albus (1991)
2. “Any system . . . that generates adaptive behviour to meet goals in a
range of environments can be <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">said</span> to be intelligent.” Fogel (1995)
3. “Achieving complex goals in complex environments.” Goertzel (2006)

164

C.3. AI researcher definitions
4. “Intelligent systems are expected to work, and <span class="t0 t2 t3 t5 t7 t10 t12 t13 t14 t16 t17 t19 t21 t24 t25 t26 t28 t29 hoverable">work</span> well, in <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> <span class="t1 t2 t18 t25 t26 t28 t29 hoverable">different</span> environments. Their property of <span class="t20 hoverable">intelligence</span> allows them to maximize the probability of success <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">even</span> if full <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> of the situation
is not available. Functioning of intelligent systems <span class="t1 t3 t4 t5 t6 t7 t8 t9 t10 t12 t13 t14 t15 t16 t19 t20 t21 t23 t24 t25 t26 t28 t29 hoverable">cannot</span> be considered
separately from the environment and the concrete situation including
the goal.” Gudwin (2000)
5. “[Performance <span class="t20 hoverable">intelligence</span> is] the successful (i.e., goal-achieving) performance of the system in a complicated environment.” Horst (2002)
6. “Intelligence is the ability to use optimally limited resources – including
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">time</span> – to achieve goals.” Kurzweil (2000)
7. “Intelligence is the <span class="t0 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">power</span> to rapidly find an adequate solution in what
appears a priori (to observers) to be an immense search space.” Lenat
and Feigenbaum (1991)
8. “Intelligence measures an agent’s ability to achieve goals in a wide range
of environments.” Legg and Hutter (2006)
9. “. . . doing <span class="t1 t2 t4 t6 t8 t10 t12 t13 t14 t17 t19 t23 t25 t28 hoverable">well</span> at a broad range of tasks is an empirical definition of
‘intelligence’ ” Masum et al. (2002)
10. “Intelligence is the computational <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">part</span> of the ability to achieve goals in
the world. Varying kinds and degrees of <span class="t20 hoverable">intelligence</span> occur in people,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">many</span> animals and some machines.” McCarthy (2004)
11. “. . . the ability to solve hard problems.” Minsky (1985)
12. “Intelligence is the ability to process information properly in a complex
environment. The criteria of properness are not predefined and hence not
available beforehand. They are acquired as a result of the information
processing.” Nakashima (1999)
13. “. . . in any real situation behavior appropriate to the ends of the system
and adaptive to the demands of the environment can occur, <span class="t5 t7 hoverable">within</span> some
limits of speed and complexity.” Newell and Simon (1976)
14. “[An intelligent agent does what] is appropriate for its circumstances
and its goal, it is flexible to changing environments and changing goals,
it learns from experience, and it makes appropriate choices <span class="t1 t2 t4 t7 t10 t11 t12 t13 t14 t16 t17 t18 t19 t20 t21 t22 t23 t24 t26 t27 t29 hoverable">given</span> perceptual limitations and finite computation.” Poole et al. (1998)
15. “Intelligence <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span> getting better over time.” Schank (1991)
16. “. . . the essential, domain-independent skills necessary for acquiring a
wide range of domain-specific <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> – the ability to learn anything.
Achieving this with ‘artificial general intelligence’ (AGI) requires a highly

165

C. Definitions of <span class="t20 hoverable">Intelligence</span>
adaptive, general-purpose system that can autonomously acquire an extremely wide range of specific <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and skills and can improve its
own cognitive ability through self-directed learning.” (Voss, 2005)
17. “Intelligence is the ability for an information processing system to adapt
to its environment with insufficient <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span> and resources.” Wang
(1995)
18. “. . . the mental ability to sustain successful life.” K. Warwick quoted
in (Asohan, 2003)

166

Bibliography
Abeles, M. (1991). Corticonics: Neural circuits of the cerebral cortex. Cambridge University Press.
Albus, J. S. (1991). Outline for a theory of intelligence. IEEE Trans. Systems,
<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Man</span> and Cybernetics, 21, 473–509.
Alvarado, N., Adams, S., Burbeck, S., and Latta, C. (2002). Beyond the
Turing test: Performance metrics for evaluating a computer simulation of
the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> mind. Performance Metrics for Intelligent Systems Workshop.
Gaithersburg, MD, USA: North-Holland.
Ananthanarayanan, R., and Modha, D. S. (2007). Anatomy of a cortical simulator. ACM/IEEE SC2007 Conference on High Performance Networking
and Computing. Reno, NV, USA.
Anastasi, A. (1992). What counselors should <span class="t0 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 hoverable">know</span> about the use and interpretation of psychological tests. Journal of Counseling and Development,
70, 610–615.
Anderson, M. (2006). Intelligence. MS Encarta online encyclopedia.
Asohan, A. (2003). Leading humanity forward. The Star.
Barzdin, J. M. (1972). Prognostication of automata and functions. Information
Processing, 71, 81–84.
Bell, T. C., Cleary, J. G., and Witten, I. H. (1990). <span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">Text</span> compression. Prentice
Hall.
Bellman, R. (1957). Dynamic programming. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> Jersey: Princeton University
Press.
Berry, D. A., and Fristedt, B. (1985). Bandit problems: Sequential allocation
of experiments. London: Chapman and Hall.
Bertsekas, D. P. (1995). Dynamic programming and optimal control, vol. (ii).
Belmont, Massachusetts: Athena Scientific.
Binet, A. (1911). Les idees modernes sur les enfants. Paris: Flammarion.
Binet, A., and Simon, T. (1905). Methodes nouvelles por le diagnostic du
niveai intellectuel des anormaux. L’Année Psychologique, 11, 191–244.

167

BIBLIOGRAPHY
Bingham, W. V. (1937). Aptitudes and aptitude testing. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Harper &
Brothers.
Block, N. (1981). Psychologism and behaviorism. Philosophical Review, 90,
5–43.
Boring, E. G. (1923). <span class="t20 hoverable">Intelligence</span> as the tests test it. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> Republic, 35, 35–37.
Boyan, J. A. (1999). Least-squares temporal difference learning. Proc. 16th
International Conf. on Machine Learning (pp. 49–56). Morgan Kaufmann,
San Francisco, CA.
Bradtke, S. J., and Barto, A. G. (1996). Linear least-squares algorithms for
temporal difference learning. Machine Learning, 22, 33–57.
Bringsjord, S., and Schimanski, B. (2003). What is artificial intelligence?
Psychometric AI as an answer. Eighteenth International Joint Conference
on Artificial Intelligence, 18, 887–893.
Burt, C. L. (1957). The causes and treatments of backwardness. University of
London press.
Calude, C. S. (2002). Information and randomness. Berlin: Springer. 2nd
edition.
Calude, C. S., Jürgensen, H., and Legg, S. (2000). Solving finitely refutable
mathematical problems. In C. S. Calude and G. Puaun (Eds.), Finite versus infinite: Contributions to an eternal dilemma, 39–52. Springer-Verlag,
London.
Carroll, J. B. (1993). <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">Human</span> cognitive abilities: A survey of factor-analytic
studies. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Cambridge University Press.
Cattell, R. B. (1987). Intelligence: Its structure, growth, and action. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span>
York: Elsevier.
Chaitin, G. J. (1982). Gödel’s theorem and information. International Journal
of Theoretical Physics, 22, 941–954.
Cilibrasi, R., and Vitányi, P. M. B. (2005). Clustering by compression. IEEE
Trans. Information Theory, 51, 1523–1545.
Cleary, J. G., Legg, S., and Witten, I. H. (1996). An MDL estimate of the
significance of rules. Information, Statistics and Induction in Science (pp.
43–45).
Creutzfeldt, O. D. (1977). Generality of the functional structure of the neocortex. Naturwissenschaften, 64, 507–517.

168

BIBLIOGRAPHY
Crevier, D. (1993). AI: The tumultuous search for artificial intelligence. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span>
York: BasicBooks.
Dawid, A. P. (1985). Comment on The impossibility of inductive inference.
Journal of the American Statistical Association, 80, 340–341.
Dawkins, R. (2006). The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">god</span> delusion. Bantam Books.
Dayan, P. (1992). The convergence of TD(λ) for general λ. Machine learning,
8, 341–362.
Doob, J. L. (1953). Stochastic processes. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: John Wiley & Sons.
Dowe, D. L., and Hajek, A. R. (1998). A non-behavioural, computational
extension to the Turing test. International Conference on Computational
<span class="t20 hoverable">Intelligence</span> & Multimedia Applications (ICCIMA ’98) (pp. 101–106). Gippsland, Australia.
Drever, J. (1952). A dictionary of psychology. Harmondsworth: Penguin
Books.
Edmonds, B. (2006). The social embedding of <span class="t20 hoverable">intelligence</span> – Towards producing a machine that <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t16 t18 t19 t20 t21 t22 t23 t24 t25 t26 t28 t29 hoverable">could</span> <span class="t5 hoverable">pass</span> the turing test. In The turing test sourcebook:
Philosophical and methodological issues in the quest for the thinking computer. Kluwer.
Eisner, J. (1991). Cognitive science and the search for intelligence. Invited
paper presented to the Socratic Society, University of Cape Town.
Feder, M., Merhav, N., and Gutman, M. (1992). Universal prediction of individual sequences. IEEE Trans. on Information Theory, 38, 1258–1270.
Fiévet, C. (2005). Mesurer l’intelligence d’une machine.
l’intelligence (pp. 42–45). Paris: Mondeo publishing.

Le Monde de

Fogel, D. B. (1995). Review of computational intelligence: Imitating life. Proc.
of the IEEE, 83.
Ford, K. M., and Hayes, P. J. (1998). On computational wings: Rethinking
the goals of artificial intelligence. Scientific American, Special Edition.
French, R. M. (1990). Subcognition and the limits of the Turing test. Mind,
99, 53–65.
Fuster, J. M. (2003). Cortex and mind: Unifying cognition. Oxford University
Press.
Gardner, H. (1993). Frames of mind: Theory of multiple intelligences. Fontana
Press.

169

BIBLIOGRAPHY
Gittins, J. C. (1989). Multi-armed bandit allocation indices. John Wiley &
Sons.
Gödel, K. (1931). Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I. Monatshefte für Matematik und Physik, 38,
173–198. [English translation by E. Mendelsohn: “On undecidable propositions of formal mathematical systems”. In M. Davis, editor, The undecidable,
pages 39–71, <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York, 1965. Raven Press, Hewlitt].
Goertzel, B. (2006). The hidden pattern. Brown Walker Press.
Gold, E. M. (1967). Language identification in the limit. Information and
Control, 10, 447–474.
Goldberg, D. E., and Richardson, J. (1987). Genetic algorithms with sharing
for multi-modal function optimization. Proc. 2nd International Conference
on Genetic Algorithms and their Applications (pp. 41–49). Cambridge, MA:
Lawrence Erlbaum Associates.
Good, I. J. (1965). Speculations concerning the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> ultraintelligent machine.
Advances in Computers, 6, 31–88.
Gottfredson, L. S. (1997a). Mainstream science on intelligence: An editorial
with 52 signatories, history, and bibliography. Intelligence, 24, 13–23.
Gottfredson, L. S. (1997b). Why g matters: The complexity of everyday life.
Intelligence, 24, 79–132.
Gottfredson, L. S. (2002). g: Highly general and highly practical. The general
factor of intelligence: How general is it? (pp. 331–380). Erlbaum.
Gould, S. J. (1981). The mismeasure of man. W. W. Norton & Company.
Graham-Rowe, D. (2005). Spotting the bots with brains. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> Scientist magazine, 2512, 27.
Graham-Rowe, D. (2007). A working brain model. Technology Review. 28
November.
Gregory, R. L. (1998). The Oxford companion to the mind. Oxford, UK:
Oxford University Press.
Gudwin, R. R. (2000). Evaluating intelligence: A computational semiotics
perspective. IEEE International conference on systems, <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">man</span> and cybernetics
(pp. 2080–2085). Nashville, Tenessee, USA.
Guilford, J. P. (1967). The <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: McGrawHill.

170

BIBLIOGRAPHY
Gunderson, K. (1971). Mentality and machines. Garden City, <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York, USA:
Doubleday and company.
Harnad, S. (1989). Minds, machines and Searle. Journal of Theoretical and
Experimental Artificial Intelligence, 1, 5–25.
Havenstein, H. (2005). Spring comes to AI winter. Computer World. 14
February.
Hawkins, J., and Blakeslee, S. (2004). On intelligence. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Owl books.
Henmon, V. A. C. (1921). The measurement of intelligence. School and Society,
13, 151–158.
Herman, L. M., and Pack, A. A. (1994). Animal intelligence: Historical perspectives and contemporary approaches. In R. Sternberg (Ed.), Encyclopedia
of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence, 86–96. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Macmillan.
Hernández-Orallo, J. (2000a). Beyond the Turing test. Journal of Logic,
Language and Information, 9, 447–466.
Hernández-Orallo, J. (2000b). On the computational measurement of <span class="t20 hoverable">intelligence</span> factors. Performance Metrics for Intelligent Systems Workshop (pp.
1–8). Gaithersburg, MD, USA.
Hernández-Orallo, J., and Minaya-Collado, N. (1998). A formal definition
of <span class="t20 hoverable">intelligence</span> based on an intensional variant of Kolmogorov complexity.
Proceedings of the International Symposium of Engineering of Intelligent
Systems (EIS’98) (pp. 146–163). ICSC Press.
Herrnstein, R. J., and Murray, C. (1996). The bell curve: <span class="t20 hoverable">Intelligence</span> and
class structure in American life. Free Press.
Horn, J. (1970). Organization of data on life-span development of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span>
abilities. Life-span developmental psychology: Research and theory. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span>
York: Academic Press.
Horst, J. (2002). A native <span class="t20 hoverable">intelligence</span> metric for artificial systems. Performance Metrics for Intelligent Systems Workshop. Gaithersburg, MD, USA.
Hsu, F. H., Campbell, M. S., and Hoane, A. J. (1995). Deep Blue system
overview. Proceedings of the 1995 International Conference on Supercomputing (pp. 240–244).
Hutchens, J. L. (1996).
How to <span class="t5 hoverable">pass</span> the Turing test by cheating.
www.cs.umbc.edu/471/current/papers/hutchens.pdf.
Hutter, M. (2001). Convergence and error bounds for universal prediction of
nonbinary sequences. Proc. 12th Eurpean Conference on Machine Learning
(ECML-2001), 239–250.

171

BIBLIOGRAPHY
Hutter, M. (2002a). The fastest and shortest algorithm for all well-defined
problems. International Journal of Foundations of Computer Science, 13,
431–443.
Hutter, M. (2002b). Fitness uniform selection to preserve genetic diversity.
Proc. 2002 Congress on Evolutionary Computation (CEC-2002) (pp. 783–
788). Washington D.C, USA: IEEE.
Hutter, M. (2005).
Universal artificial intelligence: Sequential decisions based on algorithmic probability. Berlin: Springer. 300 pages,
http://www.hutter1.net/ai/uaibook.htm.
Hutter, M. (2006).
The
http://prize.hutter1.net.

<span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">Human</span>

<span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">knowledge</span>

compression

prize.

Hutter, M. (2007a). On universal prediction and Bayesian confirmation. Theoretical Computer Science, 384, 33–48.
Hutter, M. (2007b). Universal algorithmic intelligence: A mathematical
top→down approach. In Artificial general intelligence, 227–290. Berlin:
Springer.
Hutter, M., and Legg, S. (2006). Fitness uniform optimization. IEEE Transactions on Evolutionary Computation, 10, 568–589.
Hutter, M., and Legg, S. (2007). Temporal difference updating <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">without</span> a
learning rate. Neural Information Processing Systems (NIPS ’07).
Hutter, M., Legg, S., and Vitányi, P. M. B. (2007). Algorithmic probability.
Scholarpedia.
Johnson, W. L. (1992). Needed: A <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> test of intelligence. SIGARTN:
SIGART Newsletter (ACM Special Interest Group on Artificial Intelligence),
3.
Johnson-Laird, P. N., and Wason, P. C. (1977). A theoretical analysis of insight
into a reasoning task. In P. N. Johnson-Laird and P. C. Wason (Eds.),
Thinking: Readings in cognitive science, 143–157. Cambridge University
Press.
Jong, K. (1975). An analysis of the behavior of a class of genetic adaptive
systems. Dissertation Abstracts International, 36.
Kandel, E. R., Schwartz, J. H., and Jessell, T. M. (2000). Principles of neural
science. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: McGraw-Hill. 4nd edition.
Kaufman, A. S. (2000). Tests of intelligence. In R. J. Sternberg (Ed.), Handbook of intelligence. Cambridge University Press.

172

BIBLIOGRAPHY
Koch, C. (1999). Biophysics of computation: Information processing in single
neurons. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Oxford University Press.
Koza, J. R., Keane, M. A., Streeter, M. J., Mydlowec, W., Yu, J., and Lanza,
G. (2003). Genetic programming IV: Routine human-competitive machine
intelligence. Kluwer Academic.
Kurzweil, R. (2000). The age of <span class="t7 t26 t27 t28 hoverable">spiritual</span> machines: When computers exceed
<span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence. Penguin.
Lagoudakis, M. G., and Parr, R. (2003). Least-squares policy iteration. Journal
of Machine Learning Research, 4, 1107–1149.
Legg, S. (1996). Minimum information estimation of linear regression models.
Information, Statistics and Induction in Science (pp. 103–111).
Legg, S. (1997). Solomonoff induction (Technical Report CDMTCS-030). Centre of Discrete Mathematics and Theoretical Computer Science, University
of Auckland.
Legg, S. (2006a). Incompleteness and artificial intelligence. Collegium Logicum
of the Kurt Gödel Society. Vienna.
Legg, S. (2006b). Is there an elegant universal theory of prediction? Proc.
Algorithmic Learning Theory (ALT 06). Barcelona, Spain.
Legg, S., and Hutter, M. (2004). A taxonomy for abstract environments (Technical Report IDSIA-20-04). IDSIA.
Legg, S., and Hutter, M. (2005a). Fitness uniform deletion for robust optimization. Proc. Genetic and Evolutionary Computation Conference (GECCO’05)
(pp. 1271–1278). Washington, OR: ACM SigEvo.
Legg, S., and Hutter, M. (2005b). A universal measure of <span class="t20 hoverable">intelligence</span> for artificial agents. Proc. 21st International Joint Conf. on Artificial <span class="t20 hoverable">Intelligence</span>
(IJCAI-2005) (pp. 1509–1510). Edinburgh.
Legg, S., and Hutter, M. (2006). A formal measure of machine intelligence. Annual Machine Learning Conference of Belgium and The Netherlands (Benelearn’06) (pp. 73–80). Ghent.
Legg, S., and Hutter, M. (2007a). A collection of definitions of intelligence.
Advances in Artificial General Intelligence: Concepts, Architectures and
Algorithms (pp. 17–24). Amsterdam, NL: IOS Press. Online version:
www.vetta.org/shane/intelligence.html.
Legg, S., and Hutter, M. (2007b). Tests of machine intelligence. 50 <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">Years</span> of
Artificial <span class="t20 hoverable">Intelligence</span> (pp. 232–242). Monte Verità, Switzerland.

173

BIBLIOGRAPHY
Legg, S., and Hutter, M. (2007c). Universal intelligence: A definition of machine intelligence. Minds and Machines, 17, 391–444.
Legg, S., Hutter, M., and Kumar, A. (2004). Tournament versus fitness uniform selection. Proc. 2004 Congress on Evolutionary Computation (CEC’04)
(pp. 2144–2151). Portland, OR: IEEE.
Legg, S., Poland, J., and Zeugmann, T. (2008). On the limits of learning with
computational models. <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">Knowledge</span> Media Science. To appear.
Lenat, D., and Feigenbaum, E. (1991). On the thresholds of knowledge. Artificial Intelligence, 47, 185–250.
Levin, L. A. (1973). Universal sequential search problems. Problems of Information Transmission, 9, 265–266.
Levin, L. A. (1974). Laws of information conservation (non-growth) and
aspects of the foundation of probability theory. Problems of Information
Transmission, 10, 206–210.
Li, M., and Vitányi, P. M. B. (1997). An introduction to Kolmogorov complexity and its applications. Springer. 2nd edition.
Loebner, H. G. (1990). The Loebner prize — The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">first</span> Turing test.
http://www.loebner.net/Prizef/loebner-prize.html.
Macphail, E. M. (1985). Vertebrate intelligence: The null hypothesis. In
L. Weiskrantz (Ed.), Animal intelligence, 37–50. Oxford: Clarendon.
Mahoney, M. V. (1999). <span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">Text</span> compression as a test for artificial intelligence.
AAAI/IAAI (p. 970).
Markoff, J., and Hansell, S. (2005). Hiding in plain sight, google seeks more
power. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York Times. 14 June.
Martin-Löf, P. (1966). The definition of random sequences. Information and
Control, 9, 602–619.
Masum, H., Christensen, S., and Oppacher, F. (2002). The Turing ratio:
Metrics for open-ended tasks. GECCO 2002: Proceedings of the Genetic and
Evolutionary Computation Conference (pp. 973–980). <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Morgan
Kaufmann Publishers.
McCarthy,
J.
(2004).
What
is
artificial
www-formal.stanford.edu/jmc/whatisai/whatisai.html.

intelligence?

Melchner, L., Pallas, S. L., and Sur, M. (2000). Visual behaviour mediated by
retinal projections directed to the auditory pathway. Nature, 404, 871–876.
Minsky, M. (1985). The society of mind. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Simon and Schuster.

174

BIBLIOGRAPHY
Moravec, H. (1998). When will computer hardware match the <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> brain?
Journal of Transhumanism, 1.
Mountcastle, V. B. (1978). An organizing principle for cerebral function: The
unit model and the distributed system. In The mindful brain. MIT Press.
Müller, M. (2006). Stationary algorithmic probability (Technical Report). TU
Berlin, Berlin. http://arXiv.org/abs/cs/0608095.
Nakashima, H. (1999). AI as complex information processing. Minds and
machines, 9, 57–80.
Neisser, U., Boodoo, G., Bouchard, Jr., T. J., Boykin, A. W., Brody, N., Ceci,
S. J., Halpern, D. F., Loehlin, J. C., Perloff, R., Sternberg, R. J., and Urbina,
S. (1996). Intelligence: Knowns and unknowns. American Psychologist, 51,
77–101.
Newell, A., and Simon, H. A. (1976). Computer science as empirical enquiry:
Symbols and search. Communications of the ACM 19, 3, 113–126.
Peng, J. (1993). Efficient dynamic programming-based learning for control.
Doctoral dissertation, Northeastern University, Boston, MA.
Peng, J., and Williams, R. J. (1996). Increamental multi-step Q-learning.
Machine Learning, 22, 283–290.
Piaget, J. (1963). The psychology of intelligence. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Routledge.
Poland, J., and Hutter, M. (2004). Convergence of discrete MDL for sequential
prediction. Proc. 17th Annual Conf. on Learning Theory (COLT’04) (pp.
300–314). Banff: Springer, Berlin.
Poland, J., and Hutter, M. (2006). Universal learning of repeated matrix
games. Proc. 15th Annual Machine Learning Conf. of Belgium and The
Netherlands (Benelearn’06) (pp. 7–14). Ghent.
Poole, D., Mackworth, A., and Goebel, R. (1998). Computational intelligence:
A logical approach. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York, NY, USA: Oxford University Press.
Raven, J. (2000). The Raven’s progressive matrices: Change and stability over
culture and time. Cognitive Psychology, 41, 1–48.
Reznikova, Z. I. (2007). Animal intelligence. Cambridge University Press.
Reznikova, Z. I., and Ryabko, B. (1986). Analysis of the language of ants by
information-theoretic methods. Problems Inform. Transmission, 22, 245–
249.
Rissanen, J. J. (1996). Fisher Information and Stochastic Complexity. IEEE
Trans. on Information Theory, 42, 40–47.

175

BIBLIOGRAPHY
Rummery, G. A. (1995). Problem solving with reinforcement learning. Doctoral
dissertation, Cambridge University.
Rummery, G. A., and Niranjan, M. (1994). On-line Q-learning using connectionist systems. Technial Report CUED/F-INFENG/TR 166). Engineering
Department, Cambridge University.
Russell, S. J., and Norvig, P. (1995). Artificial intelligence. A modern approach. Englewood Cliffs: Prentice-Hall.
Sanghi, P., and Dowe, D. L. (2003). A computer program capable of passing
I.Q. tests. Proc. 4th ICCS International Conference on Cognitive Science
(ICCS’03) (pp. 570–575). Sydney, NSW, Australia.
Saygin, A., Cicekli, I., and Akman, V. (2000). Turing test: 50 <span class="t0 t1 t2 t4 t5 t6 t7 t8 t10 t12 t13 t15 t16 t17 t20 t22 t23 t28 hoverable">years</span> later.
Minds and Machines, 10.
Schaeffer, J., Burch, N., Björnsson, Y., Kishimoto, A., Müller, M., Lake, R.,
Lu, P., and Sutphen, S. (2007). Checkers is solved. Science. 19 July.
Schank, R. (1991). Where’s the AI? AI magazine, 12, 38–49.
Schmidhuber, J. (2002). The Speed Prior: a <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">new</span> simplicity measure yielding near-optimal computable predictions. Proc. 15th Annual Conference
on Computational Learning Theory (COLT 2002) (pp. 216–228). Sydney,
Australia: Springer.
Schmidhuber, J. (2004). Optimal ordered problem solver. Machine Learning,
54, 211–254.
Schmidhuber, J. (2005). Gödel machines: Self-referential universal problem
solvers making provably optimal self-improvements. In Artificial general
intelligence. Springer, in press.
Schweizer, P. (1998). The truly total Turing test. Minds and Machines, 8,
263–272.
Searle, J. (1980). Minds, brains, and programs. Behavioral & Brain Sciences,
3, 417–458.
Shieber, S. (1994). Lessons from a restricted Turing test. CACM: Communications of the ACM, 37.
Simonton, D. K. (2003). An interview with Dr. Simonton. In J. A. Plucker
(Ed.), <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">Human</span> intelligence: Historical influences, current controversies,
teaching resources. http://www.indiana.edu/∼ intell.
Singer, E. (2007). A wiring diagram of the brain. Technology Review. 19
November.

176

BIBLIOGRAPHY
Slatter, J. (2001). Assessment of children: Cognitive applications. San Diego:
Jermone M. Satler Publisher Inc. 4th edition.
Slotnick, B. M., and Katz, H. M. (1974). Olfactory learning-set formation in
rats. Science, 185, 796–798.
Smith, T. C., Witten, I. H., Cleary, J. G., and Legg, S. (1994). Objective
evaluation of inferred context-free grammars. Australian and <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> Zealand
Conference on Intelligent Information Systems (pp. 393–396).
Smith, W. D. (2006). Mathematical definition of “intelligence” (and consequences). http://math.temple.edu/∼ wds/homepage/works.html.
Solomonoff, R. J. (1964). A formal theory of inductive inference: <span class="t0 t1 t2 t3 t4 t5 t6 t8 t9 t10 t11 t12 t13 t14 t15 t17 t18 t20 t21 t23 t24 t25 t26 t27 t28 t29 hoverable">Part</span> 1 and
2. Inform. Control, 7, 1–22, 224–254.
Solomonoff, R. J. (1978). Complexity-based induction systems: comparisons
and convergence theorems. IEEE Trans. Information Theory, IT-24, 422–
432.
Spearman, C. E. (1927). The abilities of man, their <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> and measurement.
<span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Macmillan.
Stern, W. L. (1912).
Leipzig: Barth.

Psychologischen Methoden der Intelligenz-Prüfung.

Sternberg, R. J. (1985). Beyond IQ: A triarchic theory of <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">human</span> intelligence.
<span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> York: Cambridge University Press.
Sternberg, R. J. (Ed.). (2000). Handbook of intelligence. Cambridge University
Press.
Sternberg, R. J. (2003). An interview with Dr. Sternberg. In J. A. Plucker
(Ed.), <span class="t1 t2 t4 t6 t7 t11 t12 t13 t14 t18 t19 t20 t23 t24 t25 t27 t28 t29 hoverable">Human</span> intelligence: Historical influences, current controversies,
teaching resources. http://www.indiana.edu/∼ intell.
Sternberg, R. J., and Berg, C. A. (1986). Quantitative integration: Definitions
of intelligence: A comparison of the 1921 and 1986 symposia. What is
intelligence? Contemporary wiewpoints on its <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> and definition (pp.
155–162). Norwood, NJ: Ablex.
Sternberg, R. J., and Grigorenko, E. L. (Eds.). (2002). Dynamic testing: The
<span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> and measurement of learning potential. Cambridge University Press.
Sutton, R., and Barto, A. (1998). Reinforcement learning: An introduction.
Cambridge, MA, MIT Press.
Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. Machine Learning, 3, 9–44.

177

BIBLIOGRAPHY
Terman, L. M., and Merrill, M. A. (1950). The Stanford-Binet <span class="t20 hoverable">intelligence</span>
scale. Boston: Houghton Mifflin.
Tesauro, G. (1995). Temporal difference learning and TD-Gammon. Communications of the ACM, 39, 58–68.
Thurstone, L. L. (1924). The <span class="t0 t1 t2 t3 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">nature</span> of intelligence. London: Routledge.
Thurstone, L. L. (1938). Primary mental abilities. Chicago: University of
Chicago Press.
Treister-Goren, A., Dunietz, J., and Hutchens, J. L. (2000). The developmental approach to evaluating artificial <span class="t20 hoverable">intelligence</span> – a proposal. Performance
Metrics for <span class="t20 hoverable">Intelligence</span> Systems.
Treister-Goren, A., and Hutchens, J. L. (2001). Creating AI: A unique interplay between the development of learning algorithms and their education.
Proceeding of the <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">First</span> International Workshop on Epigenetic Robotics.
Turing, A. M. (1950). Computing machinery and intelligence. Mind.
Voss, P. (2005). Essentials of general intelligence: The direct path to AGI.
Artificial General Intelligence. Springer-Verlag.
V’yugin, V. V. (1998). Non-stochastic infinite and finite sequences. Theoretical
computer science, 207, 363–382.
Wallace, C. S. (2005). Statistical and inductive inference by minimum message
length. Berlin: Springer.
Wallace, C. S., and Boulton, D. M. (1968). An information measure for classification. Computer Jrnl., 11, 185–194.
Wang, P. (1995). On the working definition of <span class="t20 hoverable">intelligence</span> (Technical Report 94). Center for Research on Concepts and Cognition, Indiana University.
Watkins, C. (1989). Learning from delayed rewards. Doctoral dissertation,
King’s College, Oxford.
Watkins, C. J. C. H., and Dayan, P. (1992). Q-learning. Machine Learning,
8, 279–292.
Watt, S. (1996). Naive psychology and the inverted Turing test. Psycoloquy,
7.
Wechsler, D. (1958). The measurement and appraisal of adult intelligence.
Baltimore: Williams & Wilkinds. 4 edition.

178

BIBLIOGRAPHY
Willems, F., Shtarkov, Y., and Tjalkens, T. (1995). The context-tree weighting
method: Basic properties. IEEE Transactions on Information Theory, 41,
653–664.
Witten, I. H. (1977). An adaptive optimal controller for discrete-time Markov
environments. Information and Control, 34, 286–295.
Wolpert, D. H., and Macready, W. G. (1997). No free lunch theorems for
optimization. IEEE Trans. on Evolutionary Computation, 1, 67–82.
Yerkes, R. M., and Yerkes, A. W. (1929). The <span class="t0 t1 t2 t3 t4 t5 t6 t7 t8 t9 t10 t11 t12 t13 t14 t15 t16 t17 t18 t19 t20 t21 t22 t23 t24 t25 t26 t27 t28 t29 hoverable">great</span> apes: A study of anthropoid
life. <span class="t0 t1 t5 t6 t7 t10 t11 t12 t13 t14 t16 t18 t19 t23 t24 t25 t26 t27 t28 hoverable">New</span> Haven: Yale University Press.
Zentall, T. R. (1997). Animal memory: The role of instructions. Learning and
Motivation, 28, 248–267.
Zentall, T. R. (2000). Animal intelligence. In R. J. Sternberg (Ed.), Handbook
of intelligence. Cambridge University Press.
Zvonkin, A. K., and Levin, L. A. (1970). The complexity of finite objects and
the development of the concepts of information and randomness by <span class="t1 t2 t3 t6 t7 t9 t13 t14 t19 t20 t21 t22 t23 t24 t26 t28 t29 hoverable">means</span>
of the theory of algorithms. Russian Mathematical Surveys, 25, 83–124.

179

Index
abstract thinking, 6
action space, 41
actions, 40, 72
Acton, Lord, 137
Adams, S., 136
Adaptive <span class="t20 hoverable">Intelligence</span> Inc., 19
agent
active, 39
AIXI, 48, 82, 93, 109, 137
choose own goals, 92
formal, 42
human, 83
informal, 40
optimal, 45
passive, 39
random, 78
self-optimising, 50, 64
simple, 80
specialised, 79
super intelligent, 82
agent-environment model, 39
AI winter, 125
Albus, J. S., 9
aliens, 135
alphabet, 41
Anastasi, A., 7
aperiodic, 63
Artificial
general
<span class="t20 hoverable">intelligence</span>
(AGI), 126
axons, 127
backgammon, 126
Bayes’ rule, 25
Bayes, T., 25
Bellman equation, 46
Binet, A., 4, 13

Bingham, W. V., 5
Block, N., 16
Blockhead argument, 89
BlueBrain project, 132
BlueGene supercomputer, 132
Boring, E. G., 9
brain, 128
brain simulation, 130
Bringsjord, S., 19
Brooks, R., 136
C-Test, 20
Carroll, J. B., 4
Cattell, R. B., 4, 10
Chaitin, G., 20, 105
checkers, 127
chess, 2, 40, 126
Chinese room argument, 89
coenumerable function, 33
Colvin, 5
communicating class, 63
competitive games, 19
complexity
Kolmogorov, 20, 31, 34, 98
Levin, 20, 93
of environments, 76
of prediction, 102
time, 21, 104
computable binary sequence, 97
computable function, 33
consciousness, 90, 127
control theory, 41, 73
cortex, 131
creativity, 90
Dearborn, 5

181

Index
diversity control, 134
Dowe, D., 17
Drever, J., 7
dumb luck, 75
eligibility trace, 111
emotion, 90
empirical future discounted reward, 112
enumerable function, 33
environment, 6, 40
accessible, 65
active, 57
Bandit, 57
bandit, 66
Bernoulli scheme, 54, 65
Classification problem, 65
classification problem, 61
computable, 74
ergodic, 64
formal, 42
function optimisation, 60, 67
higher <span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> MDP, 58
Markov chain, 54, 62
Markov
decision
process
(MDP), 57
Partially Observable MDP
(POMDP), 59
passive, 56
repeated strategic game, 61,
66
reward summable, 73, 88
sequence prediction, 56, 67
totally passive, 55
wide range of, 74
Epicurus, principle of multiple explanations, 24
evidence, 26
evolution, 133
expected future discounted reward,
44, 73, 110
fitness function, 134
fitness uniform optimisation, 135
Fogel, D. B., 9

182

freewill, 90
g-factor, 4, 14
Gödel incompleteness, 105, 129
Gödel machine, 129
Galton, F., 13
Gardner, H., 3
genetic algorithm, 133
geometric temporal discounting, 45
Goertzel, B., 10
Good, I. J., 135
Google, 127
Gottfredson, L. S., 1, 6, 20
Gudwin, R. R., 9
Guilford, P. J., 3
HAL project, 18
Hawkins, J., 132
Hay, N., 137
Henmon, 7
Hernandez-Orallo, J., 20
HL(λ) algorithm, 115
Horst, J., 10
human, 83
Hutter prize, 18
Hutter, M., 18, 21
imagination, 90
indifference principle, 36
inductive inference, 23
<span class="t20 hoverable">intelligence</span>
animal, 15
crystallized, 4
cultural bias, 12
definitions, 4, 9
dynamic tests, 13
efficiency, 11
everyday meaning, 91
explosion, 135
fluid, 4
informal thesis definition, 6
IQ, 11, 14, 20
machine, 16
<span class="t0 t6 t7 t8 t11 t13 t14 t17 t18 t19 t20 t21 t24 t26 t28 t29 hoverable">order</span> relation, 71
static tests, 12

Index
tests, 11
theories, 3
universal, 77, 133
interaction history, 42, 44
Jensen, A., 8
Joshua blue project, 18
Joy, B., 137
knowledge, 7
Kronecker delta, 30, 115
Kurzweil, R., 10, 133, 137
learning, 6
Levin, L., 32
linguistic complexity, 18
Loebner prize, 17
love, 90
lower semi-computable, 34
LSTD(λ) algorithm, 111
Mahoney, M., 17
Masum, H., 9, 19
maximum a posteriori (MAP), 39
maximum likelihood (ML), 39
McCarthy, J., 9
measure
computable, Mc , 33
enumerable, Me , 33
probability, 28
meat machine, 127
minimum
description
length
(MDL), 39, 95
minimum message length (MML),
39, 95
Minsky, M., 10
mixture distribution, 29, 49
Moravec, H., 133
Neisser, U., 5
Newell, A., 10
No free lunch theorem, 93
non-deterministic polynomial time,
NP, 22
Norvig, P., 136
Numenta, 132

observations, 40, 73
Occam’s razor, 14, 25, 35, 75
Ockham, W., 25
Optimal ordered problem solver
(OOPS), 129
Pareto optimal, 49
Pell, B., 136
perception space, 41
perceptions, 40, 72
Pinter, 5
planning, 6
polynomial time, P, 21
Poole, D., 10
posterior distribution, 26
prediction with expert advice, 77
predictor
computable, 95, 97
Solomonoff, 38
prior distribution, 26
algorithmic, 34
dominant, 35
improper, 27
Solomonoff, 30
Solomonoff-Levin, 32, 35
Speed, 93, 109, 129
universal, 36
universal over environments,
47
prosperity, 137
psychometric tests, 19
Q(λ), 122
qualia, 127
R. J. Sternberg, 4
randomness, Martin-Löf, 78
Raven, J., 14
reasoning, 6
Rees, M, 137
reference machine, 32
reinforcement learning, 41
reward, 40, 72
reward space, 41
Roadrunner supercomputer, 133

183

Index
Saffo, P., 136
Sarsa(λ), 121
Search for Extraterrestrial <span class="t20 hoverable">Intelligence</span> (SETI), 135
Searle, J., 16
self-optimising, 50
semi-measure, 32
sequence prediction, 27, 56
sequence prediction test, 21
Simon, H., 125
Simonton, D. K., 5
simple computable sequences, 100
Singularity Institute for Artificial
<span class="t20 hoverable">Intelligence</span> (SIAI), 136
Smith’s test, 21
Smith, W. D., 21
Snow, 5
Solomonoff induction, 38
Solomonoff, R. J., 30
soul, 90
Standford-Binet, 13
Sternberg, R. J., 5
supercomputer, 132
symbols, 41
temporal difference learning, 110
temporal preferences, 44
Terman, L. M., 7, 13
<span class="t2 t7 t9 t11 t12 t13 t14 t17 t20 t24 t29 hoverable">text</span> compression, 17
Thiel, P., 136
Thurstone, L. L., 3
Turing machine
monotone universal, 96
prefix universal, 30
Turing test, 16
fail, 17
universal inference, 36
vitalism, 127
Voss, P., 19
Wallach, W, 136
Wang, P., 11
Warwick, K., 9

184

Wechsler adult <span class="t20 hoverable">intelligence</span> scale,
14
Wechsler, D., 5, 13
Wikipedia, 18
Woodrow, H., 8
working memory, 14
Yudkowsky, E., 137

