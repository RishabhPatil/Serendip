{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piyus_000\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/NewsArticles/\"\n",
    "files = os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(nltk.corpus.stopwords.words('english'))\n",
    "s.add('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file in files:\n",
    "    with open(data_folder+file,encoding=\"ISO-8859-1\") as f:\n",
    "        text = f.read()\n",
    "        text = re.split('[, \\.\\n]', text)\n",
    "        text = [word.lower() for word in text if word.lower() not in s and word.isalpha() and len(word)>=3]\n",
    "        texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(common_corpus,id2word=common_dictionary ,num_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 0.990136)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(common_dictionary.doc2bow(texts[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text v/s Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[str(0) for i in range(150)] for i in range(len(texts))]\n",
    "\n",
    "### Increase for more Radius\n",
    "factor = 1\n",
    "###\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    topics = lda.get_document_topics(common_dictionary.doc2bow(texts[i]))\n",
    "    for t in topics:\n",
    "        matrix[i][t[0]] = str(t[1] * factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ','+','.join([\"Topic\"+str(i) for i in range(30)])\n",
    "for i in range(len(texts)):\n",
    "    string += '\\ndoc' + str(i) + ',' + ','.join(matrix[i])\n",
    "\n",
    "with open(\"data/text_topic_matrix.csv\", \"w\") as f:\n",
    "    f.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word v/s Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_string = 'word,value'\n",
    "\n",
    "for t in lda.get_topic_terms(1, topn=100):\n",
    "#     print( lda.id2word[t[0]],t[1],t[0])\n",
    "    csv_string += '\\n' + lda.id2word[t[0]] + ',' + str(t[1])\n",
    "# t = lda.get_topic_terms(57, topn=100)\n",
    "\n",
    "with open('topic1.csv', 'w') as f:\n",
    "    f.write(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1715"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = {}\n",
    "for k,v in lda.id2word.items():\n",
    "    words[v] = k\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for topic in range(30):\n",
    "    word_list = lda.get_topic_terms(topic, topn=100)\n",
    "    for word_tuple in word_list:\n",
    "        word_id = word_tuple[0]\n",
    "        word_score = word_tuple[1]\n",
    "        if word_id not in word_dict:\n",
    "            word_dict[word_id] = {}\n",
    "        word_dict[word_id][topic] = word_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "ix = 0\n",
    "for file in files:\n",
    "    print(ix)\n",
    "    with open(data_folder+file,encoding=\"ISO-8859-1\") as f:\n",
    "        text = \"\\n\"+f.read()\n",
    "        text_words = set(re.split('[, \\.\\n]', text))\n",
    "        html_string = text\n",
    "        for word in text_words:\n",
    "            if word.lower() in words and words[word.lower()] in word_dict:\n",
    "                keys = list(word_dict[words[word.lower()]].keys())\n",
    "                classes = ''\n",
    "                for i in keys:\n",
    "                    classes += 't'+str(i)+\" \"\n",
    "#                 html_string = html_string.replace(r\"\\b\"+word+\"\\b\",\"<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\")\n",
    "                html_string = html_string.replace(\"\\n\"+word+\" \",\"\\n<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span> \")\n",
    "                html_string = html_string.replace(\" \"+word+\"\\n\",\" <span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\\n\")\n",
    "                html_string = html_string.replace(\" \"+word+\" \",\" <span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span> \")\n",
    "                html_string = html_string.replace(\"\\n\"+word+\"\\n\",\"\\n<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\\n\")\n",
    "#                 html_string = re.sub(word+\" \",\"<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\",html_string)\n",
    "#                 html_string = re.sub(\" \"+word,\"<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\",html_string)\n",
    "        with open(\"data/doc\"+str(ix)+\".txt\",\"w\",encoding=\"ISO-8859-1\") as f1:\n",
    "            f1.write(html_string)\n",
    "    ix+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC V/S WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(30):\n",
    "    word_list = lda.get_topic_terms(topic, topn=100)\n",
    "    file_str = \"word,value\"\n",
    "    for word_tuple in word_list:\n",
    "        word_id = word_tuple[0]\n",
    "        file_str += \"\\n\" + str(lda.id2word[word_tuple[0]]) +\",\"+ str(word_tuple[1])\n",
    "    with open(\"data/topic\"+str(topic)+\".csv\", \"w\") as f:\n",
    "        f.write(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c13f31a2f9e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdoc_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'doc'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/doc_ids.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "files = sorted(files)\n",
    "doc_id = {}\n",
    "for i in range(len(files)):\n",
    "    doc_id['doc'+str(i)] = files[i]\n",
    "with open('data/doc_ids.json','w') as f:\n",
    "    json.dump(doc_id, f, indent=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
