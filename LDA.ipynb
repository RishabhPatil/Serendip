{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data/NewsArticles/\"\n",
    "files = os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(files)\n",
    "doc_id = {}\n",
    "for i in range(len(files)):\n",
    "    doc_id['doc'+str(i)] = files[i]\n",
    "with open('data/doc_ids.json','w') as f:\n",
    "    json.dump(doc_id, f, indent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"STOPWORDS\", 'r') as f:\n",
    "    STOPWORDS = set(f.read().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(nltk.corpus.stopwords.words('english')) | STOPWORDS\n",
    "s.add('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " \"'ll\",\n",
       " 'a',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abst',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'adj',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ah',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'announce',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apparently',\n",
       " 'approximately',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'arent',\n",
       " 'arise',\n",
       " 'around',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'at',\n",
       " 'auth',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'b',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'begin',\n",
       " 'beginning',\n",
       " 'beginnings',\n",
       " 'begins',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'biol',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'briefly',\n",
       " 'but',\n",
       " 'by',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'came',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'co',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'could',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'couldnt',\n",
       " 'd',\n",
       " 'date',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'done',\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'due',\n",
       " 'during',\n",
       " 'e',\n",
       " 'each',\n",
       " 'ed',\n",
       " 'edu',\n",
       " 'effect',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'eighty',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'enough',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'et-al',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'except',\n",
       " 'f',\n",
       " 'far',\n",
       " 'few',\n",
       " 'ff',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'g',\n",
       " 'gave',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'h',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'hed',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'heres',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'hes',\n",
       " 'hi',\n",
       " 'hid',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'home',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " \"i'll\",\n",
       " \"i've\",\n",
       " 'id',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'im',\n",
       " 'immediate',\n",
       " 'immediately',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'in',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'index',\n",
       " 'information',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'invention',\n",
       " 'inward',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'itd',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'j',\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep \\tkeeps',\n",
       " 'kept',\n",
       " 'kg',\n",
       " 'km',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'l',\n",
       " 'largely',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " 'lets',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'line',\n",
       " 'little',\n",
       " 'll',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'ltd',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'made',\n",
       " 'mainly',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " 'me',\n",
       " 'mean',\n",
       " 'means',\n",
       " 'meantime',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'mg',\n",
       " 'might',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'million',\n",
       " 'miss',\n",
       " 'ml',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'mug',\n",
       " 'must',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'n',\n",
       " 'na',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nay',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessarily',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'ninety',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'nonetheless',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'nos',\n",
       " 'not',\n",
       " 'noted',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'o',\n",
       " 'obtain',\n",
       " 'obtained',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'omitted',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'ord',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'owing',\n",
       " 'own',\n",
       " 'p',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'part',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'past',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'poorly',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'potentially',\n",
       " 'pp',\n",
       " 'predominantly',\n",
       " 'present',\n",
       " 'previously',\n",
       " 'primarily',\n",
       " 'probably',\n",
       " 'promptly',\n",
       " 'proud',\n",
       " 'provides',\n",
       " 'put',\n",
       " 'q',\n",
       " 'que',\n",
       " 'quickly',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'r',\n",
       " 'ran',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'readily',\n",
       " 'really',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'ref',\n",
       " 'refs',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'related',\n",
       " 'relatively',\n",
       " 'research',\n",
       " 'respectively',\n",
       " 'resulted',\n",
       " 'resulting',\n",
       " 'results',\n",
       " 'right',\n",
       " 'run',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'sec',\n",
       " 'section',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sent',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'shed',\n",
       " 'shes',\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'show',\n",
       " 'showed',\n",
       " 'shown',\n",
       " 'showns',\n",
       " 'shows',\n",
       " 'significant',\n",
       " 'significantly',\n",
       " 'similar',\n",
       " 'similarly',\n",
       " 'since',\n",
       " 'six',\n",
       " 'slightly',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'somethan',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specifically',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'stop',\n",
       " 'strongly',\n",
       " 'sub',\n",
       " 'substantially',\n",
       " 'successfully',\n",
       " 'such',\n",
       " 'sufficiently',\n",
       " 'suggest',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "text_dict = {}\n",
    "for file in files:\n",
    "    with open(data_folder+file,encoding=\"ISO-8859-1\") as f:\n",
    "        text = f.read()\n",
    "        text = re.split('[, \\.\\n]', text)\n",
    "        text = [word.lower() for word in text if word.lower() not in s and word.isalpha() and len(word)>=3]\n",
    "        texts.append(text)\n",
    "        text_dict[file] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(common_corpus,id2word=common_dictionary ,num_topics=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(20, 0.9876068)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.get_document_topics(common_dictionary.doc2bow(texts[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_json = {}\n",
    "for i in doc_id:\n",
    "    topic_array = [0 for i in range(30)]\n",
    "    topics = lda.get_document_topics(common_dictionary.doc2bow(text_dict[doc_id[i]]))\n",
    "    for tpc in topics:\n",
    "        topic_array[tpc[0]] = str(tpc[1])\n",
    "    doc_topic_json[i] = topic_array\n",
    "with open(\"data/doc_topic.json\",\"w\") as f:\n",
    "    json.dump(doc_topic_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text v/s Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[str(0) for i in range(150)] for i in range(len(texts))]\n",
    "\n",
    "### Increase for more Radius\n",
    "factor = 1\n",
    "###\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    topics = lda.get_document_topics(common_dictionary.doc2bow(texts[i]))\n",
    "    for t in topics:\n",
    "        matrix[i][t[0]] = str(t[1] * factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ','+','.join([\"Topic\"+str(i) for i in range(30)])\n",
    "for i in range(len(texts)):\n",
    "    string += '\\ndoc' + str(i) + ',' + ','.join(matrix[i])\n",
    "\n",
    "with open(\"data/text_topic_matrix.csv\", \"w\") as f:\n",
    "    f.write(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word v/s Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_string = 'word,value'\n",
    "\n",
    "for t in lda.get_topic_terms(1, topn=100):\n",
    "#     print( lda.id2word[t[0]],t[1],t[0])\n",
    "    csv_string += '\\n' + lda.id2word[t[0]] + ',' + str(t[1])\n",
    "# t = lda.get_topic_terms(57, topn=100)\n",
    "\n",
    "with open('topic1.csv', 'w') as f:\n",
    "    f.write(csv_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1589"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = {}\n",
    "for k,v in lda.id2word.items():\n",
    "    words[v] = k\n",
    "with open(\"data/word_ids.json\", \"w\") as f:\n",
    "    json.dump(words, f)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({2: '0.0034798672', 16: '0.001532287'},\n",
       " {'actions': 0,\n",
       "  'administration': 1,\n",
       "  'appointment': 2,\n",
       "  'asked': 3,\n",
       "  'attorney': 4,\n",
       "  'authority': 5,\n",
       "  'bring': 6,\n",
       "  'bringing': 7,\n",
       "  'charges': 8,\n",
       "  'connected': 9,\n",
       "  'conservative': 10,\n",
       "  'consider': 11,\n",
       "  'contended': 12,\n",
       "  'counsel': 13,\n",
       "  'criminal': 14,\n",
       "  'decisions': 15,\n",
       "  'department': 16,\n",
       "  'designating': 17,\n",
       "  'dojs': 18,\n",
       "  'door': 19,\n",
       "  'email': 20,\n",
       "  'fast': 21,\n",
       "  'floated': 22,\n",
       "  'furious': 23,\n",
       "  'general': 24,\n",
       "  'generally': 25,\n",
       "  'groups': 26,\n",
       "  'gun': 27,\n",
       "  'hewitt': 28,\n",
       "  'hillary': 29,\n",
       "  'host': 30,\n",
       "  'hugh': 31,\n",
       "  'idea': 32,\n",
       "  'include': 33,\n",
       "  'internal': 34,\n",
       "  'interview': 35,\n",
       "  'ire': 36,\n",
       "  'jeff': 37,\n",
       "  'justice': 38,\n",
       "  'left': 39,\n",
       "  'noncommittal': 40,\n",
       "  'obama': 41,\n",
       "  'open': 42,\n",
       "  'openness': 43,\n",
       "  'operated': 44,\n",
       "  'practices': 45,\n",
       "  'provoked': 46,\n",
       "  'radio': 47,\n",
       "  'republican': 48,\n",
       "  'revenue': 49,\n",
       "  'review': 50,\n",
       "  'scandal': 51,\n",
       "  'second': 52,\n",
       "  'sessions': 53,\n",
       "  'special': 54,\n",
       "  'suggested': 55,\n",
       "  'suggests': 56,\n",
       "  'take': 57,\n",
       "  'taken': 58,\n",
       "  'thursday': 59,\n",
       "  'treatment': 60,\n",
       "  'would': 61,\n",
       "  'years': 62,\n",
       "  'academics': 63,\n",
       "  'activists': 64,\n",
       "  'airtime': 65,\n",
       "  'angry': 66,\n",
       "  'approach': 67,\n",
       "  'bbc': 68,\n",
       "  'believes': 69,\n",
       "  'bodies': 70,\n",
       "  'campaign': 71,\n",
       "  'canadian': 72,\n",
       "  'changing': 73,\n",
       "  'child': 74,\n",
       "  'children': 75,\n",
       "  'clinic': 76,\n",
       "  'clothes': 77,\n",
       "  'comfortable': 78,\n",
       "  'concerned': 79,\n",
       "  'culture': 80,\n",
       "  'daily': 81,\n",
       "  'decision': 82,\n",
       "  'disagrees': 83,\n",
       "  'documentary': 84,\n",
       "  'encourages': 85,\n",
       "  'encouraging': 86,\n",
       "  'expert': 87,\n",
       "  'favoured': 88,\n",
       "  'fear': 89,\n",
       "  'features': 90,\n",
       "  'featuring': 91,\n",
       "  'feel': 92,\n",
       "  'field': 93,\n",
       "  'fight': 94,\n",
       "  'film': 95,\n",
       "  'fired': 96,\n",
       "  'gender': 97,\n",
       "  'helping': 98,\n",
       "  'identifies': 99,\n",
       "  'identity': 100,\n",
       "  'issues': 101,\n",
       "  'kenneth': 102,\n",
       "  'kids': 103,\n",
       "  'lend': 104,\n",
       "  'live': 105,\n",
       "  'lives': 106,\n",
       "  'opinions': 107,\n",
       "  'opposite': 108,\n",
       "  'places': 109,\n",
       "  'political': 110,\n",
       "  'preferable': 111,\n",
       "  'psychologist': 112,\n",
       "  'result': 113,\n",
       "  'sex': 114,\n",
       "  'shut': 115,\n",
       "  'sustained': 116,\n",
       "  'trans': 117,\n",
       "  'transgender': 118,\n",
       "  'views': 119,\n",
       "  'war': 120,\n",
       "  'weight': 121,\n",
       "  'young': 122,\n",
       "  'zucker': 123,\n",
       "  'aliens': 124,\n",
       "  'ann': 125,\n",
       "  'association': 126,\n",
       "  'belonged': 127,\n",
       "  'belongs': 128,\n",
       "  'bill': 129,\n",
       "  'border': 130,\n",
       "  'breitbart': 131,\n",
       "  'build': 132,\n",
       "  'call': 133,\n",
       "  'calls': 134,\n",
       "  'case': 135,\n",
       "  'coulter': 136,\n",
       "  'curiel': 137,\n",
       "  'diego': 138,\n",
       "  'distributed': 139,\n",
       "  'dollars': 140,\n",
       "  'donald': 141,\n",
       "  'ensure': 142,\n",
       "  'entire': 143,\n",
       "  'federal': 144,\n",
       "  'firm': 145,\n",
       "  'going': 146,\n",
       "  'gonzalo': 147,\n",
       "  'gop': 148,\n",
       "  'hate': 149,\n",
       "  'hates': 150,\n",
       "  'hearings': 151,\n",
       "  'hundreds': 152,\n",
       "  'illegal': 153,\n",
       "  'imagine': 154,\n",
       "  'immigrant': 155,\n",
       "  'judge': 156,\n",
       "  'law': 157,\n",
       "  'lawyers': 158,\n",
       "  'male': 159,\n",
       "  'meaning': 160,\n",
       "  'media': 161,\n",
       "  'mexican': 162,\n",
       "  'mexicans': 163,\n",
       "  'months': 164,\n",
       "  'nomination': 165,\n",
       "  'nominee': 166,\n",
       "  'organization': 167,\n",
       "  'outraged': 168,\n",
       "  'paid': 169,\n",
       "  'pause': 170,\n",
       "  'people': 171,\n",
       "  'persistent': 172,\n",
       "  'presidential': 173,\n",
       "  'race': 174,\n",
       "  'racist': 175,\n",
       "  'raza': 176,\n",
       "  'rulings': 177,\n",
       "  'san': 178,\n",
       "  'scholarships': 179,\n",
       "  'sends': 180,\n",
       "  'speeches': 181,\n",
       "  'spent': 182,\n",
       "  'states': 183,\n",
       "  'stunning': 184,\n",
       "  'telling': 185,\n",
       "  'thinks': 186,\n",
       "  'thousands': 187,\n",
       "  'title': 188,\n",
       "  'trump': 189,\n",
       "  'united': 190,\n",
       "  'university': 191,\n",
       "  'violated': 192,\n",
       "  'wall': 193,\n",
       "  'whatever': 194,\n",
       "  'white': 195,\n",
       "  'words': 196,\n",
       "  'agent': 197,\n",
       "  'agents': 198,\n",
       "  'apprehend': 199,\n",
       "  'area': 200,\n",
       "  'arizona': 201,\n",
       "  'arrived': 202,\n",
       "  'assaulted': 203,\n",
       "  'assigned': 204,\n",
       "  'attack': 205,\n",
       "  'battle': 206,\n",
       "  'began': 207,\n",
       "  'breaking': 208,\n",
       "  'brian': 209,\n",
       "  'bullets': 210,\n",
       "  'carry': 211,\n",
       "  'cartel': 212,\n",
       "  'cochise': 213,\n",
       "  'confirm': 214,\n",
       "  'confirmed': 215,\n",
       "  'continue': 216,\n",
       "  'corridor': 217,\n",
       "  'county': 218,\n",
       "  'customs': 219,\n",
       "  'deputies': 220,\n",
       "  'distance': 221,\n",
       "  'drug': 222,\n",
       "  'effort': 223,\n",
       "  'enforcement': 224,\n",
       "  'fire': 225,\n",
       "  'firing': 226,\n",
       "  'fled': 227,\n",
       "  'foot': 228,\n",
       "  'group': 229,\n",
       "  'gunfire': 230,\n",
       "  'gunmen': 231,\n",
       "  'gunmnen': 232,\n",
       "  'heard': 233,\n",
       "  'hereford': 234,\n",
       "  'honors': 235,\n",
       "  'individuals': 236,\n",
       "  'injured': 237,\n",
       "  'investigating': 238,\n",
       "  'killed': 239,\n",
       "  'load': 240,\n",
       "  'local': 241,\n",
       "  'locate': 242,\n",
       "  'located': 243,\n",
       "  'manhunt': 244,\n",
       "  'mobile': 245,\n",
       "  'multiple': 246,\n",
       "  'observed': 247,\n",
       "  'officer': 248,\n",
       "  'officers': 249,\n",
       "  'officials': 250,\n",
       "  'operating': 251,\n",
       "  'overnight': 252,\n",
       "  'patrol': 253,\n",
       "  'patrolling': 254,\n",
       "  'protection': 255,\n",
       "  'provided': 256,\n",
       "  'responded': 257,\n",
       "  'return': 258,\n",
       "  'rifle': 259,\n",
       "  'rival': 260,\n",
       "  'road': 261,\n",
       "  'scene': 262,\n",
       "  'shots': 263,\n",
       "  'smuggling': 264,\n",
       "  'southern': 265,\n",
       "  'station': 266,\n",
       "  'steal': 267,\n",
       "  'struck': 268,\n",
       "  'surveillance': 269,\n",
       "  'suspected': 270,\n",
       "  'suspects': 271,\n",
       "  'terry': 272,\n",
       "  'texas': 273,\n",
       "  'trying': 274,\n",
       "  'two': 275,\n",
       "  'unit': 276,\n",
       "  'unknown': 277,\n",
       "  'unoccupied': 278,\n",
       "  'update': 279,\n",
       "  'vehicle': 280,\n",
       "  'aint': 281,\n",
       "  'americans': 282,\n",
       "  'attacks': 283,\n",
       "  'barack': 284,\n",
       "  'bernardino': 285,\n",
       "  'big': 286,\n",
       "  'blasting': 287,\n",
       "  'california': 288,\n",
       "  'carly': 289,\n",
       "  'coming': 290,\n",
       "  'condemning': 291,\n",
       "  'contest': 292,\n",
       "  'convention': 293,\n",
       "  'criticized': 294,\n",
       "  'crooked': 295,\n",
       "  'cruz': 296,\n",
       "  'day': 297,\n",
       "  'declaring': 298,\n",
       "  'disagreed': 299,\n",
       "  'donors': 300,\n",
       "  'earlier': 301,\n",
       "  'effectively': 302,\n",
       "  'evil': 303,\n",
       "  'extremists': 304,\n",
       "  'fiorina': 305,\n",
       "  'francisco': 306,\n",
       "  'impostor': 307,\n",
       "  'islamic': 308,\n",
       "  'jersey': 309,\n",
       "  'lecturing': 310,\n",
       "  'man': 311,\n",
       "  'message': 312,\n",
       "  'needed': 313,\n",
       "  'night': 314,\n",
       "  'nominating': 315,\n",
       "  'party': 316,\n",
       "  'prematurely': 317,\n",
       "  'president': 318,\n",
       "  'primary': 319,\n",
       "  'rally': 320,\n",
       "  'reckoning': 321,\n",
       "  'referenced': 322,\n",
       "  'rewarded': 323,\n",
       "  'ripped': 324,\n",
       "  'saturday': 325,\n",
       "  'sen': 326,\n",
       "  'speech': 327,\n",
       "  'spoke': 328,\n",
       "  'stands': 329,\n",
       "  'state': 330,\n",
       "  'system': 331,\n",
       "  'ted': 332,\n",
       "  'terrorism': 333,\n",
       "  'throw': 334,\n",
       "  'told': 335,\n",
       "  'touchdown': 336,\n",
       "  'victory': 337,\n",
       "  'violent': 338,\n",
       "  'adviser': 339,\n",
       "  'ambassador': 340,\n",
       "  'bigger': 341,\n",
       "  'casting': 342,\n",
       "  'cloud': 343,\n",
       "  'communication': 344,\n",
       "  'communications': 345,\n",
       "  'contacts': 346,\n",
       "  'content': 347,\n",
       "  'controversy': 348,\n",
       "  'days': 349,\n",
       "  'democrats': 350,\n",
       "  'denied': 351,\n",
       "  'describe': 352,\n",
       "  'developments': 353,\n",
       "  'early': 354,\n",
       "  'eerie': 355,\n",
       "  'elected': 356,\n",
       "  'election': 357,\n",
       "  'firestorm': 358,\n",
       "  'flynn': 359,\n",
       "  'house': 360,\n",
       "  'intelligence': 361,\n",
       "  'intercepted': 362,\n",
       "  'language': 363,\n",
       "  'latest': 364,\n",
       "  'members': 365,\n",
       "  'mentioned': 366,\n",
       "  'michael': 367,\n",
       "  'misled': 368,\n",
       "  'monday': 369,\n",
       "  'national': 370,\n",
       "  'office': 371,\n",
       "  'operatives': 372,\n",
       "  'outright': 373,\n",
       "  'parallels': 374,\n",
       "  'phone': 375,\n",
       "  'place': 376,\n",
       "  'presidency': 377,\n",
       "  'reported': 378,\n",
       "  'resigned': 379,\n",
       "  'richard': 380,\n",
       "  'russia': 381,\n",
       "  'russian': 382,\n",
       "  'security': 383,\n",
       "  'statements': 384,\n",
       "  'sunk': 385,\n",
       "  'times': 386,\n",
       "  'took': 387,\n",
       "  'trumps': 388,\n",
       "  'tuesday': 389,\n",
       "  'used': 390,\n",
       "  'using': 391,\n",
       "  'watergate': 392,\n",
       "  'word': 393,\n",
       "  'york': 394,\n",
       "  'agitators': 395,\n",
       "  'america': 396,\n",
       "  'american': 397,\n",
       "  'bernie': 398,\n",
       "  'booed': 399,\n",
       "  'breakdown': 400,\n",
       "  'burning': 401,\n",
       "  'candidate': 402,\n",
       "  'center': 403,\n",
       "  'city': 404,\n",
       "  'class': 405,\n",
       "  'clinton': 406,\n",
       "  'control': 407,\n",
       "  'correspondents': 408,\n",
       "  'crashed': 409,\n",
       "  'delegates': 410,\n",
       "  'democratic': 411,\n",
       "  'disaster': 412,\n",
       "  'dnc': 413,\n",
       "  'emerged': 414,\n",
       "  'event': 415,\n",
       "  'flag': 416,\n",
       "  'floor': 417,\n",
       "  'force': 418,\n",
       "  'green': 419,\n",
       "  'inside': 420,\n",
       "  'jill': 421,\n",
       "  'literally': 422,\n",
       "  'mainstream': 423,\n",
       "  'nationalists': 424,\n",
       "  'nervous': 425,\n",
       "  'news': 426,\n",
       "  'nominate': 427,\n",
       "  'outnumbered': 428,\n",
       "  'personally': 429,\n",
       "  'philly': 430,\n",
       "  'police': 431,\n",
       "  'populist': 432,\n",
       "  'protest': 433,\n",
       "  'protesters': 434,\n",
       "  'rampaged': 435,\n",
       "  'real': 436,\n",
       "  'revelations': 437,\n",
       "  'rigged': 438,\n",
       "  'rocked': 439,\n",
       "  'rode': 440,\n",
       "  'rules': 441,\n",
       "  'sanders': 442,\n",
       "  'seized': 443,\n",
       "  'set': 444,\n",
       "  'speakers': 445,\n",
       "  'stein': 446,\n",
       "  'stomping': 447,\n",
       "  'story': 448,\n",
       "  'streets': 449,\n",
       "  'suffers': 450,\n",
       "  'suspended': 451,\n",
       "  'voice': 452,\n",
       "  'vote': 453,\n",
       "  'walked': 454,\n",
       "  'wikileaks': 455,\n",
       "  'world': 456,\n",
       "  'airport': 457,\n",
       "  'airports': 458,\n",
       "  'angeles': 459,\n",
       "  'assembled': 460,\n",
       "  'bradley': 461,\n",
       "  'components': 462,\n",
       "  'cooper': 463,\n",
       "  'countries': 464,\n",
       "  'country': 465,\n",
       "  'demonstrating': 466,\n",
       "  'demonstrations': 467,\n",
       "  'division': 468,\n",
       "  'dueling': 469,\n",
       "  'eastern': 470,\n",
       "  'erupt': 471,\n",
       "  'executive': 472,\n",
       "  'february': 473,\n",
       "  'friday': 474,\n",
       "  'front': 475,\n",
       "  'gathered': 476,\n",
       "  'hands': 477,\n",
       "  'headscarves': 478,\n",
       "  'hour': 479,\n",
       "  'international': 480,\n",
       "  'issue': 481,\n",
       "  'knx': 482,\n",
       "  'late': 483,\n",
       "  'lax': 484,\n",
       "  'led': 485,\n",
       "  'los': 486,\n",
       "  'major': 487,\n",
       "  'middle': 488,\n",
       "  'morning': 489,\n",
       "  'numbered': 490,\n",
       "  'order': 491,\n",
       "  'photos': 492,\n",
       "  'pic': 493,\n",
       "  'posted': 494,\n",
       "  'proponents': 495,\n",
       "  'protesting': 496,\n",
       "  'protests': 497,\n",
       "  'read': 498,\n",
       "  'reporter': 499,\n",
       "  'restraining': 500,\n",
       "  'restricting': 501,\n",
       "  'restriction': 502,\n",
       "  'ruled': 503,\n",
       "  'rummell': 504,\n",
       "  'seattle': 505,\n",
       "  'sides': 506,\n",
       "  'signs': 507,\n",
       "  'sizes': 508,\n",
       "  'temporarily': 509,\n",
       "  'temporary': 510,\n",
       "  'terminal': 511,\n",
       "  'tom': 512,\n",
       "  'travel': 513,\n",
       "  'twitter': 514,\n",
       "  'varying': 515,\n",
       "  'well': 516,\n",
       "  'women': 517,\n",
       "  'yellow': 518,\n",
       "  'areas': 519,\n",
       "  'arrested': 520,\n",
       "  'article': 521,\n",
       "  'bosses': 522,\n",
       "  'capture': 523,\n",
       "  'cartels': 524,\n",
       "  'chronicles': 525,\n",
       "  'citizen': 526,\n",
       "  'ciudad': 527,\n",
       "  'coahuila': 528,\n",
       "  'comedic': 529,\n",
       "  'communities': 530,\n",
       "  'death': 531,\n",
       "  'dressing': 532,\n",
       "  'english': 533,\n",
       "  'entertainment': 534,\n",
       "  'erick': 535,\n",
       "  'exclusive': 536,\n",
       "  'exploits': 537,\n",
       "  'expose': 538,\n",
       "  'face': 539,\n",
       "  'forced': 540,\n",
       "  'forcing': 541,\n",
       "  'guevara': 542,\n",
       "  'gulf': 543,\n",
       "  'hammer': 544,\n",
       "  'hitmen': 545,\n",
       "  'hunting': 546,\n",
       "  'including': 547,\n",
       "  'interesting': 548,\n",
       "  'journalists': 549,\n",
       "  'kiss': 550,\n",
       "  'learn': 551,\n",
       "  'lingerie': 552,\n",
       "  'loko': 553,\n",
       "  'marine': 554,\n",
       "  'marino': 555,\n",
       "  'mocking': 556,\n",
       "  'morales': 557,\n",
       "  'navy': 558,\n",
       "  'operate': 559,\n",
       "  'original': 560,\n",
       "  'pseudonym': 561,\n",
       "  'published': 562,\n",
       "  'recruit': 563,\n",
       "  'risk': 564,\n",
       "  'silencing': 565,\n",
       "  'spanish': 566,\n",
       "  'sport': 567,\n",
       "  'tamaulipas': 568,\n",
       "  'team': 569,\n",
       "  'top': 570,\n",
       "  'traveled': 571,\n",
       "  'various': 572,\n",
       "  'victoria': 573,\n",
       "  'wear': 574,\n",
       "  'willing': 575,\n",
       "  'womens': 576,\n",
       "  'writers': 577,\n",
       "  'written': 578,\n",
       "  'zetas': 579,\n",
       "  'additional': 580,\n",
       "  'aimed': 581,\n",
       "  'barriers': 582,\n",
       "  'business': 583,\n",
       "  'close': 584,\n",
       "  'deportations': 585,\n",
       "  'ease': 586,\n",
       "  'economic': 587,\n",
       "  'economics': 588,\n",
       "  'economists': 589,\n",
       "  'expand': 590,\n",
       "  'expanding': 591,\n",
       "  'fifteen': 592,\n",
       "  'flow': 593,\n",
       "  'half': 594,\n",
       "  'immigrants': 595,\n",
       "  'immigration': 596,\n",
       "  'import': 597,\n",
       "  'imports': 598,\n",
       "  'impose': 599,\n",
       "  'increased': 600,\n",
       "  'industries': 601,\n",
       "  'issued': 602,\n",
       "  'leave': 603,\n",
       "  'limits': 604,\n",
       "  'march': 605,\n",
       "  'nationalism': 606,\n",
       "  'nationalist': 607,\n",
       "  'number': 608,\n",
       "  'numbers': 609,\n",
       "  'opposed': 610,\n",
       "  'percent': 611,\n",
       "  'policies': 612,\n",
       "  'policy': 613,\n",
       "  'priority': 614,\n",
       "  'program': 615,\n",
       "  'proposals': 616,\n",
       "  'reducing': 617,\n",
       "  'released': 618,\n",
       "  'restrictions': 619,\n",
       "  'spending': 620,\n",
       "  'support': 621,\n",
       "  'surprising': 622,\n",
       "  'survey': 623,\n",
       "  'think': 624,\n",
       "  'threatened': 625,\n",
       "  'tiny': 626,\n",
       "  'trade': 627,\n",
       "  'unfair': 628,\n",
       "  'visas': 629,\n",
       "  'vital': 630,\n",
       "  'workers': 631,\n",
       "  'adolf': 632,\n",
       "  'allegation': 633,\n",
       "  'claim': 634,\n",
       "  'claimed': 635,\n",
       "  'closed': 636,\n",
       "  'comments': 637,\n",
       "  'committee': 638,\n",
       "  'conduct': 639,\n",
       "  'constitution': 640,\n",
       "  'continues': 641,\n",
       "  'controversial': 642,\n",
       "  'creation': 643,\n",
       "  'criticises': 644,\n",
       "  'decide': 645,\n",
       "  'disciplinary': 646,\n",
       "  'doors': 647,\n",
       "  'ended': 648,\n",
       "  'engaged': 649,\n",
       "  'entering': 650,\n",
       "  'evidence': 651,\n",
       "  'fights': 652,\n",
       "  'future': 653,\n",
       "  'hearing': 654,\n",
       "  'hitler': 655,\n",
       "  'israel': 656,\n",
       "  'israeli': 657,\n",
       "  'jewish': 658,\n",
       "  'jews': 659,\n",
       "  'ken': 660,\n",
       "  'killing': 661,\n",
       "  'labour': 662,\n",
       "  'leader': 663,\n",
       "  'livingstone': 664,\n",
       "  'lobby': 665,\n",
       "  'london': 666,\n",
       "  'mad': 667,\n",
       "  'mayor': 668,\n",
       "  'member': 669,\n",
       "  'nazi': 670,\n",
       "  'resumes': 671,\n",
       "  'smear': 672,\n",
       "  'sparked': 673,\n",
       "  'stay': 674,\n",
       "  'supported': 675,\n",
       "  'thereafter': 676,\n",
       "  'tribunal': 677,\n",
       "  'whether': 678,\n",
       "  'zionism': 679,\n",
       "  'alliance': 680,\n",
       "  'app': 681,\n",
       "  'appeared': 682,\n",
       "  'avoid': 683,\n",
       "  'ban': 684,\n",
       "  'barring': 685,\n",
       "  'called': 686,\n",
       "  'company': 687,\n",
       "  'customers': 688,\n",
       "  'deleting': 689,\n",
       "  'demand': 690,\n",
       "  'drivers': 691,\n",
       "  'facilitate': 692,\n",
       "  'fares': 693,\n",
       "  'halted': 694,\n",
       "  'higher': 695,\n",
       "  'jfk': 696,\n",
       "  'kick': 697,\n",
       "  'longer': 698,\n",
       "  'patient': 699,\n",
       "  'periods': 700,\n",
       "  'posting': 701,\n",
       "  'pricing': 702,\n",
       "  'refugee': 703,\n",
       "  'response': 704,\n",
       "  'riders': 705,\n",
       "  'servicing': 706,\n",
       "  'social': 707,\n",
       "  'statement': 708,\n",
       "  'strike': 709,\n",
       "  'taxi': 710,\n",
       "  'travelers': 711,\n",
       "  'tried': 712,\n",
       "  'turned': 713,\n",
       "  'tweeted': 714,\n",
       "  'uber': 715,\n",
       "  'users': 716,\n",
       "  'wait': 717,\n",
       "  'went': 718,\n",
       "  'allgemeine': 719,\n",
       "  'apparent': 720,\n",
       "  'arrests': 721,\n",
       "  'attacked': 722,\n",
       "  'attacking': 723,\n",
       "  'changed': 724,\n",
       "  'cologne': 725,\n",
       "  'completely': 726,\n",
       "  'considered': 727,\n",
       "  'crime': 728,\n",
       "  'decades': 729,\n",
       "  'essen': 730,\n",
       "  'eve': 731,\n",
       "  'family': 732,\n",
       "  'female': 733,\n",
       "  'festival': 734,\n",
       "  'friendly': 735,\n",
       "  'gang': 736,\n",
       "  'gangs': 737,\n",
       "  'german': 738,\n",
       "  'germany': 739,\n",
       "  'girls': 740,\n",
       "  'groped': 741,\n",
       "  'harassed': 742,\n",
       "  'highest': 743,\n",
       "  'incident': 744,\n",
       "  'incidents': 745,\n",
       "  'large': 746,\n",
       "  'men': 747,\n",
       "  'migrant': 748,\n",
       "  'migrants': 749,\n",
       "  'migration': 750,\n",
       "  'molesting': 751,\n",
       "  'overshadowed': 752,\n",
       "  'paper': 753,\n",
       "  'presence': 754,\n",
       "  'received': 755,\n",
       "  'report': 756,\n",
       "  'reports': 757,\n",
       "  'roving': 758,\n",
       "  'separate': 759,\n",
       "  'sexually': 760,\n",
       "  'ten': 761,\n",
       "  'town': 762,\n",
       "  'vein': 763,\n",
       "  'victims': 764,\n",
       "  'weekend': 765,\n",
       "  'westen': 766,\n",
       "  'without': 767,\n",
       "  'year': 768,\n",
       "  'zeitung': 769,\n",
       "  'action': 770,\n",
       "  'based': 771,\n",
       "  'bucks': 772,\n",
       "  'cease': 773,\n",
       "  'cities': 774,\n",
       "  'cooperate': 775,\n",
       "  'cudahy': 776,\n",
       "  'defund': 777,\n",
       "  'deprive': 778,\n",
       "  'filed': 779,\n",
       "  'filing': 780,\n",
       "  'funds': 781,\n",
       "  'gather': 782,\n",
       "  'gathering': 783,\n",
       "  'grassroots': 784,\n",
       "  'initiative': 785,\n",
       "  'joseph': 786,\n",
       "  'larger': 787,\n",
       "  'launch': 788,\n",
       "  'letters': 789,\n",
       "  'level': 790,\n",
       "  'mere': 791,\n",
       "  'papers': 792,\n",
       "  'paperwork': 793,\n",
       "  'process': 794,\n",
       "  'prop': 795,\n",
       "  'refuse': 796,\n",
       "  'repeal': 797,\n",
       "  'repeatedly': 798,\n",
       "  'sanctuary': 799,\n",
       "  'signature': 800,\n",
       "  'signatures': 801,\n",
       "  'starting': 802,\n",
       "  'status': 803,\n",
       "  'tax': 804,\n",
       "  'turner': 805,\n",
       "  'utility': 806,\n",
       "  'voters': 807,\n",
       "  'warned': 808,\n",
       "  'weeks': 809,\n",
       "  'yet': 810,\n",
       "  'access': 811,\n",
       "  'attached': 812,\n",
       "  'classified': 813,\n",
       "  'classifying': 814,\n",
       "  'computer': 815,\n",
       "  'contradiction': 816,\n",
       "  'dept': 817,\n",
       "  'document': 818,\n",
       "  'documents': 819,\n",
       "  'emails': 820,\n",
       "  'employees': 821,\n",
       "  'fbi': 822,\n",
       "  'guidance': 823,\n",
       "  'handling': 824,\n",
       "  'highlighted': 825,\n",
       "  'job': 826,\n",
       "  'keeping': 827,\n",
       "  'numerous': 828,\n",
       "  'official': 829,\n",
       "  'preserve': 830,\n",
       "  'preserving': 831,\n",
       "  'protect': 832,\n",
       "  'protecting': 833,\n",
       "  'puzzle': 834,\n",
       "  'questioners': 835,\n",
       "  'receive': 836,\n",
       "  'remember': 837,\n",
       "  'standard': 838,\n",
       "  'testifying': 839,\n",
       "  'train': 840,\n",
       "  'trained': 841,\n",
       "  'training': 842,\n",
       "  'weekly': 843,\n",
       "  'anarchists': 844,\n",
       "  'attempt': 845,\n",
       "  'badly': 846,\n",
       "  'bang': 847,\n",
       "  'biggest': 848,\n",
       "  'black': 849,\n",
       "  'cleveland': 850,\n",
       "  'code': 851,\n",
       "  'cop': 852,\n",
       "  'deliver': 853,\n",
       "  'efforts': 854,\n",
       "  'escorting': 855,\n",
       "  'failed': 856,\n",
       "  'failures': 857,\n",
       "  'final': 858,\n",
       "  'fireworks': 859,\n",
       "  'glimpses': 860,\n",
       "  'handful': 861,\n",
       "  'hardest': 862,\n",
       "  'hatred': 863,\n",
       "  'impotent': 864,\n",
       "  'indicated': 865,\n",
       "  'instance': 866,\n",
       "  'ladies': 867,\n",
       "  'leftists': 868,\n",
       "  'majority': 869,\n",
       "  'masks': 870,\n",
       "  'matter': 871,\n",
       "  'movement': 872,\n",
       "  'mustered': 873,\n",
       "  'occasional': 874,\n",
       "  'occurred': 875,\n",
       "  'organizing': 876,\n",
       "  'outshouted': 877,\n",
       "  'participate': 878,\n",
       "  'pink': 879,\n",
       "  'planned': 880,\n",
       "  'planning': 881,\n",
       "  'promised': 882,\n",
       "  'proved': 883,\n",
       "  'radical': 884,\n",
       "  'radicals': 885,\n",
       "  'rage': 886,\n",
       "  'republicans': 887,\n",
       "  'running': 888,\n",
       "  'simply': 889,\n",
       "  'small': 890,\n",
       "  'street': 891,\n",
       "  'success': 892,\n",
       "  'sucked': 893,\n",
       "  'surrounded': 894,\n",
       "  'tampa': 895,\n",
       "  'threads': 896,\n",
       "  'treat': 897,\n",
       "  'turn': 898,\n",
       "  'wanted': 899,\n",
       "  'warzone': 900,\n",
       "  'watch': 901,\n",
       "  'wearing': 902,\n",
       "  'alternative': 903,\n",
       "  'ariane': 904,\n",
       "  'ball': 905,\n",
       "  'books': 906,\n",
       "  'century': 907,\n",
       "  'chandler': 908,\n",
       "  'clues': 909,\n",
       "  'cnn': 910,\n",
       "  'construction': 911,\n",
       "  'curtains': 912,\n",
       "  'discover': 913,\n",
       "  'earliest': 914,\n",
       "  'expansion': 915,\n",
       "  'follow': 916,\n",
       "  'french': 917,\n",
       "  'galaxies': 918,\n",
       "  'galaxy': 919,\n",
       "  'gold': 920,\n",
       "  'golf': 921,\n",
       "  'guiana': 922,\n",
       "  'heart': 923,\n",
       "  'helped': 924,\n",
       "  'hole': 925,\n",
       "  'hubble': 926,\n",
       "  'images': 927,\n",
       "  'instrument': 928,\n",
       "  'intended': 929,\n",
       "  'james': 930,\n",
       "  'jwst': 931,\n",
       "  'life': 932,\n",
       "  'lynn': 933,\n",
       "  'massive': 934,\n",
       "  'mirror': 935,\n",
       "  'mission': 936,\n",
       "  'nasa': 937,\n",
       "  'october': 938,\n",
       "  'opening': 939,\n",
       "  'peer': 940,\n",
       "  'peering': 941,\n",
       "  'planets': 942,\n",
       "  'potential': 943,\n",
       "  'quarter': 944,\n",
       "  'returned': 945,\n",
       "  'rewrite': 946,\n",
       "  'rewrote': 947,\n",
       "  'rocket': 948,\n",
       "  'scheduled': 949,\n",
       "  'scientists': 950,\n",
       "  'search': 951,\n",
       "  'son': 952,\n",
       "  'space': 953,\n",
       "  'spectacular': 954,\n",
       "  'speeding': 955,\n",
       "  'spokesperson': 956,\n",
       "  'successor': 957,\n",
       "  'telescope': 958,\n",
       "  'text': 959,\n",
       "  'time': 960,\n",
       "  'tough': 961,\n",
       "  'universe': 962,\n",
       "  'way': 963,\n",
       "  'webb': 964,\n",
       "  'working': 965,\n",
       "  'announced': 966,\n",
       "  'announcing': 967,\n",
       "  'bid': 968,\n",
       "  'brexit': 969,\n",
       "  'confront': 970,\n",
       "  'currently': 971,\n",
       "  'editor': 972,\n",
       "  'existing': 973,\n",
       "  'failings': 974,\n",
       "  'gaunt': 975,\n",
       "  'gaunty': 976,\n",
       "  'grilled': 977,\n",
       "  'grills': 978,\n",
       "  'happened': 979,\n",
       "  'independence': 980,\n",
       "  'islam': 981,\n",
       "  'joined': 982,\n",
       "  'jon': 983,\n",
       "  'kassam': 984,\n",
       "  'kingdom': 985,\n",
       "  'leadership': 986,\n",
       "  'listen': 987,\n",
       "  'opposition': 988,\n",
       "  'platform': 989,\n",
       "  'proper': 990,\n",
       "  'purpose': 991,\n",
       "  'raheem': 992,\n",
       "  'range': 993,\n",
       "  'referendum': 994,\n",
       "  'repeated': 995,\n",
       "  'role': 996,\n",
       "  'sentiment': 997,\n",
       "  'source': 998,\n",
       "  'takes': 999,\n",
       "  ...})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['6'], words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "for topic in range(30):\n",
    "    word_list = lda.get_topic_terms(topic, topn=100)\n",
    "    for word_tuple in word_list:\n",
    "        word_id = word_tuple[0]\n",
    "        word_score = word_tuple[1]\n",
    "        if str(word_id) not in word_dict:\n",
    "            word_dict[str(word_id)] = {}\n",
    "        word_dict[str(word_id)][topic] = str(word_score)\n",
    "\n",
    "word_dict\n",
    "with open(\"data/word_topic.json\", \"w\") as f:\n",
    "    json.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOC TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "ix = 0\n",
    "for file in files:\n",
    "    print(ix)\n",
    "    with open(data_folder+file,encoding=\"ISO-8859-1\") as f:\n",
    "        text = \"\\n\"+f.read()\n",
    "        text_words = set(re.split('[, \\.\\n]', text))\n",
    "        html_string = text\n",
    "        for word in text_words:\n",
    "            if word.lower() in words and str(words[word.lower()]) in word_dict:\n",
    "                keys = list(word_dict[str(words[word.lower()])].keys())\n",
    "                classes = ''\n",
    "                for i in keys:\n",
    "                    classes += 't'+str(i)+\" \"\n",
    "#                 html_string = html_string.replace(r\"\\b\"+word+\"\\b\",\"<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\")\n",
    "                html_string = html_string.replace(\"\\n\"+word+\" \",\"\\n<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span> \")\n",
    "                html_string = html_string.replace(\" \"+word+\"\\n\",\" <span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\\n\")\n",
    "                html_string = html_string.replace(\" \"+word+\" \",\" <span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span> \")\n",
    "                html_string = html_string.replace(\"\\n\"+word+\"\\n\",\"\\n<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\\n\")\n",
    "#                 html_string = re.sub(word+\" \",\"<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\",html_string)\n",
    "#                 html_string = re.sub(\" \"+word,\"<span class=\\\"\"+classes+\"hoverable\\\">\"+word+\"</span>\",html_string)\n",
    "        with open(\"data/doc\"+str(ix)+\".txt\",\"w\",encoding=\"ISO-8859-1\") as f1:\n",
    "            f1.write(html_string)\n",
    "    ix+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC V/S WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(30):\n",
    "    word_list = lda.get_topic_terms(topic, topn=100)\n",
    "    file_str = \"word,value\"\n",
    "    for word_tuple in word_list:\n",
    "        word_id = word_tuple[0]\n",
    "        file_str += \"\\n\" + str(lda.id2word[word_tuple[0]]) +\",\"+ str(word_tuple[1])\n",
    "    with open(\"data/topic\"+str(topic)+\".csv\", \"w\") as f:\n",
    "        f.write(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "ix = 0\n",
    "for file in files:\n",
    "    print(ix)\n",
    "    with open(data_folder+file,encoding=\"ISO-8859-1\") as f:\n",
    "        text = \"\\n\"+f.read()\n",
    "#         lines = text.split(\"\\n\")\n",
    "        line_word_list = re.split('[, \\.\\n]', text)\n",
    "        no_lines = len(line_word_list)//10\n",
    "        \n",
    "        line_graph_data = []\n",
    "        for i in range(30):\n",
    "            l = {\n",
    "                \"name\": \"Topic\"+str(i),\n",
    "                \"id\": \"t\"+str(i),\n",
    "                \"values\": []\n",
    "            }\n",
    "            line_graph_data.append(l)\n",
    "        \n",
    "        \n",
    "        for line_index in range(no_lines):\n",
    "            line_words = line_word_list[10*line_index:10*(line_index+1)]\n",
    "            line_scores = {}\n",
    "            for word in line_words:\n",
    "                if word.lower() in words and str(words[word.lower()]) in word_dict:\n",
    "                    t_scores = word_dict[str(words[word.lower()])]\n",
    "                    for t_ix in t_scores:\n",
    "                        word_score = float(t_scores[t_ix]) \n",
    "                        if t_ix not in line_scores:\n",
    "                            line_scores[t_ix] = 0\n",
    "                        line_scores[t_ix] += word_score\n",
    "\n",
    "            for topic in range(30):\n",
    "                if topic in line_scores:\n",
    "                    line_graph_data[topic][\"values\"].append({\"line\":str(line_index+1), \"frequency\":str(line_scores[topic])})\n",
    "                else:\n",
    "                    line_graph_data[topic][\"values\"].append({\"line\":str(line_index+1), \"frequency\":str(0)})\n",
    "    with open(\"data/doc\"+str(ix)+\"_linegraph.json\", \"w\") as f:\n",
    "        json.dump({\"data\":line_graph_data}, f, indent=True)\n",
    "    ix+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
